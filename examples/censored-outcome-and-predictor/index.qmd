# Example Model 3: Censored outcome and censored predictor

```{r setup, include = FALSE}
library(cmdstanr)
# ggplot2 theme setup
library(ggplot2)
ggplot2::theme_set(hgp::theme_ms())
pth_base <- here::here("examples", "censored-outcome-and-predictor")
```

Now that we've covered the simple cases, we'll try to be a bit more
adventurous. For the next model, we'll simulate data where both the outcome
and the predictor model are censored. We'll also implement both lower and upper
limits of detection for both the outcome and the predictor, so we'll only
have one main example in this section. We'll also only implement one method --
in general the method where we integrate out censored values has been easier to
code, so we'll stick with that method for both our predictor and our output
here.

## Data simulation

As usual, we'll begin our data simulation by writing out the true data
generating process (likelihood model) that we'll use to generate the data.
This model is a bit complicated--of course we'll have the same regression
part of the model as we've had before, that relates the latent $y^*$ values to
the latent $x^*$ values. But then the observation model will include a
censoring scheme for the observation of both $x$ and $y$.

Importantly, in this model we also need to specify a distributional assumption
for $X$, otherwise we can't estimate what the uncensored $X$ values should look
like. So for the sake of simplicity, we'll assume a Gaussian distribution for
the $x$-values as well, although this is definitely something we need to think
more about in the future. **Furthermore, let's assume $x$ has a standard
normal distribution, since we can standardize $x$ before modeling.**

$$
\begin{align*}
y_i &= \begin{cases}
y_\min, & y_i^* \leq y_\min \\
y_i^* & y_\min < y_i^* \leq y_\max \\
y_\max &  y_\max < y_i^*
\end{cases} \\
x_i &= \begin{cases}
x_\min, & x_i^* \leq x_\min \\
x_i^* & x_\min < x_i^* \leq x_\max \\
x_\max &  x_\max < x_i^*
\end{cases} \\
y^*_i &\sim \mathrm{Normal}(\mu_i, \sigma^2) \\
\mu_i &= \alpha + \beta \cdot x^*_i \\
x_i^* &\sim \mathrm{Normal}(0, 1)
\end{align*}
$$

Again, we can choose whatever parameters we want for the simulation. I played
around with the simulation until I got a plot I thought looked about right.
Those simulation parameters are printed below.

```{r simulation parameters, echo = FALSE}
set.seed(13280)
sim_parms <- list(
	n = 400,
	alpha = 1,
	beta = 4,
	sigma = 5,
	y_min = -8,
	y_max = 11,
	x_min = 0,
	x_max = 2
)
str(sim_parms)
```

So with those parameters, we can then simulate some data according to
this generative model.

```{r data simulation, echo = FALSE}
gen_data <- function(n, lambda, tau, alpha, beta, sigma,
										 y_min, y_max, x_min, x_max) {
	out <- tibble::tibble(
		x_star = rnorm(n, 0, 1),
		mu = alpha + beta * x_star,
		y_star = rnorm(n, mu, sigma),
		x = dplyr::case_when(
			x_star <= x_min ~ x_min,
			x_star >  x_max ~ x_max,
			TRUE ~ x_star
		),
		y = dplyr::case_when(
			y_star <= y_min ~ y_min,
			y_star >  y_max ~ y_max,
			TRUE ~ y_star
		)
	)
	
	return(out)
}

sim_data <- do.call(gen_data, sim_parms)

perc_x <- round(mean(sim_data$x_star != sim_data$x) * 100, 2)
perc_y <- round(mean(sim_data$y_star != sim_data$y) * 100, 2)
perc_b <- round(mean(
	(sim_data$x_star != sim_data$x) &
	(sim_data$y_star != sim_data$y)
) * 100, 2)
perc_o <- perc_x + perc_y - perc_b

print(sim_data)
```

Since we've simulated the data, we know the latent values and the observed
values, so we can plot our simulated data in order to get a better understanding
of how much the censoring process will affect our estimates.

```{r data plot}
#| code-fold: true
#| code-summary: "Plotting code"
sim_data |>
	ggplot() +
	geom_hline(
		yintercept = c(sim_parms$y_min, sim_parms$y_max),
		alpha = 0.5,
		linewidth = 1,
		linetype = "dashed",
		color = "darkgray"
	) +
	geom_vline(
		xintercept = c(sim_parms$x_min, sim_parms$x_max),
		alpha = 0.5,
		linewidth = 1,
		linetype = "dashed",
		color = "darkgray"
	) +
	geom_segment(
		data = subset(sim_data, (x != x_star) | (y != y_star)),
		aes(x = x_star, xend = x, y = y_star, yend = y),
		color = "gray",
		alpha = 0.25,
		lwd = 1
	) +
	geom_point(aes(x = x_star, y = y_star), color = "gray") +
	geom_point(aes(x = x, y = y)) +
	coord_cartesian(
		xlim = c(-3, 3),
		ylim = c(-22, 22)
	) +
	labs(
		x = "Independent variable",
		y = "Dependent variable"
	)
```

We can see that a substantial amount of the data points are censored. In total,
$`r perc_x`\%$ of records were censored in $x$ only, $`r perc_y`\%$ of records
were censored in $y$ only, and $`r perc_b`\%$ of records were censored in both
$x$ and $y$. Thus, $`r perc_o`\%$ of records were censored in some way.

I also deliberately set the upper and lower limits for both $x$ and $y$ to
be asymmetrical so we can more clearly see how our censoring process can
strongly bias the estimates: we have more records censored at lower values
than higher values, which gives us a shifted window where we observe data.

So now that we have the data simulated, we want to try to recover the original
parameters with a Bayesian model.

## Stan data setup

I also want to write the Stan code to accept data in a specific format that
we want to test. The data should be formatted like the table below.

| X        | X_L      | X_U      | Y        | Y_L      | Y_U      |
|----------|----------|----------|----------|----------|----------|
| $x_1$    | $x_\min$ | $x_\max$ | $y_1$    | $y_\min$ | $y_\max$ |
| $x_2$    | $x_\min$ | $x_\max$ | $y_2$    | $y_\min$ | $y_\max$ |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $x_n$    | $x_\min$ | $x_\max$ | $y_n$    | $y_\min$ | $y_\max$ |

Here, $x_\min$ is the lower limit of detection for $x$ and $x_\max$ is the
upper limit of detection for $X$ (and similar for $Y$). Eventually, if this
is the data format we decide to permanently adopt going forward, we will want
to write a suite of helper functions to conveniently get the data in this form.
But for now I will do it manually. Fortunately it is quite easy. And if the
censoring limits changed for any observations, it would have been easier
to store the data in this format in the first place.

```{r data reshaping}
stan_data <-
	sim_data |>
	dplyr::select(x, y) |>
	dplyr::mutate(
		x_l = sim_parms$x_min,
		x_u = sim_parms$x_max,
		.after = x
	) |>
	dplyr::mutate(
		y_l = sim_parms$y_min,
		y_u = sim_parms$y_max,
		.after = y
	)

stan_data |> print(n = 5)
```

Now we just need to convert the data frame to a list format and add a variable
for the number of records.

```{r make data list}
stan_list <- as.list(stan_data)
stan_list$N <- nrow(stan_data)
str(stan_list)
```

## Stan code

Of course as usual we need to compile the Stan code. The code is also
included here for reference.

```{r compile stan model, message = FALSE}
mod_pth <- here::here(pth_base, "Ex3.stan")
mod <- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)
mod$compile(force_recompile = TRUE)
```

<!-- need to debug
::: {.callout-note icon=false collapse=true appearance="simple"}
### Model code {.unnumbered}

```{.stan include="Ex3.stan"}
```

:::
-->

## Model fitting and performance

Now that the model is successfully compiled, we need to generate MCMC samples
from the posterior distribution. We'll use 4 chains (run in parallel) with
500 warmup iterations and 2500 sampling iterations each, for a total of 10000
samples overall, which should be plenty for this problem. Otherwise, we'll
leave the control parameters at their default values.

```{r sample MCMC}
fit <- mod$sample(
	stan_list,
	seed = 123123,
	parallel_chains = 4,
	iter_warmup = 500,
	iter_sampling = 2500,
	show_messages = FALSE
)
```

As usual, we want to check the diagnostics, and fortunately `cmdstanr` gives
us an easy to use diagnostic flagger.

```{r diagnose fit}
fit$cmdstan_diagnose()
```

We can examine the trace plots and posterior distributions of the parameters
of interest to confirm that there is no funny business.

```{r traceplots}
post <- posterior::as_draws_array(fit)
bayesplot::mcmc_combo(post, par = c("alpha", "beta", "sigma"))
```

And so now we can finally examine the fitted values and compare them to our
true simulation values.

```{r model summary}
fit$summary() |>
	dplyr::filter(variable != "lp__") |>
	knitr::kable(digits = 2)
```

We can see that our model estimated the slope and variance quite well, although
it is not doing too great at figuring out the intercept. In fact, the true
value of $\alpha = 1$ isn't even in the credible interval. However, the
estimates for $\beta$ and $\sigma$ are very close to the true estimates.
In most applications, the intercept is not too useful and the slope is what we
want an accurate estimate of anyway, so this is probably acceptable.

TODO figure out what else needs to go in this example.


<!-- END OF FILE -->

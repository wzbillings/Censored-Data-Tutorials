# Example Model 1: One censored predictor

```{r setup, include = FALSE}
# ggplot2 theme setup
library(ggplot2)
ggplot2::theme_set(
	ggplot2::theme_minimal() +
		ggplot2::theme(
			plot.background = ggplot2::element_rect(
				fill = "white", color = "white"
			),
			axis.text = ggplot2::element_text(size = 12, color = "black"),
			axis.title = ggplot2::element_text(size = 22),
			plot.subtitle = ggplot2::element_text(
				size = 16, hjust = 0, margin = ggplot2::margin(b = 2)
			),
			plot.title = ggplot2::element_text(
				size = 19, hjust = 0, margin = ggplot2::margin(b = 2, l = 2)
			),
			plot.caption = ggplot2::element_text(size = 14),
			strip.text = ggplot2::element_text(
				size = 16, hjust = 0.5, margin = ggplot2::margin(b = 2, t = 2)
			),
			panel.spacing = ggplot2::unit(2, "lines"),
			legend.position = "bottom",
			legend.text = ggplot2::element_text(size = 16, color = "black"),
			legend.title = ggplot2::element_text(size = 18, color = "black"),
			plot.margin = ggplot2::margin(t = 6, r = 6, b = 6, l = 6)
		)
)
```


For our first example model, we will consider an example where we have a single
censored predictor. Other than censoring, we assume the predictor is measured
without error. We also assume a non-censored perfectly observed outcome,
which follows a Gaussian linear model where the only predictor is that
censored one. (We could add additional non-censored predictors but it would
just make this example more complicated.) The data generating process for the
outcome, $y_i, \ i = \{1, 2, \ldots, n\}$ is
$$
\begin{align*}
y_i &\sim \mathrm{Normal}\left(\mu, \sigma^2\right) \\
\mu &= \beta_0 + \beta_1 x_i^*
\end{align*}
$$
where $x_i^*$ is the latent, true value of the predictor $x$. However, because
of some imperfect measurement process, we can't observed $x_i^*$, instead we
observe
$$
x_i = \begin{cases}
x_{\min}, & x_i^* \leq x_{\min} \\
x_i^*, & x_i^* > x_{\min}
\end{cases}.
$$
So if we want to correctly model $y$, we will need to take the left censoring
of $x$ into account.

```{r}
generate_data <- function(n = 100, xmin = 3, beta0 = 1, beta1 = 2, sigma = 3,
													seed = 83719273) {
	l <- tibble::tibble(
		x_star = runif(n, 0, 10),
		x = ifelse(x_star <= xmin, xmin, x_star),
		mu = beta0 + beta1 * x_star,
		y = rnorm(n, mu, sigma)
	)
	
	o <- dplyr::select(l, x, y)
	
	out <- list("latent" = l, "observed" = o)
}

dat <- generate_data()
dat_l <- dat$latent
dat_o <- dat$observed
```

We can take a quick look at the data to see what the relationship looks
like when x is censored or not.

```{r}
dat_l |>
	ggplot() +
	geom_vline(xintercept = 3, linetype = "dashed", color = "gray") +
	geom_line(
		aes(x = x_star, y = 1 + 2 * x_star), color = "red", linetype = "dashed",
		linewidth = 1
	) +
	geom_point(aes(x = x_star, y = y), color = "gray", size = 2) +
	geom_point(aes(x = x, y = y), color = "black", size = 2) +
	labs(x = "x", y = "y")
```

Here the gray points show the true latent values of the censored points, and
the black points show what we actually observed. The red line is the true
regression line from the data generating process.

For a Bayesian model, before we can fit anything to the data we need to choose
suitable priors.

```{r}
dat_o |>
	dplyr::mutate(
		cens = factor(x <= 3, levels = c(FALSE, TRUE), labels = c('obs', 'cens'))
	) |>
	tidyr::pivot_wider(
		names_from = cens,
		values_from = x,
		names_glue = "x_{cens}"
	)
```



<!-- END OF FILE -->

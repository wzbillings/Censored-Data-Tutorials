[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian censoring",
    "section": "",
    "text": "Preface\nThis book is where we can store our thoughts and codes on dealing with censoring data in Bayesian models. See the README to find where everything is and how to contribute.\nContributors:\n\nZane Billings (https://wzbillings.com/)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  What are censored data?",
    "section": "",
    "text": "Censoring is a selection phenomenon on data which occurs when we can only obtain precise measurements for certain measurement values. The classical example is a scale which can only measure weights up to a certain threshold, say \\(y_{\\mathrm{max}}\\). A scale that can measure any weight would produce the set of measurements \\(\\{y_1^*, y_2^*, \\ldots, y_n^*\\}\\) where \\(n\\) is the sample size. We call these the latent values or true values. Our imperfect scale would then produce the observed values, \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\leq y_{\\mathrm{max}} \\\\\ny_{\\mathrm{max}}, & y_i^* &gt; y_{\\mathrm{max}}\n\\end{cases}; \\quad i = 1, \\ldots, n.\n\\] Specifically, this is an example of right censoring, where there is an upper limit of detection, or maximum value that we can observe with precision.\nWe can also have the opposite case, where we can detect any theoretical measurement above a certain value. In this case, the data are said to have a lower limit of detection and this phenomenon is called left censoring. For example, imagine we are testing the concentration of lead in the tap water of several buildings. Our test cannot detect lead levels below 5 parts per billion (ppb), but can detect any larger amount of lead. In that case, our observed values would instead look like this: \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\] where \\(y_{\\mathrm{min}}\\) is the lower limit of detection.\n\\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\]\nFinally, we can have interval censoring, where we know a data value is within some interval, but we do not know precisely where the value lies within that interval. An example of this is antibody titer dilutions: for flu HAI titer, the values are typically reported as 10, 20, 40, etc., but a value of 10 does not mean the precise value of the measurement should be 10, it means the true value is between 10 and 20. If we assume our titer is measured on the log scale and has no limits of detection, we could write \\[\ny_i = \\lfloor y_i^* \\rfloor; \\quad i = 1, \\ldots, n,\n\\] because we only perform a discrete number of dilutions. This gives us the interval value for \\(y_i\\) as \\[y_i \\in \\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right).\\]\nA given variable can be subject to all of these types of censoring simultaneously: for example, HAI titers are interval censored in this way, but they also have lower limits of detection and upper limits of detection as well (though the upper limits are rarely important in practice because they can be arbitrarily increased during the assay). However, a particular observation of this variable can only be subject to one type of censoring at a time: e.g., if an observation is below the detection limit, that value is left censored, it cannot simultaneously be right censored or interval censored.\nNotably, the distinction between “types” of censoring in this way is useful for several analytic methods, but is not strictly necessary. All censored values can be implicitly treated as interval censored data, where the lower endpoint for a left censored value is negative infinity, and the upper endpoint for a right censored value is positive infinity. Thus, we could write the data generating process for HAI titers with LOD as \\[\ny_i = \\begin{cases}\ny_{\\text{min}}, \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\lfloor y_i^* \\rfloor, \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\ny_{\\text{max}}, y \\leq y_{\\text{max}}\n\\end{cases},\n\\] where \\(y_{\\text{min}}\\) is the lower limit of detection and \\(y_{\\text{max}}\\) is the upper limit of detection. To express the DGP in interval notation, we would write \\[\ny_i \\in \\begin{cases}\n\\left(-\\infty, y_{\\text{min}}\\right), \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right), \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\n\\left[y_{\\text{max}}, \\infty\\right), \\ y \\leq y_{\\text{max}}\n\\end{cases}.\n\\] Note also that assuming \\(y^*_i\\) is drawn from an absolutely continuous distribution (e.g. normal or lognormal, etc.), the final likelihood model will be equivalent regardless of which intervals are open or closed. This model would allow us to put all of the censored observations into the likelihood function in the same framework without having to worry about sorting the observations into buckets w.r.t. the type of censoring.\nThe probability of each observation \\(y_i\\) can then be expressed as the probability that the random variable \\(Y\\) takes on a realization inside the given interval. If \\(F\\) is the CDF for some parametric distribution which we assume the latent variable \\(y_i^*\\) is drawn from, with parameter \\(\\theta\\), the contribution of \\(y_i\\) to the likelihood is then \\[\\mathcal{L}(\\theta \\mid Y_i) = F(\\text{upper limit of interval}) - F(\\text{lower limit of interval}).\\] If we call the lower limit of the interval for \\(y_i\\) \\(L_i\\) and the corresponding upper limit \\(U_i\\), we can write the likelihood of the sample as \\[\n\\mathcal{L}(\\theta \\mid \\mathbf{Y}) = \\prod_{i=1}^n \\left(\nF(y_i \\mid \\theta)\\bigg\\rvert_{y_i = L_i}^{U_i}\n\\right)^{C_i}\\bigg(f(y_i\\mid \\theta) \\bigg)^{C_i},\n\\] where \\(C_i\\) is the indicator variable for \\(y_i\\) being censored. Notably, for an uncensored observation the likelihood is equal to the density. However, for the typical types of HAI data that we see, all of the assay values are subject to the same censoring process, and thus we could neglect the density component.\nSo now the remaining issue is to specify \\(F\\), the CDF of the latent variables."
  },
  {
    "objectID": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "href": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "title": "2  Example Model 1: One censored predictor",
    "section": "2.1 The other method (Bjorn method)",
    "text": "2.1 The other method (Bjorn method)\n\nmod2 &lt;- cmdstanr::cmdstan_model(stan_file = here::here('Ex1b.stan'))\n\n\ndat2 &lt;- list()\ndat2$y &lt;- df_stan$y\ndat2$x &lt;- df_stan$x\ndat2$x_cens &lt;- df_stan$cens\ndat2$N &lt;- length(dat2$y)\ndat2$DL &lt;- lod\n\n\nfit2 &lt;- mod2$sample(dat2, seed = 100, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpq4W877/model-44107d12126b.stan', line 64, column 3 to column 55)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 9.3 seconds.\nChain 4 finished in 9.3 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 9.4 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 9.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 9.4 seconds.\nTotal execution time: 9.6 seconds.\n\n\n\nfit2$summary() |&gt;\n    dplyr::filter(!startsWith(variable, \"x\")) |&gt;\n    print(n = Inf)\n\n# A tibble: 6 × 10\n  variable      mean     median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;num&gt;      &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -3099.    -3098.     23.6    23.3    -3.14e+3 -3.06e+3  1.00    1381.\n2 a            0.158     0.148   0.329   0.327  -3.79e-1  6.89e-1  1.00    2660.\n3 b            2.17      2.17    0.0564  0.0566  2.07e+0  2.26e+0  1.00    2732.\n4 s            4.90      4.90    0.109   0.110   4.73e+0  5.09e+0  1.00    6319.\n5 mu_x        -0.174    -0.0675  9.86    9.45   -1.67e+1  1.62e+1  1.00    6405.\n6 sigma_x      0.510     0.342   0.521   0.363   2.73e-2  1.57e+0  1.00    5287.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\n\npost &lt;- posterior::as_draws_array(fit2)"
  },
  {
    "objectID": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "href": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "title": "3  Example Model 2: One censored outcome",
    "section": "3.1 Lower limit of detection",
    "text": "3.1 Lower limit of detection\nFor the first example, we’ll work with an outcome that has a lower limit of detection. First we need to simulate the data, which means we need to write out a generative model for the data. We’ll randomly sample x for the purposes of generating data, but for the purposes of our model we’ll assume x_i is a completely observed covariate and thus is known and does not need a random component in the model.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\n\\mathrm{DL}, & y^*_i \\leq \\mathrm{DL} \\\\\ny^*_i, & y^*_i &gt; \\mathrm{DL}\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\ni &= 1, 2, \\ldots, n\n\\end{align*}\n\\] Here, DL is the Detection Limit, aka the lower limit of detection for the variable. Of course in our generative model, we have set \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\) to be fixed population parameters, but for Bayesian inference we would need to assign suitable priors. Let’s set the values and simulate our data. The parameters I set for this example are as follows.\n\n\n\nParameter\nValue\nMeaning\n\n\n\n\n\\(n\\)\n271\nSample size\n\n\n\\(\\alpha\\)\n72\nRegression intercept\n\n\n\\(\\beta\\)\n3\nRegression slope\n\n\n\\(\\sigma\\)\n5\nStandard deviation of outcome\n\n\n\\(\\mathrm{DL}\\)\n80\nLower limit of detection\n\n\n\nThe \\(x\\)-values were drawn from a uniform distribution on \\((0, 10)\\). Since we know the true population parameters for our simulation, we can plot the data to see the effect of the censoring process on our observed \\(y\\) values.\n\n\n\n\n\nIn this plot, the black data points show our observed data. For those observations where the \\(y\\) value was below the limit of detection and thus censored, the gray points show the true latent values, which we could have observed with a perfect measurement process. The gray line segments connect each latent measurement to its corresponding observed measurement.\nApproximatly \\(22.88\\%\\) of data points were below the limit of detection and were therefore censored. Of course in real life, we would only observe the black points (observed values), and the gray points would be unobservable to us. But for the purposes of understanding how to analyze censored data, visualizing how different the observed and latent datasets are is quite valuable and informative. Since the datasets look so different, we should not be surprised that our regression estimates would be incorrect if we treated all of the censored values as the same constant value, or ignored them entirely!\nSo, if our standard linear regression model that we know and love (even the Bayesian version) would give us incorrect estimates using any of these naive methods, how then are we to proceed? According to the Stan manual (Stan Development Team 2023, chap. 4), there are two main ways of handling the censoring in the outcome in our model. The first of these methods relies on imputation and the second on integration of the likelihood function and manual updating of the target likelihood in Stan. The imputation method is conceptually easier and less mathematically daunting, so we begin our treatment there.\n\n3.1.1 Imputation-type method\nThe first method for dealing with censored data treats the censored values as missing values where the latent value is constrained to fall within a specific range. For a normally distributed outcome, all values below the lower limit of detection are constrained to fall within \\((-\\infty, \\mathrm{DL})\\).\nREAD THAT PART OF RETHINKING AND EXPLAIN HOW MISSING DATA WORKS HERE!!!\nTo implement such a model in Stan, we need to pass in the number of observed and the number of censored values and the observed y-values in Stan. We then declare the censored \\(y\\)-values as a parameter in the Stan code, meaning they will be sampled from their constrained distribution during the fitting process, whereas the observed \\(y\\) values will be used to update the parameter estimates.\nFirst, let’s look at the Stan code for this model.\nSHOW THE STAN CODE HERE.\nSince the data need to be in kind of a clunky format to use this method, we first need to do some wrangle and get the data in the correct format for Stan.\n\ndat_2a &lt;- list()\nwith(\n    sim_data, {\n        dat_cens &lt;- subset(sim_data, cens)\n        dat_obs &lt;- subset(sim_data, !cens)\n        dat_2a$N_cens &lt;&lt;- nrow(dat_cens)\n        dat_2a$N_obs &lt;&lt;- nrow(dat_obs)\n        dat_2a$y_obs &lt;&lt;- dat_obs$y\n        dat_2a$x_obs &lt;&lt;- dat_obs$x\n        dat_2a$y_cens &lt;&lt;- dat_cens$y\n        dat_2a$x_cens &lt;&lt;- dat_cens$x\n        dat_2a$DL &lt;&lt;- as.integer(sim_parms$DL)\n    }\n)\n\nstr(dat_2a)\n\nList of 7\n $ N_cens: int 62\n $ N_obs : int 209\n $ y_obs : num [1:209] 92.6 104.2 86.2 88.7 94.9 ...\n $ x_obs : num [1:209] 8.98 9.31 3.42 8.5 8.14 ...\n $ y_cens: num [1:62] 80 80 80 80 80 80 80 80 80 80 ...\n $ x_cens: num [1:62] 0.227 0.555 1.168 2.853 2.434 ...\n $ DL    : int 80\n\n\nNow we can compile the Stan program (via cmdstanr as usual).\n\nmod_2a &lt;- cmdstanr::cmdstan_model(here::here(\"Ex2a.stan\"), compile = FALSE)\nmod_2a$compile(pedantic = TRUE, force_recompile = TRUE)\n\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.stan', line 56, column 21: Argument\n    0.01 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.stan', line 55, column 18: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.stan', line 54, column 19: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning: The parameter y_cens has no priors. This means either no prior is\n    provided, or the prior(s) depend on data variables. In the later case,\n    this may be a false positive.\n\n\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function&lt;typename unary_traits&lt;Predicate&gt;::argument_type,bool&gt;\n      |                                             ^~~~~~~~~~~~~~\n\n\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function&lt;\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template&lt;cl\n\n\nass _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function&lt;T*, S&gt;\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function&lt;T*, A, S&gt;\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function&lt;const T*, S&gt;\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: \n\n\n'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function&lt;const T*, A, S&gt;\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function&lt;T&, S&gt;\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;T&, A, S&gt;\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function&lt;const T&, S&gt;\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n\n\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;const T&, A, S&gt;\n      |                                                                    ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function&lt;Arg,Result&gt;\n      |                                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function&lt;Arg1,Arg2,Result&gt;\n      |                                                                          ^~~~~~~~~~~~~~~\n\n\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = Ex2a_model_namespace::Ex2a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.hpp:443: note:   by 'Ex2a_model_namespace::Ex2a_model::write_array'\n  443 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex2a_model_namespace::Ex2a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/model-574c5e3b404.hpp:443: note:   by 'Ex2a_model_namespace::Ex2a_model::write_array'\n  443 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n\nAnd since the program compiles correctly, we can use Stan’s sampling algorithm to generate samples from the posterior distribution. We’ll run 4 chains in parallel with 500 warmup iterations and 5000 sampling iterations per chains, with all of the other control parameters (e.g. maximum treedepth and adaptive delta) left at the cmdstan defaults. This many samples is overkill for this problem, but it is also quite fast and thus we can do many samples just to be safe.\n\nfit_2a &lt;- mod_2a$sample(\n    dat_2a, seed = 100, parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 5000\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 1 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 1 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 1 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 1 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 1 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 2 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 2 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 2 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 3 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 3 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 4 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 4 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 4 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 1 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 1 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 2 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 2 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 2 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 2 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 3 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 3 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 3 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 4 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 4 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 4 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 4 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 1 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 1 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 2 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 2 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 3 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 3 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 4 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 1 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 1 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 2 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 2 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 3 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 4 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 1 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 1 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 2 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 2 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 3 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 4 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 1 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 1 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 2 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 3 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 4 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 4 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 1 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 1 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 2 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 2 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 3 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 3 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 4 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 4 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 1 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 1 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 2 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 2 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 3 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 4 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 4 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 4 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 1 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 1 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 2 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 3 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 3 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 4 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 4 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 4 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 1 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 2 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 2 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 3 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 3 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 4 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 4 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 1 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 1 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 2 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 2 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 3 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 3 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 4 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 4 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 1 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 1 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 2 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 2 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 3 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 3 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 4 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 1 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 1 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 2 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 3 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 3 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 4 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 4 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 1 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 1 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 2 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 2 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 3 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 4 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 1 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 1 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 2 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 2 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 3 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 4 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 4 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 1 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 1 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 2 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 2 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 3 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 3 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 4 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 4 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 1 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 2 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 4 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 4 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 1 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 2 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 2 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 4 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 4 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 4 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 1 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 1 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 1 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 2 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 3 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 3 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 4 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 4 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 4 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 4 finished in 2.8 seconds.\nChain 1 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 1 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 2 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 2 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 3 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 1 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 1 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 2 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 2 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 3 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 1 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 1 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 2 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 3 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 3 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 3 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 3 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 2 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 3 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 1 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 1 finished in 3.5 seconds.\nChain 3 finished in 3.4 seconds.\nChain 2 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 2 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 2 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 2 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 finished in 4.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.4 seconds.\nTotal execution time: 4.4 seconds.\n\n# Extract the posterior samples in a nicer format for later\npost_2a &lt;- posterior::as_draws_df(fit_2a)\n\nThe first thing we should do after sampling is check for any diagnostic warnings. We have access to all of the individual diagnostics, but fortunately cmdstan has a built-in diagnostic checker to flag any potential problems.\n\nfit_2a$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2a-202311060835-1-5f62f2.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2a-202311060835-2-5f62f2.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2a-202311060835-3-5f62f2.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2a-202311060835-4-5f62f2.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nGreat, no issues with the sampling procedure, that is what we like to see. Let’s manually check the trace plots for our main three parameters of interest. (We could also check the plots for all of the imputed y-values, but these are unlikely to be interesting or useful, any problems should hopefully propagate through to the interesting parameters.)\n\nbayesplot::mcmc_combo(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nThose look like nice healthy trace plots, so with that combined with our diagnostic check, it seems that the chains mixed well and explored the posterior distribution. We can also check if those parameters were correlated.\n\nbayesplot::mcmc_pairs(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nWe see that the slope and intercept estimates were strongly correlated, which makes sense, and the sigma parameter was slightly correlated with both of those but not strongly with either. We can notice here that the histograms for \\(\\beta\\) and \\(\\sigma\\) are not quite centered at the true values, but they do have some probability mass at those true values. Let’s look at the median estimates and CIs from our samples.\n\npar_sum &lt;-\n    fit_2a$summary(variables = c(\"alpha\", \"beta\", \"sigma\"))\npar_sum |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n73.175358\n73.195200\n0.7612478\n0.7649475\n71.910180\n74.395245\n1.000667\n8533.648\n10108.17\n\n\nbeta\n2.809101\n2.806420\n0.1216903\n0.1226851\n2.614248\n3.010981\n1.000255\n9050.044\n11862.90\n\n\nsigma\n4.897496\n4.891605\n0.2447610\n0.2444807\n4.515708\n5.316280\n1.000053\n16448.745\n13836.85\n\n\n\n\n\nWe can also plot those along with the true values for reference.\n\n\nShow plot code (messy)\ntruth &lt;- tibble::tibble(\n    name = c(\"alpha\", \"beta\", \"sigma\"),\n    value = c(sim_parms$alpha, sim_parms$beta, sim_parms$sigma)\n)\n\nhd &lt;- post_2a |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::select(alpha, beta, sigma) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything())\n\nggplot() +\n    aes(x = value) +\n    geom_histogram(\n        data = subset(hd, name == \"alpha\"),\n        boundary = 0,\n        binwidth = 0.25,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"beta\"),\n        boundary = 0,\n        binwidth = 0.05,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"sigma\"),\n        boundary = 0,\n        binwidth = 0.1,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_vline(\n        data = truth,\n        aes(xintercept = value),\n        linetype = \"dashed\",\n        linewidth = 1,\n        color = \"red\"\n    ) +\n    facet_wrap(~name, scales = \"free\") +\n    labs(x = NULL)\n\n\n\n\n\nFrom these histograms, we can see that while there is a decent amount of samples close to the true values of alpha and beta, the posterior distributions are not centered around the true values. At the time of writing, I am not sure if that is a fixable problem or just something we have to deal with from having imperfectly observed data.\nWe can also do a check of how close the imputed \\(y\\) values were on average to the actual \\(y\\) values.\n\ny_cens_sum &lt;-\n    fit_2a$summary(variables = paste0('y_cens[', 1:dat_2a$N_cens, ']'))\n\ndat_comp &lt;-\n    sim_data |&gt;\n    subset(cens) |&gt;\n    dplyr::select(y_star) |&gt;\n    dplyr::bind_cols(y_cens_sum) |&gt;\n    dplyr::mutate(\n        col = dplyr::case_when(\n            (mean &gt;= y_star) & (q5 &lt;= y_star) ~ TRUE,\n            (mean &lt;= y_star) & (q95 &gt;= y_star) ~ TRUE,\n            TRUE ~ FALSE\n        )\n    )\n\n# ggplot(dat_comp) +\n#   aes(x = y_star, y = mean, ymin = q5, ymax = q95, color = col) +\n#       geom_abline(\n#           slope = 1, intercept = 0, linetype = 2, linewidth = 1,\n#                               alpha = 0.5\n#           ) +\n#   geom_errorbar(alpha = 0.25) +\n#   geom_point() +\n#   coord_fixed() +\n#   scale_color_manual(\n#       values = c(\"orange\", \"turquoise\"),\n#       name = \"CI crosses diagonal\"\n#   )\n\nggplot(dat_comp) +\n    aes(x = (y_star - mean)) +\n    geom_histogram(boundary = 0, binwidth = 1, color = \"black\", fill = \"gray\") +\n    scale_x_continuous(breaks = seq(-10, 10, 2), limits = c(-10, 10)) +\n    labs(\n        x = \"True value - mean estimated value\"\n    )\n\n\n\n\nTODO make this relative error instead to make it easier to understand.\n\n\n3.1.2 Integration-type method\nThe second method relies on calculating the direct contribution of the censored data measurements to the likelihood by integrating the density over the region where censored data can occur. That is, if the \\(i\\)th observation is below the detection limit, we know that the contribution of that observation to the sample likelihood is \\[\n\\mathcal{L}(\\theta \\mid y_i) = P(Y_i \\leq \\mathrm{DL}) = \\int_{-\\infty}^{\\mathrm{DL}}f(y_i \\mid \\theta) \\ dy = \\lim_{a \\to -\\infty} \\left[F(y_i \\mid \\theta)\\right]_{a}^{\\mathrm{DL}},\n\\] which is why we refer to this method as “integrating out” the censored values.\nBy adapting the Stan code from the manual (Stan Development Team 2023, cp. 4) to include \\(x\\) values in the calculation of the mean, we can implement this method for dealing with our censored \\(y\\) values. First we’ll load and compile the Stan model.\n\nmod_2b &lt;- cmdstanr::cmdstan_model(here::here(\"Ex2b.stan\"), compile = FALSE)\nmod_2b$compile(force_recompile = TRUE)\n\n//\n// Ex2a: One censored predictor using the imputation method\n// has one lower limit of detection only\n// See: https://mc-stan.org/docs/stan-users-guide/censored-data.html\n// Zane Billings\n// 2023-10-17\n//\n\n// The input data consists of:\n// - N: \n// - N_cens: an integer, the number of data points where the outcome was below\n//   the DL and thus censored.\n// - y: real array of observed outcome variable\n// - cens: int array; indicator which is 1 if the corresponding measurement of\n//   y is censored and 0 otherwise.\n// - x: real array of observed predictor variable\n// - DL: int, the detection limit of the assay (right now it has to be\n//   identical for all observations but we want to change that in future)\ndata {\n\tint&lt;lower=0&gt; N;\n\tint&lt;lower=0, upper=N&gt; N_cens;\n\tarray[N] real y;\n\tarray[N] int cens;\n\tarray[N] real x;\n\tint&lt;upper=to_int(min(y))&gt; DL;\n}\n\n// transformed data are values we can calculate directly from the inputted data.\n// - N_cens is an integer equal to N_cens + N_obs.\n// transformed data {\n// \tint&lt;lower=0 upper=N&gt; N_obs;\n// \tN_obs = N_cens - N;\n// }\n\n// The parameters accepted by the model. Our model\n// accepts the real-valued parameters alpha, beta, (the regression coefs) and\n// the positive real-valued parameter sigma (the variance of the outcome\n// distribution).\nparameters {\n\t// Regression parameters\n\treal alpha;\n\treal beta;\n\treal&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated.\nmodel {\n\t// Priors for parameters\n\talpha ~ normal(0, 100);\n\tbeta ~ normal(0, 100);\n\tsigma ~ exponential(0.01);\n\t\n\t// Loop through each observation and calculate the mean. If the current\n\t// y value is observed, treat it like normal. If it is censored, we need to\n\t// update the likelihood by integrating out the value.\n\tarray[N] real mu;\n\tfor (i in 1:N) {\n\t\tmu[i] = alpha + beta * x[i];\n\t\tif (cens[i] == 0) {\n\t\t\ty[i] ~ normal(mu[i], sigma);\n\t\t} else if (cens[i] == 1) {\n\t\t\ttarget += normal_lcdf(DL | mu[i], sigma);\n\t\t}\n\t}\n}\n\nAs you can see from the above program, the data needs to be in a different format for this method. Actually, it’s much easier to set up the data in the way this program specifies, and it’s very similar to the data frame we already have. We just need a list and a few other components.\n\ndat_2b &lt;- list()\ndat_2b$N &lt;- nrow(sim_data)\ndat_2b$N_cens &lt;- sum(sim_data$cens)\ndat_2b$y &lt;- sim_data$y\ndat_2b$cens &lt;- as.integer(sim_data$cens)\ndat_2b$x &lt;- sim_data$x\ndat_2b$DL &lt;- as.integer(sim_parms$DL)\n\nstr(dat_2b)\n\nList of 6\n $ N     : int 271\n $ N_cens: int 62\n $ y     : num [1:271] 92.6 104.2 86.2 88.7 94.9 ...\n $ cens  : int [1:271] 0 0 0 0 0 0 0 0 0 0 ...\n $ x     : num [1:271] 8.98 9.31 3.42 8.5 8.14 ...\n $ DL    : int 80\n\n\nNow that we have the program ready and the data set up, we can give the data to the program and do some MCMC sampling. We’ll use a similar setup that we did for the previous example, namely 4 parallel chains which each run 500 warmup iterations and 1000 sampling iterations (no need for overkill like we did before).\n\nfit_2b &lt;- mod_2b$sample(\n    dat_2b, seed = 100, parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 1000,\n    show_messages = FALSE\n)\n\n# Extract the posterior samples in a nicer format for later\npost_2b &lt;- posterior::as_draws_df(fit_2b)\n\nWe didn’t get any warnings or errors, which means that the model finished the sampling procedure without any major errors, and we should next check the diagnostics.\n\nfit_2b$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2b-202311060836-1-69aa6f.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2b-202311060836-2-69aa6f.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2b-202311060836-3-69aa6f.csv, C:/Users/Zane/AppData/Local/Temp/RtmpWWjlVK/Ex2b-202311060836-4-69aa6f.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nEverything looks good here, but let’s again look at the traceplots.\n\nbayesplot::mcmc_combo(post_2b, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nWe got some nice, healthy looking fuzzy caterpillars, so now we can be confident in our summary results. So now let’s finally look at the parameter estimates.\n\nfit_2b$summary(variables = c('alpha', 'beta', 'sigma'))\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 alpha    73.1   73.1  0.767 0.803 71.8  74.3   1.00    1324.    1250.\n2 beta      2.82   2.81 0.122 0.126  2.62  3.02  1.00    1285.    1479.\n3 sigma     4.90   4.90 0.243 0.244  4.52  5.32  1.00    2114.    2269.\n\n\nThese estimates are pretty similar to the estimates from the other method, which is good in a way because it means both methods are similar. The frequentist tobit model estimate is also similar (see the appendix).\nUnfortunately, none of the three models to estimate the regression value while taking the censoring into account produce estimates that are exactly the same as the true simulation parameters. However, unlike the much worse naive model estimates, at least our uncertainty intervals correctly contain the true values this time. So we cannot fully erase the effect of the flawed observation process on our data, but we can do a lot better by taking the censored data into consideration."
  },
  {
    "objectID": "Ex2-Simple-Censored-Outcome.html#appendix-tobit-model-check",
    "href": "Ex2-Simple-Censored-Outcome.html#appendix-tobit-model-check",
    "title": "3  Example Model 2: One censored outcome",
    "section": "Appendix: tobit model check",
    "text": "Appendix: tobit model check\nSince we’re dealing with one censored outcome with a known limit of detection, there are actually some well-developed frequentist methods for this problem. Namely, we can use a tobit model, which specificies the likelihood model in the same way we did for the bayesian estimation method, and works very similarly to the second method where we integrate out the censored data points. However, instead of specifying priors to get a posterior distribution via Bayes’ theorem, we instead estimate the parameters by finding the parameters which maximize the sample likelihood.\nMany models for censored outcomes with a variety of distributions are implemented in the R core package survival, but the formula for specifying a tobit model with a gaussian outcome distribution correctly is very unintuitive. Thankfully, the package AER provides a simple tobit() wrapper which translates a more standard formula into the appropriate form for the survReg() function and fits the model. Fitting our model using AER::tobit() is simple.\n\ntobit_model &lt;- AER::tobit(\n    y ~ x,\n    data = sim_data,\n    left = sim_parms$DL,\n    right = Inf,\n    dist = \"gaussian\"\n)\n\nsummary(tobit_model)\n\n\nCall:\nAER::tobit(formula = y ~ x, left = sim_parms$DL, right = Inf, \n    dist = \"gaussian\", data = sim_data)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           271             62            209              0 \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 73.21256    0.75531   96.93   &lt;2e-16 ***\nx            2.80427    0.12071   23.23   &lt;2e-16 ***\nLog(scale)   1.57771    0.04957   31.83   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 4.844 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 6 \nLog-likelihood: -661.2 on 3 Df\nWald-statistic: 539.7 on 1 Df, p-value: &lt; 2.22e-16 \n\n\nIf we want a lot of compatibility with standard R functions however (e.g. broom::tidy() to get the confidence intervals for the parameters), we need to use survreg. Fortunately the documentation for AER::tobit() explains how the formula is transmogrified.\n\nU &lt;- sim_parms$DL\nsurvreg_model &lt;- survival::survreg(\n    survival::Surv(y, y &gt; U, type = 'left') ~ x,\n    data = sim_data,\n    dist = \"gaussian\"\n)\nsummary(survreg_model)\n\n\nCall:\nsurvival::survreg(formula = survival::Surv(y, y &gt; U, type = \"left\") ~ \n    x, data = sim_data, dist = \"gaussian\")\n              Value Std. Error    z      p\n(Intercept) 73.2126     0.7553 96.9 &lt;2e-16\nx            2.8043     0.1207 23.2 &lt;2e-16\nLog(scale)   1.5777     0.0496 31.8 &lt;2e-16\n\nScale= 4.84 \n\nGaussian distribution\nLoglik(model)= -661.2   Loglik(intercept only)= -831\n    Chisq= 339.55 on 1 degrees of freedom, p= 8e-76 \nNumber of Newton-Raphson Iterations: 6 \nn= 271 \n\n\nWe can see that the two models are exactly the same. But since we’ve used a model from survival, we get the benefit of widespread compatibility with other R-ecosystem functionality. For example, we can easily get confidence intervals for all three estimated parameters with broom.\n\nbroom::tidy(survreg_model, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    73.2     0.755       96.9 0            71.7      74.7 \n2 x               2.80    0.121       23.2 2.15e-119     2.57      3.04\n3 Log(scale)      1.58    0.0496      31.8 2.77e-222    NA        NA   \n\n\nOr at least I thought we could. Apparently there is not a built-in method to give the CI for the scale parameter, and we have to do it ourselves.\n\n\nCode for 95% CI for scale\npaste0(\n    \"Scale estimate: \",\n    round(exp(1.58), 2),\n    \", 95% CI: \",\n    round(exp(1.58 - 1.96 * 0.0496), 2),\n    \" - \",\n    round(exp(1.58 + 1.96 * 0.0496), 2),\n    \".\"\n)\n\n\n[1] \"Scale estimate: 4.85, 95% CI: 4.41 - 5.35.\"\n\n\nAnyways, we can compare these to the Bayesian estimates above and see that they are quite similar.\n\n\n\n\n\nStan Development Team. 2023. Stan Modeling Language Users’ Guide and Reference Manual. 2.33 ed."
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#data-simulation",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#data-simulation",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.1 Data simulation",
    "text": "4.1 Data simulation\nAs usual, we’ll begin our data simulation by writing out the true data generating process (likelihood model) that we’ll use to generate the data. This model is a bit complicated–of course we’ll have the same regression part of the model as we’ve had before, that relates the latent \\(y^*\\) values to the latent \\(x^*\\) values. But then the observation model will include a censoring scheme for the observation of both \\(x\\) and \\(y\\).\nImportantly, in this model we also need to specify a distributional assumption for \\(X\\), otherwise we can’t estimate what the uncensored \\(X\\) values should look like. So for the sake of simplicity, we’ll assume a Gaussian distribution for the \\(x\\)-values as well, although this is definitely something we need to think more about in the future. Furthermore, let’s assume \\(x\\) has a standard normal distribution, since we can standardize \\(x\\) before modeling.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\ny_\\min, & y_i^* \\leq y_\\min \\\\\ny_i^* & y_\\min &lt; y_i^* \\leq y_\\max \\\\\ny_\\max &  y_\\max &lt; y_i^*\n\\end{cases} \\\\\nx_i &= \\begin{cases}\nx_\\min, & x_i^* \\leq x_\\min \\\\\nx_i^* & x_\\min &lt; x_i^* \\leq x_\\max \\\\\nx_\\max &  x_\\max &lt; x_i^*\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}(0, 1)\n\\end{align*}\n\\]\nAgain, we can choose whatever parameters we want for the simulation. I played around with the simulation until I got a plot I thought looked about right. Those simulation parameters are printed below.\n\n\nList of 8\n $ n    : num 400\n $ alpha: num 1\n $ beta : num 4\n $ sigma: num 5\n $ y_min: num -9\n $ y_max: num 12\n $ x_min: num -1\n $ x_max: num 2\n\n\nSo with those parameters, we can then simulate some data according to this generative model.\n\n\n# A tibble: 400 × 5\n    x_star      mu  y_star       x       y\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1.80     8.19   7.08    1.80    7.08  \n 2  1.16     5.64   6.71    1.16    6.71  \n 3  0.155    1.62   6.05    0.155   6.05  \n 4  0.0988   1.40   1.24    0.0988  1.24  \n 5 -3.16   -11.6   -7.35   -1      -7.35  \n 6 -0.682   -1.73  -1.46   -0.682  -1.46  \n 7  1.56     7.25   3.40    1.56    3.40  \n 8 -0.195    0.219 -0.216  -0.195  -0.216 \n 9  0.628    3.51   7.10    0.628   7.10  \n10  0.821    4.28   0.0773  0.821   0.0773\n# ℹ 390 more rows\n\n\nSince we’ve simulated the data, we know the latent values and the observed values, so we can plot our simulated data in order to get a better understanding of how much the censoring process will affect our estimates.\n\n\nPlotting code\nsim_data |&gt;\n    ggplot() +\n    geom_hline(\n        yintercept = c(sim_parms$y_min, sim_parms$y_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_vline(\n        xintercept = c(sim_parms$x_min, sim_parms$x_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_segment(\n        data = subset(sim_data, (x != x_star) | (y != y_star)),\n        aes(x = x_star, xend = x, y = y_star, yend = y),\n        color = \"gray\",\n        alpha = 0.25,\n        lwd = 1\n    ) +\n    geom_point(aes(x = x_star, y = y_star), color = \"gray\") +\n    geom_point(aes(x = x, y = y)) +\n    coord_cartesian(\n        xlim = c(-3, 3),\n        ylim = c(-22, 22)\n    ) +\n    labs(\n        x = \"Independent variable\",\n        y = \"Dependent variable\"\n    )\n\n\n\n\n\nWe can see that a substantial amount of the data points are censored. In total, \\(16.5\\%\\) of records were censored in \\(x\\) only, \\(13.25\\%\\) of records were censored in \\(y\\) only, and \\(5\\%\\) of records were censored in both \\(x\\) and \\(y\\). Thus, \\(24.75\\%\\) of records were censored in some way.\nI also deliberately set the upper and lower limits for both \\(x\\) and \\(y\\) to be asymmetrical so we can more clearly see how our censoring process can strongly bias the estimates: we have more records censored at lower values than higher values, which gives us a shifted window where we observe data.\nSo now that we have the data simulated, we want to try to recover the original parameters with a Bayesian model."
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#stan-data-setup",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#stan-data-setup",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.2 Stan data setup",
    "text": "4.2 Stan data setup\nI also want to write the Stan code to accept data in a specific format that we want to test. The data should be formatted like the table below.\n\n\n\nX\nX_L\nX_U\nY\nY_L\nY_U\n\n\n\n\n\\(x_1\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_1\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(x_2\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_2\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_n\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_n\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\nHere, \\(x_\\min\\) is the lower limit of detection for \\(x\\) and \\(x_\\max\\) is the upper limit of detection for \\(X\\) (and similar for \\(Y\\)). Eventually, if this is the data format we decide to permanently adopt going forward, we will want to write a suite of helper functions to conveniently get the data in this form. But for now I will do it manually. Fortunately it is quite easy. And if the censoring limits changed for any observations, it would have been easier to store the data in this format in the first place.\n\nstan_data &lt;-\n    sim_data |&gt;\n    dplyr::select(x, y) |&gt;\n    dplyr::mutate(\n        x_l = sim_parms$x_min,\n        x_u = sim_parms$x_max,\n        .after = x\n    ) |&gt;\n    dplyr::mutate(\n        y_l = sim_parms$y_min,\n        y_u = sim_parms$y_max,\n        .after = y\n    )\n\nstan_data |&gt; print(n = 5)\n\n# A tibble: 400 × 6\n        x   x_l   x_u     y   y_l   y_u\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.80      -1     2  7.08    -9    12\n2  1.16      -1     2  6.71    -9    12\n3  0.155     -1     2  6.05    -9    12\n4  0.0988    -1     2  1.24    -9    12\n5 -1         -1     2 -7.35    -9    12\n# ℹ 395 more rows\n\n\nNow we just need to convert the data frame to a list format and add a variable for the number of records.\n\nstan_list &lt;- as.list(stan_data)\nstan_list$N &lt;- nrow(stan_data)\nstr(stan_list)\n\nList of 7\n $ x  : num [1:400] 1.7973 1.1599 0.1547 0.0988 -1 ...\n $ x_l: num [1:400] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ x_u: num [1:400] 2 2 2 2 2 2 2 2 2 2 ...\n $ y  : num [1:400] 7.08 6.71 6.05 1.24 -7.35 ...\n $ y_l: num [1:400] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 ...\n $ y_u: num [1:400] 12 12 12 12 12 12 12 12 12 12 ...\n $ N  : int 400"
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#stan-code",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#stan-code",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.3 Stan code",
    "text": "4.3 Stan code\nOf course as usual we need to compile the Stan code. The code is also included here for reference.\n\npth &lt;- here::here(\"Ex3.stan\")\nmod &lt;- cmdstanr::cmdstan_model(pth, compile = FALSE)\nmod$compile(force_recompile = TRUE)\n\n\n\n\n\n\n\nModel code\n\n\n\n\n\n//\n\t// Example 3 Stan Code: censored outcome and censored predictor\n\t// where both the outcome and predictor have lower and upper limits of\n\t// detection (that are known and constant).\n\t// Zane 2023-10-18\n//\n\n// Input data for the model\ndata {\n\tint&lt;lower=0&gt; N;\n\tarray[N] real y;\n\tarray[N] real y_l;\n\tarray[N] real y_u;\n\tarray[N] real x;\n\tarray[N] real x_l;\n\tarray[N] real x_u;\n}\n\n// The parameters accepted by the model.\nparameters {\n\treal alpha;\n\treal beta;\n\treal&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated.\nmodel {\n\t// Define mu vector\n\tvector[N] mu;\n\t\n\t// Priors go here\n\tsigma ~ exponential(1);\n\tbeta ~ normal(0, 2);\n\talpha ~ normal(0, 2);\n\t\n\tfor (i in 1:N) {\n\t\t// Cases for dealing with censored predictor x based on how it is censored\n\t\tif (x[i] &lt;= x_l[i]) {\n\t\t\t// If x is left-censored, use the CDF and add contribution to target\n\t\t\ttarget += normal_lcdf(x_l[i] | 0, 1);\n\t\t} else if (x[i] &gt; x_u[i]) {\n\t\t\t// If x is right-censored, use the complimentary CDF to add to target\n\t\t\ttarget += normal_lccdf(x_u[i] | 0, 1);\n\t\t} else {\n\t\t\t// If x is observed, update evrything like normal\n\t\t\tx[i] ~ normal(0, 1);\n\t\t}\n\t\t\n\t\t// Calculate mu now that x is dealt with\n\t\tmu[i] = alpha + beta * x[i];\n\t\t\n\t\t// Dealing with the outcome likelihood\n\t\t// if Y is below the lower bound, integrate with lcdf\n\t\tif (y[i] &lt;= y_l[i]) {\n\t\t\ttarget += normal_lcdf(y_l[i] | mu[i], sigma);\n\t\t\t// If Y is above the upper bound, integrate with lccdf\n\t\t} else if (y[i] &gt; y_u[i]) {\n\t\t\ttarget += normal_lccdf(y_u[i] | mu[i], sigma);\n\t\t\t// If Y is in the middle of the censoring bounds, update the likelihood\n\t\t\t// like normal.\n\t\t} else {\n\t\t\ty[i] ~ normal(mu[i], sigma);\n\t\t}\n\t}\n}\n\n// End of program"
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#model-fitting-and-performance",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#model-fitting-and-performance",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.4 Model fitting and performance",
    "text": "4.4 Model fitting and performance\nNow that the model is successfully compiled, we need to generate MCMC samples from the posterior distribution. We’ll use 4 chains (run in parallel) with 500 warmup iterations and 2500 sampling iterations each, for a total of 10000 samples overall, which should be plenty for this problem. Otherwise, we’ll leave the control parameters at their default values.\n\nfit &lt;- mod$sample(\n    stan_list,\n    seed = 123123,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = FALSE\n)\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/model-1e7428801b98.stan', line 62, column 3 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nAs usual, we want to check the diagnostics, and fortunately cmdstanr gives us an easy to use diagnostic flagger.\n\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-1-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-2-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-3-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-4-126c21.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nWe can examine the trace plots and posterior distributions of the parameters of interest to confirm that there is no funny business.\n\npost &lt;- posterior::as_draws_array(fit)\nbayesplot::mcmc_combo(post, par = c(\"alpha\", \"beta\", \"sigma\"))\n\n\n\n\nAnd so now we can finally examine the fitted values and compare them to our true simulation values.\n\nfit$summary() |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n0.43\n0.43\n0.26\n0.26\n0.00\n0.86\n1\n8190.67\n7220.56\n\n\nbeta\n4.14\n4.14\n0.29\n0.29\n3.66\n4.63\n1\n9034.90\n7285.48\n\n\nsigma\n5.10\n5.10\n0.19\n0.19\n4.80\n5.42\n1\n9738.63\n7383.51\n\n\n\n\n\nWe can see that our model estimated the slope and variance quite well, although it is not doing too great at figuring out the intercept. In fact, the true value of \\(\\alpha = 1\\) isn’t even in the credible interval. However, the estimates for \\(\\beta\\) and \\(\\sigma\\) are very close to the true estimates. In most applications, the intercept is not too useful and the slope is what we want an accurate estimate of anyway, so this is probably acceptable.\nTODO figure out what else needs to go in this example."
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-with-one-patient-group",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-with-one-patient-group",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.1 Model with one patient group",
    "text": "5.1 Model with one patient group\nFor the first model, we’ll consider a simpler case where we only have one group of patients. This model would be appropriate for, e.g., an observational study where all patients are given the vaccine of interest.\nThe data generating model is as follows:\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p\\right) \\\\\n\\mathrm{logit}(p) &= \\alpha + \\beta \\cdot x_i\n\\end{align*}\n\\] where \\(y_i\\) is a binary outcome where 1 indicates infection and 0 indicates no infection, and \\(x_i\\) is our antibody titer. However, the specific problem we deal with in practice is that these antibody titers tend to have lower limits of detection. Thus, we need to add an observation model to our data generating process to reflect the incomplete observations we obtain of \\(x\\).\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p_i\\right) \\\\\n\\mathrm{logit}(p_i) &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right) \\\\\nx_i &= \\begin{cases}\n\\frac{1}{2}\\mathrm{LoD}, & x^*_i &lt; \\mathrm{LoD} \\\\\nx^*_i, & x^*_i \\geq \\mathrm{LoD}\n\\end{cases} \\\\\n\\end{align*}\n\\] Here, we assume that we work with the \\(x\\) variable on the log scale at all times,\nmostly cause it’s annoying and confusing to write out all the logs every time, so we could also write \\[x_i^* = \\mathrm{log} \\left(z^*_i\\right)\\] and say \\(z^*_i\\) is the actual latent un-logged titer.\nNow that we have the data generating process written out, we can simulate some example data. Note that in this example, we can interpret \\(\\alpha\\) as the log-odds of infection if a person were to have no antibodies. For example, if we assume that this probability is \\(50\\%\\) we would apply the logit transformation to get that \\(\\alpha = 0\\). However, let’s assume that the inoculum dose is quite high and during our subject selection process we’ve included anyone who might have a genetic resistance to the disease (i.e., FUT2- individuals for norovirus). So let’s say if a person has zero antibodies, their probably of getting sick should be \\(90\\%\\). Then, \\(\\log(0.9 / 0.1) \\approx 2.2\\).\nWe then want our true \\(\\beta\\) value to be negative, indicating that as the number of antibodies rise, the log-odds of infection decrease. We can interpret \\(\\beta\\) as the change in the log-odds ratio associated with a one-unit change in antibody titer – the nonlinearity here makes it a bit more difficult to interpret this effect. We can, however, intercept \\(\\exp(\\beta)\\) as the odds ratio between individuals with titer \\(x_i + 1\\) and individuals with titer \\(x_i\\). This corresponds to a nonlinear change in risk that depends on the value of \\(x_i\\). However, if we want the odds of infection to halve for each 1 unit increase in antibody titer, we would set \\(\\beta = -\\log(2) \\approx -0.7\\).\n\nset.seed(134125)\nsim_parms &lt;- list(\n    n = 110,\n    alpha = 2.2,\n    beta = -1.37,\n    mu_x = 2,\n    sigma_x = 2,\n    LoD = 0\n)\n\n\ninv_logit &lt;- function(x) {return(1 / (1 + exp(-x)))}\n\nsim_one_group &lt;- function(n, alpha, beta, mu_x, sigma_x, LoD) {\n    out &lt;- tibble::tibble(\n        x_star = rnorm(n, mu_x, sigma_x),\n        x = ifelse(x_star &lt; LoD, 0.5 * LoD, x_star),\n        p = inv_logit(alpha + beta * x_star),\n        y = rbinom(n, size = 1, prob = p)\n    )\n}\n\nsim_data &lt;- do.call(sim_one_group, sim_parms)\n\nOf course visualizing the relationship between a binary outcome and a continuous predictor is in some sense more complex than visualizing the relationship between a continuous outcome and a continuous predictor.\nFirst, let’s look at how the distribution of the predictor variable changes if we condition on the outcome.\n\nsim_data |&gt;\n    tidyr::pivot_longer(cols = c(x, x_star)) |&gt;\n    dplyr::mutate(\n        yf = factor(\n            y,\n            levels = c(0, 1),\n            labels = c(\"Not infected\", \"Infected\")\n        ),\n        name = factor(\n            name,\n            levels = c(\"x_star\", \"x\"),\n            labels = c(\"Latent variable\", \"Observed variable\")\n        )\n    ) |&gt;\n    ggplot() +\n    aes(x = value, fill = yf) +\n    geom_vline(\n        xintercept = 0,\n        linetype = \"dashed\",\n        color = \"black\",\n        linewidth = 1\n    ) +\n    geom_histogram(\n        binwidth = 0.5, boundary = 0, closed = \"left\",\n        position = \"identity\", alpha = 0.6,\n        color = \"black\"\n    ) +\n    scale_x_continuous(\n        name = \"Simulated log titer\",\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_y_continuous(breaks = scales::breaks_pretty()) +\n    scale_fill_brewer(palette = \"Dark2\", name = NULL) +\n    facet_wrap(facets = vars(name))\n\n\n\n\nEssentially everything below our lower threshold gets bumped up, which will make the summary statistics of the distribution more similar between groups for the observed titer values than they would have been for the latent titer values. However, we can still see a large difference.\n\ninterp &lt;-\n    tibble::tibble(\n        value = seq(-2, 6, 0.1),\n        p = inv_logit(sim_parms$alpha + sim_parms$beta * value)\n    )\n\ninterp2 &lt;-\n    dplyr::bind_rows(\n        \"Latent variable\" = interp,\n        \"Observed variable\" = interp,\n        .id = \"name\"\n    )\n\nlab1 &lt;- latex2exp::TeX(r\"($Pr(y_{i} = 1 \\ | \\ x_{i})$)\")\n\nsim_data |&gt;\n    tidyr::pivot_longer(cols = c(x, x_star)) |&gt;\n    dplyr::mutate(\n        yf = factor(\n            y,\n            levels = c(0, 1),\n            labels = c(\"Not infected\", \"Infected\")\n        ),\n        name = factor(\n            name,\n            levels = c(\"x_star\", \"x\"),\n            labels = c(\"Latent variable\", \"Observed variable\")\n        )\n    ) |&gt;\n    ggplot() +\n    aes(x = value, color = name) +\n    geom_line(\n        data = interp2, aes(y = p), color = \"darkgray\", linetype = 2, linewidth = 1\n    ) +\n    geom_point(\n        aes(y = y), size = 3, alpha = 0.5,\n        position = position_jitter(width = 0, height = 0.05, seed = 370)\n    ) +\n    scale_x_continuous(\n        name = \"Simulated log titer\",\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_y_continuous(\n        name = lab1,\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_color_brewer(palette = \"Accent\", name = NULL) +\n    facet_wrap(facets = vars(name))\n\n\n\n\n\ndata_list &lt;-\n    sim_data |&gt;\n    dplyr::mutate(x_l = sim_parms$LoD) |&gt;\n    dplyr::select(x, x_l, y) |&gt;\n    as.list()\n\ndata_list$N &lt;- sim_parms$n\n\nstr(data_list)\n\nList of 4\n $ x  : num [1:110] 0.123 1.872 0.932 1.865 1.925 ...\n $ x_l: num [1:110] 0 0 0 0 0 0 0 0 0 0 ...\n $ y  : int [1:110] 1 0 1 0 0 0 1 0 0 0 ...\n $ N  : num 110\n\n\n````{.stan, include = “Ex4a.stan”}\n\n::: {.cell}\n\n```{.r .cell-code}\nmod &lt;- cmdstanr::cmdstan_model(here::here(\"Ex4a.stan\"), compile = FALSE)\nmod$compile(force_recompile = TRUE, pedantic = TRUE)\n\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f10756f6390.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function&lt;typename unary_traits&lt;Predicate&gt;::argument_type,bool&gt;\n      |                                             ^~~~~~~~~~~~~~\n\n\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function&lt;\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template&lt;cl\n\n\nass _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\n\n\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function&lt;T*, S&gt;\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function&lt;T*, A, S&gt;\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function&lt;const T*, S&gt;\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function&lt;const T*, A, S&gt;\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: \n\n\ndeclared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function&lt;T&, S&gt;\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;T&, A, S&gt;\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function&lt;const T&, S&gt;\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;const T&, A, S&gt;\n      |                                        \n\n\n                            ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function&lt;Arg,Result&gt;\n      |                                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function&lt;Arg1,Arg2,Result&gt;\n      |                                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = Ex4a_model_namespace::Ex4a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f10756f6390.hpp:392: note:   by 'Ex4a_model_namespace::Ex4a_model::write_array'\n  392 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex4a_model_namespace::Ex4a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f10756f6390.hpp:392: note:   by 'Ex4a_model_namespace::Ex4a_model::write_array'\n  392 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n:::\n\nfit &lt;- mod$sample(\n    data_list,\n    seed = 25452345,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f10756f6390.stan', line 59, column 3 to column 32)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.9 seconds.\n\n\n\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/Ex4a-202310312225-1-879365.csv, C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/Ex4a-202310312225-2-879365.csv, C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/Ex4a-202310312225-3-879365.csv, C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/Ex4a-202310312225-4-879365.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\n\nfit$summary()\n\n# A tibble: 5 × 10\n  variable    mean  median    sd   mad      q5      q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 lp__     -190.   -189.   1.44  1.21  -192.   -188.     1.00    4408.    5589.\n2 alpha       1.91    1.90 0.411 0.416    1.26    2.60   1.00    5508.    5501.\n3 beta       -1.23   -1.22 0.226 0.226   -1.62   -0.881  1.00    5323.    5512.\n4 mu_x        1.84    1.85 0.234 0.231    1.45    2.22   1.00    8886.    6064.\n5 sigma_x     2.32    2.31 0.177 0.174    2.05    2.63   1.00    9161.    6999."
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#effect-of-vaccine",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#effect-of-vaccine",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.2 Effect of vaccine",
    "text": "5.2 Effect of vaccine\nOf course, a more interesting question is when we have \\(k\\) different treatment groups. These groups could be vaccine and placebo, like the example that motivated this project, or they could be multiple different vaccine candidates, doses, etc. So we now need to incorporate the effect of the treatment into the model. However, we know that the treatment will have a direct effect on \\(x\\), the antibody titers, and we can add a direct effect on \\(y\\) to represent the combined effect of the vaccine on other facets of the immune system (e.g. cell-mediated responses) which explain variations in infection risk that are not due to antibodies.\nIn this framework, \\(x\\) becomes a mediator of the relationship between \\(t\\), the treatment, and \\(y\\). For simplicity, we model the effect of \\(t\\) on \\(x\\), and the effect of \\(t\\) and \\(x\\) jointly on \\(y\\), both as linear functions of the predictors. Specifically, the data generating model is given as follows.\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p_i\\right) \\\\\n\\mathrm{logit}(p_i) &= \\beta_{1, T[i]} + \\beta_{2, T[i]} \\cdot x^*_i \\\\\n\\log\\left(x_i^*\\right) &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right) \\\\\n\\mu_x &= \\alpha_{T[i]} \\\\\nx_i &= \\begin{cases}\n\\frac{1}{2}\\mathrm{LoD}, & x^*_i &lt; \\mathrm{LoD} \\\\\nx^*_i, & x^*_i \\geq \\mathrm{LoD}\n\\end{cases} \\\\\nT[i] &= \\begin{cases}\n1, & \\text{individual } i \\text{ is in the placebo group} \\\\\n2, & \\text{individual } i \\text{ is in the vaccine group}\n\\end{cases}\n\\end{align*}\n\\]\nGiven the generative model, we can simulate data which follow our assumptions.\n\nset.seed(341341)\n# Some parameters are commented out because I originally had a global\n# intercept for mu and for p, but then the intercept parameters are\n# nonidentifiable under index coding as written.\nsim2_parms &lt;- list(\n    n = 116,\n    #a0 = 2,\n    a1 = c(2.5, 4),\n    #b0 = 1.5,\n    b1 = c(1.7, 2.2),\n    b2 = c(-0.67, -1.37),\n    sigma_x = 1.5,\n    LoD = 3,\n    latent = TRUE\n)\nsim_two_groups &lt;- function(n, b1, b2, a1, sigma_x, LoD,\n                                                     latent = TRUE) {\n    out &lt;- tibble::tibble(\n        # Randomly assign each individual to 1 (placebo) or 2 (vaccine)\n        t = rbinom(n, size = 1, prob = 0.5) + 1,\n        mu = a1[t],\n        x_star = rnorm(n, mu, sigma_x),\n        x = dplyr::if_else(x_star &lt; LoD, 0.5 * LoD, x_star),\n        p = inv_logit(b1[t] + b2[t] * x_star),\n        y = rbinom(n, 1, prob = p)\n    )\n    \n    # If the arg 'latent' is specified as anything other than FALSE, return the\n    # latent variables that we don't observe. Otherwise return only (X, y).\n    if (isFALSE(latent)) {\n        out &lt;- out |&gt; dplyr::select(t, x, y)\n    }\n    \n    return(out)\n}\nsim_data_4b &lt;- do.call(sim_two_groups, sim2_parms)\n\n\ntab_dat &lt;-\n    sim_data_4b |&gt;\n    dplyr::mutate(\n        t = factor(\n            t,\n            levels = c(2, 1),\n            labels = c(\"Vaccine\", \"Placebo\")\n        ),\n        y = factor(\n            y,\n            levels = c(1, 0),\n            labels = c(\"Infected\", \"Not infected\")\n        )\n    )\ntab_dat |&gt;\n    gtsummary::tbl_cross(\n        row = t, col = y,\n        label = list(t ~ \"Treatment\", y ~ \"Outcome\")\n    )\n\n\n\n\n\n  \n    \n    \n      \n      \n        Outcome\n      \n      Total\n    \n    \n      Infected\n      Not infected\n    \n  \n  \n    Treatment\n\n\n\n        Vaccine\n5\n49\n54\n        Placebo\n28\n34\n62\n    Total\n33\n83\n116\n  \n  \n  \n\n\n\n\nBecause the data are from a (hypothetical) clinical trial, the typical epidemiological approach to data analysis, if we do not care about the effect of the mediator \\(x\\) would be to calculate the risk ratio.\n\ntab &lt;- table(tab_dat$t, tab_dat$y, dnn = c(\"Treatment\", \"Outcome\"))\nepiR_out &lt;- epiR::epi.2by2(\n    tab,\n    method = \"cohort.count\"\n)\nepiR_out\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +            5           49         54       9.26 (3.08 to 20.30)\nExposed -           28           34         62     45.16 (32.48 to 58.32)\nTotal               33           83        116     28.45 (20.46 to 37.57)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.21 (0.09, 0.49)\nInc odds ratio                                 0.12 (0.04, 0.35)\nAttrib risk in the exposed *                   -35.90 (-50.50, -21.30)\nAttrib fraction in the exposed (%)            -387.74 (-1074.55, -102.54)\nAttrib risk in the population *                -16.71 (-31.57, -1.85)\nAttrib fraction in the population (%)         -58.75 (-89.23, -33.18)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 18.276 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n\n# This one takes the variables in the opposite direction so easier to do it\n# this way\nepiDisplay::csi(\n    caseexp = tab[[1]],\n    controlex = tab[[3]],\n    casenonex = tab[[2]],\n    controlnonex = tab[[4]]\n)\n\n\n          Exposure\nOutcome    Non-exposed Exposed Total\n  Negative 34          49      83   \n  Positive 28          5       33   \n  Total    62          54      116  \n                                    \n           Rne         Re      Rt   \n  Risk     0.45        0.09    0.28 \n                                         Estimate Lower95ci Upper95ci\n Risk difference (Re - Rne)              -0.36    -0.5      -0.2     \n Risk ratio                              0.21     0.1       0.45     \n Protective efficacy =(Rne-Re)/Rne*100   79.5     55.46     90.1     \n   or percent of risk reduced                                        \n Number needed to treat (NNT)            2.79     2         4.99     \n   or -1/(risk difference)                                           \n\n\nSo if we didn’t care about the effect of \\(x\\) at all, we would conclude that the vaccine appears to be protective with a RR of \\(`r round(epiR_out\\)massoc.summary[[1, 2]], 2)$ and a 95% CI of $\\left(r round(epiR_out\\(massoc.summary[[1, 3]], 2) - round(epiR_out\\)massoc.summary[[1, 4]], 2)`)$. Note that this analysis is marginal to the censored \\(x_i\\) values, and since the data generating process for \\(y_i\\) relies on the latent \\(x^*_i\\) values, this analysis should not be biased by the censoring process.\nHowever, in our study we specifically want to know how much of the lower risk is explained by the antibody titer, and how much is not. This analysis is more complicated, and requires us to use a regression model. Fortunately we know the data generating process, so writing the Stan code for an accurate model is not too hard.\n\n````{.stan, include = “Ex4b.stan”}\n\n::: {.cell}\n\n```{.r .cell-code}\npth &lt;- here::here(\"Ex4b.stan\")\nmod4b &lt;- cmdstanr::cmdstan_model(pth, compile = F)\nmod4b$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f1070fa5285.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function&lt;typename unary_traits&lt;Predicate&gt;::argument_type,bool&gt;\n      |                                             ^~~~~~~~~~~~~~\n\n\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function&lt;\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template&lt;cl\n\n\nass _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function&lt;T*, S&gt;\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function&lt;T*, A, S&gt;\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function&lt;const T*, S&gt;\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: \n\n\n'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function&lt;const T*, A, S&gt;\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function&lt;T&, S&gt;\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;T&, A, S&gt;\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function&lt;const T&, S&gt;\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n\n\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;const T&, A, S&gt;\n      |                                                                    ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function&lt;Arg,Result&gt;\n      |                                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function&lt;Arg1,Arg2,Result&gt;\n      |                                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = Ex4b_model_namespace::Ex4b_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f1070fa5285.hpp:492: note:   by 'Ex4b_model_namespace::Ex4b_model::write_array'\n  492 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex4b_model_namespace::Ex4b_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f1070fa5285.hpp:492: note:   by 'Ex4b_model_namespace::Ex4b_model::write_array'\n  492 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n:::\n\nmod4b_data &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = sim2_parms$LoD, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x, x_l) |&gt;\n    as.list()\n\nmod4b_data &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data$t)),\n    mod4b_data\n)\nstr(mod4b_data)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 1.5 3.19 1.5 3.79 4.7 ...\n $ x_l: num [1:116] 3 3 3 3 3 3 3 3 3 3 ...\n\npaste0(\n    \"Naruto checked the data and he says:\\n\",\n    round(mean(mod4b_data$x &lt;= mod4b_data$x_l), 4) * 100,\n    \"% of x values are below the LoD!\\nBelieve it!\"\n) |&gt; cat()\n\nNaruto checked the data and he says:\n46.55% of x values are below the LoD!\nBelieve it!\n\n\n\nfit4b &lt;- mod4b$sample(\n    mod4b_data,\n    seed = 5234521,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f1070fa5285.stan', line 75, column 3 to column 50)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpm0Bkna/model-5f1070fa5285.stan', line 75, column 3 to column 50)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 finished in 1.8 seconds.\nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 finished in 1.9 seconds.\nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 5.2 seconds.\nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 5.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.6 seconds.\nTotal execution time: 5.6 seconds.\n\n\n\nfit4b$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad       q5       q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;     &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__       -157.    -157.    1.95  1.85  -161.    -154.      1.00    4215.\n2 alpha_1[1]    2.45     2.46  0.292 0.284    1.95     2.90    1.00    8685.\n3 alpha_1[2]    3.85     3.85  0.254 0.256    3.42     4.26    1.00    9349.\n4 sigma_x       1.75     1.74  0.181 0.175    1.48     2.07    1.00    8116.\n5 beta_1[1]     0.760    0.758 0.530 0.536   -0.102    1.65    1.00    6824.\n6 beta_1[2]     1.45     1.40  1.17  1.15    -0.351    3.47    1.00    5524.\n7 beta_2[1]    -0.397   -0.392 0.197 0.197   -0.732   -0.0818  1.00    6880.\n8 beta_2[2]    -1.69    -1.61  0.666 0.640   -2.94    -0.752   1.00    5384.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary &lt;- fit4b$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npo &lt;-\n    ggplot(fit_summary) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated with observed (censored) values with correction\"\n    )"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-if-x-was-not-censored",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-if-x-was-not-censored",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.3 Model if x was not censored",
    "text": "5.3 Model if x was not censored\n\nmod4b_data_l &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = -9999, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x = x_star, x_l) |&gt;\n    as.list()\n\nmod4b_data_l &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data_l$t)),\n    mod4b_data_l\n)\nstr(mod4b_data_l)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 2.07 3.19 2.2 3.79 4.7 ...\n $ x_l: num [1:116] -9999 -9999 -9999 -9999 -9999 ...\n\n\n\nfit4b_l &lt;- mod4b$sample(\n    mod4b_data_l,\n    seed = 5234521,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 1.6 seconds.\nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 finished in 1.7 seconds.\nChain 3 finished in 1.7 seconds.\nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 1.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 1.9 seconds.\n\n\n\nfit4b_l$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__       -166.    -165.    1.87  1.73  -169.    -163.     1.00    4661.\n2 alpha_1[1]    2.63     2.64  0.204 0.203    2.30     2.97   1.00   11142.\n3 alpha_1[2]    3.90     3.91  0.214 0.213    3.55     4.25   1.00   12564.\n4 sigma_x       1.61     1.61  0.108 0.108    1.44     1.79   1.00   10927.\n5 beta_1[1]     1.50     1.48  0.599 0.597    0.545    2.51   1.00    7405.\n6 beta_1[2]     1.21     1.19  1.09  1.09    -0.530    3.06   1.00    7679.\n7 beta_2[1]    -0.673   -0.661 0.213 0.216   -1.04    -0.342  1.00    7502.\n8 beta_2[2]    -1.26    -1.22  0.449 0.446   -2.06    -0.603  1.00    7520.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary_l &lt;- fit4b_l$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npl &lt;-\n    ggplot(fit_summary_l) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated using true latent values\"\n    )"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#do-it-the-naive-way",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#do-it-the-naive-way",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.4 Do it the naive way",
    "text": "5.4 Do it the naive way\n\nmod4b_data_n &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = -9999, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x = x, x_l) |&gt;\n    as.list()\n\nmod4b_data_n &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data_n$t)),\n    mod4b_data_n\n)\nstr(mod4b_data_n)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 1.5 3.19 1.5 3.79 4.7 ...\n $ x_l: num [1:116] -9999 -9999 -9999 -9999 -9999 ...\n\n\n\nfit4b_n &lt;- mod4b$sample(\n    mod4b_data_n,\n    seed = 873215,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.7 seconds.\nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 1.7 seconds.\nChain 3 finished in 1.7 seconds.\nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 1.9 seconds.\n\n\n\nfit4b_n$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad        q5       q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__       -170.    -169.    1.89  1.73  -173.     -167.      1.00    4136.\n2 alpha_1[1]    2.53     2.53  0.209 0.204    2.19      2.87    1.00   10825.\n3 alpha_1[2]    3.75     3.75  0.217 0.215    3.39      4.11    1.00   10929.\n4 sigma_x       1.63     1.62  0.107 0.106    1.46      1.81    1.00   10108.\n5 beta_1[1]     0.776    0.768 0.521 0.522   -0.0606    1.64    1.00    6724.\n6 beta_1[2]     1.45     1.40  1.17  1.13    -0.390     3.46    1.00    6954.\n7 beta_2[1]    -0.404   -0.396 0.194 0.191   -0.731    -0.0970  1.00    6755.\n8 beta_2[2]    -1.69    -1.61  0.666 0.649   -2.92     -0.748   1.00    6854.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary_n &lt;- fit4b_n$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npn &lt;-\n    ggplot(fit_summary_n) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated using censored values without censoring correction\"\n    )\n\n\npo / pl / pn\n\n\n\n\n\nall_fits &lt;-\n    dplyr::bind_rows(\n        \"corrected\" = fit_summary,\n        \"latent\" = fit_summary_l,\n        \"naive\" = fit_summary_n,\n        .id = \"model\"\n    )\n\nall_fits |&gt;\n    ggplot() +\n    aes(\n        x = variable, y = median, ymin = q5, ymax = q95,\n        color = model\n    ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_crossbar(\n        aes(y = truth, ymin = truth, ymax = truth),\n        width = 0.5,\n        color = \"black\",\n        fatten = 0.1\n    ) +\n    geom_pointrange(position = position_dodge2(width = 0.3)) +\n    scale_color_brewer(palette = \"Dark2\") +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; line marks true value\"\n    )"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-exploration",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#model-exploration",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.5 Model exploration",
    "text": "5.5 Model exploration\nthese curves are implied by the model and parameters, not by the simulated data ::: {.cell}\nx_vec &lt;- seq(-6, 12, 0.01)\nr1 &lt;- sapply(x_vec, \\(x) inv_logit(1.5 + 0.2 - 0.67 * x))\nr2 &lt;- sapply(x_vec, \\(x) inv_logit(1.5 + 0.7 - 1.37 * x))\n\nlayout(matrix(c(1, 2, 3, 3), ncol = 2, byrow = TRUE))\nplot(x_vec, r2 - r1, ylab = \"risk difference\", type = \"l\", xlab = \"\")\nabline(h = 0, lty = 2)\nplot(x_vec, r2 / r1, ylab = \"risk ratio\", type = \"l\", xlab = \"\")\nabline(h = 1, lty = 2)\nlab2 &lt;- latex2exp::TeX(r\"($Pr(y_{i} = 1 \\ | \\ x_{i}, T_{i})$)\")\nplot(\n    NULL, NULL,\n    ylim = c(0, 1),\n    xlim = c(-6, 12),\n    yaxs = \"i\",\n    xaxs = \"i\",\n    xlab = \"Simulated log titer\",\n    ylab = lab2\n)\nlines(x_vec, r1, lty = 2, lwd = 1.5) # placebo\nlines(x_vec, r2, lty = 1, lwd = 1.5) # vaccine\n\n\n\n# IDK what's wrong with the legend, seems it doesn't like layout.\n# switch to ggplot to fix\n#legend(x = 9, y = 0.8, c('Unexposed', 'Exposed'), lty = c(2, 1), lwd = 2)\n:::\n\nx_dens &lt;-\n    tibble::tibble(\n        Latent = x_vec,\n        Observed = dplyr::if_else(\n            x_vec &lt; sim2_parms$LoD,\n            sim2_parms$LoD,\n            x_vec\n        ),\n        Placebo = sim2_parms$a1[1],\n        Vaccine = sim2_parms$a1[2]\n    ) |&gt;\n    tidyr::pivot_longer(\n        cols = c(Placebo, Vaccine),\n        names_to = \"t\",\n        values_to = \"mu\"\n    ) |&gt;\n    tidyr::pivot_longer(\n        cols = c(Latent, Observed),\n        names_to = \"o\",\n        values_to = \"x\"\n    ) |&gt;\n    dplyr::mutate(\n        d = dplyr::if_else(\n            o == \"Latent\",\n            dnorm(x, mean = mu, sd = sim2_parms$sigma_x),\n            crch::dcnorm(\n                x, mean = mu, sd = sim2_parms$sigma_x,\n                left = sim2_parms$LoD, right = Inf\n            )\n        )\n    )\n\nanno_df &lt;-\n    x_dens[1:4, ] |&gt;\n    dplyr::filter(o == \"Observed\")\n\nx_dens |&gt;\n    ggplot() +\n    aes(x = x, y = d, linetype = t, group = t) +\n    geom_vline(\n        xintercept = sim2_parms$LoD,\n        linetype = 1, linewidth = 1, color = \"gray\"\n    ) +\n    geom_line(linewidth = 1.5) +\n    geom_point(\n        data = anno_df,\n        size = 2,\n        stroke = 2,\n        shape = 21,\n        color = \"black\",\n        fill = \"darkgray\"\n    ) +\n    facet_grid(vars(o), vars(t)) +\n    scale_linetype_discrete(name = NULL) +\n    scale_x_continuous(breaks = scales::breaks_pretty()) +\n    scale_y_continuous(breaks = scales::breaks_pretty()) +\n    labs(\n        x = \"Simulated log titer\",\n        y = \"Implied probability density\"\n    ) +\n    #coord_cartesian(expand = FALSE, ylim = c(-0.01, 0.28)) +\n    theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#try-gamma-dist.-for-x-on-non-logged-scale",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#try-gamma-dist.-for-x-on-non-logged-scale",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.6 Try gamma dist. for x on non-logged scale?",
    "text": "5.6 Try gamma dist. for x on non-logged scale?"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html#do-we-want-to-work-out-a-hierarchical-model",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html#do-we-want-to-work-out-a-hierarchical-model",
    "title": "5  Example Model 4: Logistic regression with censored predictors",
    "section": "5.7 Do we want to work out a hierarchical model?",
    "text": "5.7 Do we want to work out a hierarchical model?"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#data-generating-process",
    "href": "Ex5-Interval-Censoring.html#data-generating-process",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.1 Data generating process",
    "text": "6.1 Data generating process\nFor this model, let’s assume that the rate of failure, \\(\\lambda_i\\) is a linear function of some covariate \\(x_i\\). I don’t know that much about machines or what would realistically cause them to fail, but we don’t want to make the example too hard at this point, so we want \\(x_i\\) to be an inherent characteristic of the machine. Let’s say \\(x_i\\) is some integer number from \\(1\\) to \\(5\\) that controls how the machine works – it is inherent to the type of machine. If I can think of a good variable that might work for this, I’ll update that later. The expected failure time increases linearly by some amount \\(\\beta\\) for each unit of increase in \\(x_i\\). There is also some baseline expected failure rate \\(\\alpha\\) shared by all of the machines. So the expected log failure time (if the failure time were constant) can be given as \\[\n\\log(\\lambda_i) = \\alpha + \\beta x_i.\n\\] We use the log link function here to ensure that expected failure times are always positive, while the function of \\(x_i\\) does not necessarily need to be.\nFinally, we assume that the longer each machine operates without being repaired, the more likely the machine is to fail. We represent this by modeling the failure time as a Weibull distribution with constant parameter \\(k\\), which influences how quickly the failure rate changes over time. We assume this parameter is the same for all machines, which are identical other than the setting \\(x_i\\).\nSo then if our failure times were completely observed, the data generating process would be as follows.\n\\[\n\\begin{align*}\nt^*_i &\\sim \\mathrm{Weibull}\\left(k, \\lambda_i\\right) \\\\\n\\lambda_i &= \\alpha + \\beta x_i\n\\end{align*}\n\\]\nBut recall what I said before about running inspections only once a week. If this were the case, assuming our failure times are also measured in weeks, we would also have to apply a censoring mechanism for the data we observe, given by \\[t_i = \\lceil t_i^* \\rceil.\\]\nWe will deal with the interval censoring, as usual, by modifying the likelihood of the outcome. For a completely observed outcome \\(t^*_i\\), the likelihood would be \\[\n\\mathcal{L}\\left(\\theta \\mid t_i^*\\right) = f(t_i^* \\mid \\theta),\n\\] where \\(f(\\cdot)\\) is the Weibull density function. However, a censored data point actually lies at a point mass of probability and for our censored observation, which we recall is integer-valued for this DGP, the contribution to the likelihood is \\[\n\\mathcal{L}\\left(\\theta \\mid t_i\\right) = \\mathrm{Pr}\\left(\nL_i&lt; t_i \\leq U_i\n\\right) = \\int_{L_i}^{U_i}f_{T_i}(\\tau) \\ d\\tau = F_{T_i}(U_i) - F_{T_i}(L_i),\n\\] where \\(L_i = \\lfloor t_i^*\\rfloor = t_i - 1\\) and \\(U_i = \\lceil t_i^* \\rceil = t_i\\), both of which are assumed to be known constants after the data are observed.\nThe Stan code for this would be {.stan} target += log_diff_exp(     weibull_lcdf(y[i] | k, lambda[i]),     weibull_lcdf(y[i] - 1 | k, lambda[i]) ) ``` or equivalently target += log_diff_exp( weibull_lcdf(y_u[i] | k, lambda[i]), weibull_lcdf(y_l[i] | k, lambda[i]) )\nif the data is specified in the format we prefer. Note that we use the Stan\ninternal function `log_diff_exp()` for increased numerical precision rather than\ndividing the log values of the two functions manually. Note than there are[some\nconcerns](https://discourse.mc-stan.org/t/interval-censored-data-fails-with-weibull-but-not-gamma/28780)\nabout the numerical stability of the Weibull CDF function implemented in Stan,\nwith a [GitHub issue](https://github.com/stan-dev/math/issues/2810) for\nimproving the stability open at time of writing. For improved numerical\nstability, we can rewrite the `weibull_lcdf()` function using Stan's newer math\nfunctions with improved stability.\n````{.stan}\nfunctions {\n  real my_weibull_lcdf(real y, real alpha, real sigma) {\n    return log1m_exp(-pow(y / sigma, alpha));\n  }\n}"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#data-simulation",
    "href": "Ex5-Interval-Censoring.html#data-simulation",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.2 Data simulation",
    "text": "6.2 Data simulation\nSo with the data generating process, including the censoring mechanism, written down, we can simulate some data. As usual, I decided to just plot the data and mess around with the parameters until I thought it looked right.\n\nset.seed(2384590)\nsim_parms &lt;- list(\n    n = 210,\n    k = 1.5,\n    alpha = 2,\n    beta = -0.35\n)\n\nstr(sim_parms)\n\nList of 4\n $ n    : num 210\n $ k    : num 1.5\n $ alpha: num 2\n $ beta : num -0.35\n\n\n\ngen_data &lt;- function(n, k, alpha, beta) {\n    out &lt;- tibble::tibble(\n        x = sample(\n            1:5, size = n, replace = TRUE,\n            prob = c(0.4, 0.25, 0.2, 0.1, 0.05)\n        ),\n        l_lambda = alpha + beta * x,\n        lambda = exp(l_lambda),\n        t_star = rweibull(n, shape = k, scale = lambda),\n        t = ceiling(t_star)\n    )\n    \n    return(out)\n}\n\nsim_data &lt;- do.call(gen_data, sim_parms)\nprint(sim_data, n = 5)\n\n# A tibble: 210 × 5\n      x l_lambda lambda t_star     t\n  &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     4     0.6    1.82  0.516     1\n2     3     0.95   2.59  3.32      4\n3     4     0.6    1.82  2.06      3\n4     1     1.65   5.21 11.0      11\n5     1     1.65   5.21  9.95     10\n# ℹ 205 more rows\n\nplot(jitter(sim_data$x), sim_data$t)\n\n\n\nsim_data |&gt; dplyr::group_by(x) |&gt; dplyr::summarise(eft = mean(t))\n\n# A tibble: 5 × 2\n      x   eft\n  &lt;int&gt; &lt;dbl&gt;\n1     1  5.82\n2     2  4.26\n3     3  3.29\n4     4  2.19\n5     5  1.75\n\n\n\nsim_data |&gt;\n    dplyr::mutate(\n        x_jitter = x + rnorm(nrow(sim_data), 0, 0.1)\n    ) |&gt;\n    ggplot2::ggplot() +\n    aes(x = x_jitter, y = t_star, group = (x)) +\n    geom_point() +\n    geom_segment(\n        aes(x = x_jitter, xend = x, y = t_star, yend = t)\n    )\n\n\n\n    geom_count(shape = 21, fill = \"#ffffff50\")\n\ngeom_point: na.rm = FALSE\nstat_sum: na.rm = FALSE\nposition_identity \n\n\nNEED TO LOOK AT HOW BRMS HANDLES INTERVAL CENSORING https://discourse.mc-stan.org/t/mixed-right-left-and-interval-censored-log-normal-with-brms/27571"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#fitting-latent-data",
    "href": "Ex5-Interval-Censoring.html#fitting-latent-data",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.3 Fitting latent data",
    "text": "6.3 Fitting latent data\n\ndat_latent &lt;- list()\ndat_latent$N &lt;- nrow(sim_data)\ndat_latent$x &lt;- sim_data$x\ndat_latent$y &lt;- sim_data$t_star\n\nstr(dat_latent)\n\nList of 3\n $ N: int 210\n $ x: int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y: num [1:210] 0.516 3.321 2.061 10.966 9.951 ...\n\n\n\nmod_l &lt;- cmdstanr::cmdstan_model(here::here(\"Ex5a.stan\"), compile = FALSE)\nmod_l$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function&lt;typename unary_traits&lt;Predicate&gt;::argument_type,bool&gt;\n      |                                             ^~~~~~~~~~~~~~\n\n\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function&lt;\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template&lt;cl\n\n\nass _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function&lt;T*, S&gt;\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function&lt;T*, A, S&gt;\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function&lt;const T*, S&gt;\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: \n\n\n'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function&lt;const T*, A, S&gt;\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function&lt;T&, S&gt;\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;T&, A, S&gt;\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function&lt;const T&, S&gt;\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n\n\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;const T&, A, S&gt;\n      |                                                                    ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function&lt;Arg,Result&gt;\n      |                                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function&lt;Arg1,Arg2,Result&gt;\n      |                                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = Ex5a_model_namespace::Ex5a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.hpp:350: note:   by 'Ex5a_model_namespace::Ex5a_model::write_array'\n  350 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex5a_model_namespace::Ex5a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.hpp:350: note:   by 'Ex5a_model_namespace::Ex5a_model::write_array'\n  350 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n\n\nfit_l &lt;- mod_l$sample(\n    dat_latent,\n    seed = 546465,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 finished in 1.5 seconds.\nChain 3 finished in 1.4 seconds.\nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.7 seconds.\nChain 4 finished in 1.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.6 seconds.\nTotal execution time: 1.9 seconds.\n\n\n\nfit_l$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -453.    -453.    1.21   0.970  -456.    -452.     1.00    4128.\n2 alpha       2.14     2.14  0.0997 0.101     1.98     2.30   1.00    3284.\n3 beta       -0.360   -0.360 0.0392 0.0396   -0.424   -0.296  1.00    3247.\n4 k           1.45     1.45  0.0782 0.0786    1.32     1.58   1.00    6020.\n# ℹ 1 more variable: ess_tail &lt;num&gt;"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#naive-method-use-the-same-model-for-censored-data",
    "href": "Ex5-Interval-Censoring.html#naive-method-use-the-same-model-for-censored-data",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.4 Naive method – use the same model for censored data",
    "text": "6.4 Naive method – use the same model for censored data\n\ndat_naive &lt;- dat_latent\ndat_naive$y &lt;- sim_data$t\nstr(dat_naive)\n\nList of 3\n $ N: int 210\n $ x: int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y: num [1:210] 1 4 3 11 10 1 3 4 2 4 ...\n\n\n\nfit_n &lt;- mod_l$sample(\n    dat_naive,\n    seed = 546465,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768229a3e45.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 1.6 seconds.\nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 finished in 1.7 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.8 seconds.\nChain 3 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 2.0 seconds.\n\n\n\nfit_n$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -461.    -460.    1.24   1.02   -463.    -459.     1.00    3681.\n2 alpha       2.19     2.19  0.0816 0.0820    2.06     2.33   1.00    4263.\n3 beta       -0.307   -0.307 0.0319 0.0316   -0.360   -0.254  1.00    4313.\n4 k           1.76     1.76  0.0948 0.0935    1.61     1.92   1.00    5299.\n# ℹ 1 more variable: ess_tail &lt;num&gt;"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#fitting-censored-data",
    "href": "Ex5-Interval-Censoring.html#fitting-censored-data",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.5 Fitting censored data",
    "text": "6.5 Fitting censored data\nIn this parametrization, the outcome variable \\(t_i\\)Note that this measurement process will generate an integer valued response. However, we know that if the value of \\(t_i = t\\), then in reality \\(t_i \\in (t - 1, t]\\), and we can never know the true value of \\(t_i\\) because we don’t do inspections more often.\n\ndat_censored &lt;- list()\ndat_censored$N &lt;- nrow(sim_data)\ndat_censored$x &lt;- sim_data$x\ndat_censored$y1 &lt;- sim_data$t - 1\ndat_censored$y2 &lt;- sim_data$t\n\nstr(dat_censored)\n\nList of 4\n $ N : int 210\n $ x : int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y1: num [1:210] 0 3 2 10 9 0 2 3 1 3 ...\n $ y2: num [1:210] 1 4 3 11 10 1 3 4 2 4 ...\n\n\n\nmod_c &lt;- cmdstanr::cmdstan_model(here::here(\"Ex5b.stan\"), compile = FALSE)\nmod_c$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768143f7857.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function&lt;typename unary_traits&lt;Predicate&gt;::argument_type,bool&gt;\n      |                                             ^~~~~~~~~~~~~~\n\n\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function&lt;\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template&lt;cl\n\n\nass _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function&lt;\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function&lt;T*, S&gt;\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function&lt;T*, A, S&gt;\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function&lt;const T*, S&gt;\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: \n\n\n'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function&lt;const T*, A, S&gt;\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function&lt;T&, S&gt;\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;T&, A, S&gt;\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function&lt;const T&, S&gt;\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n\n\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function&lt;const T&, A, S&gt;\n      |                                                                    ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template&lt;class _Arg, class _Result&gt; struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function&lt;Arg,Result&gt;\n      |                                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template&lt;class _Arg1, class _Arg2, class _Result&gt; struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function&lt;Arg1,Arg2,Result&gt;\n      |                                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = Ex5b_model_namespace::Ex5b_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768143f7857.hpp:408: note:   by 'Ex5b_model_namespace::Ex5b_model::write_array'\n  408 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex5b_model_namespace::Ex5b_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/RtmpuIYSLd/model-6768143f7857.hpp:408: note:   by 'Ex5b_model_namespace::Ex5b_model::write_array'\n  408 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n\n\nfit_c &lt;- mod_c$sample(\n    dat_censored,\n    seed = 3248315,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Rejecting initial value:\n\n\nChain 1   Log probability evaluates to log(0), i.e. negative infinity.\n\n\nChain 1   Stan can't start sampling from this initial value.\n\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 finished in 3.2 seconds.\nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 3.7 seconds.\nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 4.0 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 4.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.7 seconds.\nTotal execution time: 4.3 seconds.\n\n\n\nfit_c$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -458.    -457.    1.26   1.02   -460.    -456.     1.00    4046.\n2 alpha       2.12     2.12  0.101  0.0977    1.96     2.29   1.00    3599.\n3 beta       -0.355   -0.354 0.0401 0.0399   -0.421   -0.288  1.00    3669.\n4 k           1.44     1.44  0.0874 0.0859    1.30     1.59   1.00    5322.\n# ℹ 1 more variable: ess_tail &lt;num&gt;"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#making-the-interval-wider",
    "href": "Ex5-Interval-Censoring.html#making-the-interval-wider",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.6 Making the interval wider",
    "text": "6.6 Making the interval wider\nWe can introduce more"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html#midpoint-correction",
    "href": "Ex5-Interval-Censoring.html#midpoint-correction",
    "title": "6  Example Model 5: Interval Censoring",
    "section": "6.7 Midpoint correction",
    "text": "6.7 Midpoint correction\n\n6.7.1 Original interval\n\n\n6.7.2 Wider interval"
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html",
    "href": "Ex6-HAI-data-outcome.html",
    "title": "7  Example Model 6: the HAI manifesto",
    "section": "",
    "text": "Most of my research about the influenza vaccine is focused on the analysis of Hemagglutination Inhibition (HAI) assay data. These data are proxies for neutralizing influenza antibodies, and have the advantage of being easy to perform and being able to capture cross-reactivity across different strains of influenza. However, the assay has a unique data generating process. I’ve discussed this before in a few posts on my blog ( first, second), but I never got to the part where we actually fit the models to make sure our censoring approach works.\nHAI makes a potentially interesting example because of three features of the data:\n\nThe outcome is left censored with a lower limit of detection;\nThe outcome is interval censored for values above the LLOD; and\nThere are two appropriate models, which we can compare and contrast.\n\nFor all of these models, we’ll work under one general example study. Suppose we recruit \\(n\\) patients for a study and give them a flu vaccine. For each of those patients, we collect their pre-vaccination titer and post-vaccination (say at 28 days) to 11 different influenza strains. These strains include the same strain that is in the vaccine (which has distance \\(0\\)), and ten other strains, which each have distances \\(d = \\{0.1, 0.2, 0.3, \\ldots, 1.0\\}\\). The outcome variable, post-vaccination titers, will be left-censored and also interval-censored. We assume that the mean of the outcome is a linear function of the distance. Unfortunately the two models we need to consider are hard to describe in the same language, so we’ll get down to brass tacks with the data generating process (DGP) in the respective sections.\nFor dealing with interval censored data, we will pass the data to Stan using the interval2 format from the survival package. In this format, we pass two columns, say y_l and y_u. If a data point is not censored, then y_l == y_u. If a data point is left-censored, then y_l = -Inf and y_u is set to the lower limit of detection. Similarly, if a data point is right-censored, y_l is set to the upper limit of detection and y_u = Inf. Finally, if a data point is interval-censored, y_l and y_u are set to the lower and upper bounds of the interval containing the datum, respectively.\n\n8 Log-normal model\nBoth of these models will share a lot of similarities, but for some reason I find the lognormal model easier to think about, so we’ll do most of the explaining under the context of this model. We start with the assumption that our underyling, latent titer value (the “true titer”) is some real number which follows a normal distribution. We assume that the mean of this distribution is a linear function of the antigenic distance, as we mentioned, and we assume constant variance. In math terms, we assume that \\[\ny_i^* \\sim \\mathrm{lognormal}\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e}\\right).\n\\] Note that we scale the coefficients by a factor of \\(\\frac{1}{\\log_2{e}}\\) because \\[\\log{y_i^*} \\sim \\mathrm{Normal}\\left( \\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e} \\right),\\] which combined with the identity that \\[\\log{y_i^*} = \\frac{\\log_2 y_i^*}{\\log_2{e}},\\] implies \\[ \\log_2 y_i^* \\sim \\mathrm{Normal}\\left( \\beta_0 + \\beta_1 x_i, \\sigma \\right) .\\] This gives us a connection to the natural units of measurement, and makes it easier to understand the models and set priors.\n\n\n9 Gamma model\n\n\n10 Midpoint correction instead of interval censoring\n\n\n11 Extensions to consider in the future\n\nAn individual is randomly missing strains due to laboratory errors\nRepeated measures across study years\nRepeated measures across study years where some strains are systematically missing based on the year\nModels which control for pre-vaccination titer."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Stan Development Team. 2023. Stan Modeling Language\nUsers’ Guide and Reference Manual.\n2.33 ed."
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html#log-normal-model",
    "href": "Ex6-HAI-data-outcome.html#log-normal-model",
    "title": "7  Example Model 6: the HAI manifesto",
    "section": "7.1 Log-normal model",
    "text": "7.1 Log-normal model\nBoth of these models will share a lot of similarities, but for some reason I find the lognormal model easier to think about, so we’ll do most of the explaining under the context of this model. We start with the assumption that our underyling, latent titer value (the “true titer”) is some real number which follows a log-normal distribution. We assume that the mean of this distribution is a linear function of the antigenic distance, as we mentioned, and we assume constant variance. In math terms, we assume that \\[\ny_i^* \\sim \\text{Log-normal}\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e}\\right).\n\\] Note that we scale the coefficients by a factor of \\(\\frac{1}{\\log_2{e}}\\) because \\[\\log{y_i^*} \\sim \\text{Normal}\\left( \\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e} \\right),\\] which combined with the identity that \\[\\log{y_i^*} = \\frac{\\log_2 y_i^*}{\\log_2{e}},\\] implies \\[ \\log_2 y_i^* \\sim \\text{Normal}\\left( \\beta_0 + \\beta_1 x_i, \\sigma \\right) .\\] This gives us a connection to the natural units of measurement, and makes it easier to understand the models and set priors."
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html#gamma-model",
    "href": "Ex6-HAI-data-outcome.html#gamma-model",
    "title": "7  Example Model 6: the HAI manifesto",
    "section": "7.2 Gamma model",
    "text": "7.2 Gamma model\nADD A CALLOUT BOX HERE\n\nGamma distribution parametrization\nThere are at least three common reparametrizations of the Gamma parametrization. Here, we follow the convention used by Stan, which is different from the parametrization implemented by Richard McElreath in the rethinking package. Note that in the common \\(\\alpha, \\beta\\) parametrization that we adopt, one can easily model the mean by implementing the probability model as \\[\ny_i \\sim \\text{Gamma}\\left(\\alpha_i = \\frac{\\mu_i}{\\beta}, \\beta\\right).\n\\] So, if you were curious, that is where the \\(\\frac{1}{\\beta}\\) comes from in the following section."
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html#midpoint-correction-instead-of-interval-censoring",
    "href": "Ex6-HAI-data-outcome.html#midpoint-correction-instead-of-interval-censoring",
    "title": "7  Example Model 6: the HAI manifesto",
    "section": "7.3 Midpoint correction instead of interval censoring",
    "text": "7.3 Midpoint correction instead of interval censoring"
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html#extensions-to-consider-in-the-future",
    "href": "Ex6-HAI-data-outcome.html#extensions-to-consider-in-the-future",
    "title": "7  Example Model 6: the HAI manifesto",
    "section": "7.4 Extensions to consider in the future",
    "text": "7.4 Extensions to consider in the future\n\nAn individual is randomly missing strains due to laboratory errors\nRepeated measures across study years\nRepeated measures across study years where some strains are systematically missing based on the year\nModels which control for pre-vaccination titer."
  }
]
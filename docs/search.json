[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian censoring",
    "section": "",
    "text": "Preface\nThis book is where we can store our thoughts and codes on dealing with censoring data in Bayesian models. See the README to find where everything is and how to contribute.\nContributors:\n\nZane Billings (https://wzbillings.com/)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  What are censored data?",
    "section": "",
    "text": "Censoring is a selection phenomenon on data which occurs when we can only obtain precise measurements for certain measurement values. The classical example is a scale which can only measure weights up to a certain threshold, say \\(y_{\\mathrm{max}}\\). A scale that can measure any weight would produce the set of measurements \\(\\{y_1^*, y_2^*, \\ldots, y_n^*\\}\\) where \\(n\\) is the sample size. We call these the latent values or true values. Our imperfect scale would then produce the observed values, \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\leq y_{\\mathrm{max}} \\\\\ny_{\\mathrm{max}}, & y_i^* &gt; y_{\\mathrm{max}}\n\\end{cases}; \\quad i = 1, \\ldots, n.\n\\] Specifically, this is an example of right censoring, where there is an upper limit of detection, or maximum value that we can observe with precision.\nWe can also have the opposite case, where we can detect any theoretical measurement above a certain value. In this case, the data are said to have a lower limit of detection and this phenomenon is called left censoring. For example, imagine we are testing the concentration of lead in the tap water of several buildings. Our test cannot detect lead levels below 5 parts per billion (ppb), but can detect any larger amount of lead. In that case, our observed values would instead look like this: \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\] where \\(y_{\\mathrm{min}}\\) is the lower limit of detection.\n\\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\]\nFinally, we can have interval censoring, where we know a data value is within some interval, but we do not know precisely where the value lies within that interval. An example of this is antibody titer dilutions: for flu HAI titer, the values are typically reported as 10, 20, 40, etc., but a value of 10 does not mean the precise value of the measurement should be 10, it means the true value is between 10 and 20. If we assume our titer is measured on the log scale and has no limits of detection, we could write \\[\ny_i = \\lfloor y_i^* \\rfloor; \\quad i = 1, \\ldots, n,\n\\] because we only perform a discrete number of dilutions. This gives us the interval value for \\(y_i\\) as \\[y_i \\in \\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right).\\]\nA given variable can be subject to all of these types of censoring simultaneously: for example, HAI titers are interval censored in this way, but they also have lower limits of detection and upper limits of detection as well (though the upper limits are rarely important in practice because they can be arbitrarily increased during the assay). However, a particular observation of this variable can only be subject to one type of censoring at a time: e.g., if an observation is below the detection limit, that value is left censored, it cannot simultaneously be right censored or interval censored.\nNotably, the distinction between “types” of censoring in this way is useful for several analytic methods, but is not strictly necessary. All censored values can be implicitly treated as interval censored data, where the lower endpoint for a left censored value is negative infinity, and the upper endpoint for a right censored value is positive infinity. Thus, we could write the data generating process for HAI titers with LOD as \\[\ny_i = \\begin{cases}\ny_{\\text{min}}, \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\lfloor y_i^* \\rfloor, \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\ny_{\\text{max}}, y \\leq y_{\\text{max}}\n\\end{cases},\n\\] where \\(y_{\\text{min}}\\) is the lower limit of detection and \\(y_{\\text{max}}\\) is the upper limit of detection. To express the DGP in interval notation, we would write \\[\ny_i \\in \\begin{cases}\n\\left(-\\infty, y_{\\text{min}}\\right), \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right), \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\n\\left[y_{\\text{max}}, \\infty\\right), \\ y \\leq y_{\\text{max}}\n\\end{cases}.\n\\] Note also that assuming \\(y^*_i\\) is drawn from an absolutely continuous distribution (e.g. normal or lognormal, etc.), the final likelihood model will be equivalent regardless of which intervals are open or closed. This model would allow us to put all of the censored observations into the likelihood function in the same framework without having to worry about sorting the observations into buckets w.r.t. the type of censoring.\nThe probability of each observation \\(y_i\\) can then be expressed as the probability that the random variable \\(Y\\) takes on a realization inside the given interval. If \\(F\\) is the CDF for some parametric distribution which we assume the latent variable \\(y_i^*\\) is drawn from, with parameter \\(\\theta\\), the contribution of \\(y_i\\) to the likelihood is then \\[\\mathcal{L}(\\theta \\mid Y_i) = F(\\text{upper limit of interval}) - F(\\text{lower limit of interval}).\\] If we call the lower limit of the interval for \\(y_i\\) \\(L_i\\) and the corresponding upper limit \\(U_i\\), we can write the likelihood of the sample as \\[\n\\mathcal{L}(\\theta \\mid \\mathbf{Y}) = \\prod_{i=1}^n \\left(\nF(y_i \\mid \\theta)\\bigg\\rvert_{y_i = L_i}^{U_i}\n\\right)^{C_i}\\bigg(f(y_i\\mid \\theta) \\bigg)^{C_i},\n\\] where \\(C_i\\) is the indicator variable for \\(y_i\\) being censored. Notably, for an uncensored observation the likelihood is equal to the density. However, for the typical types of HAI data that we see, all of the assay values are subject to the same censoring process, and thus we could neglect the density component.\nSo now the remaining issue is to specify \\(F\\), the CDF of the latent variables."
  },
  {
    "objectID": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "href": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "title": "2  Example Model 1: One censored predictor",
    "section": "2.1 The other method (Bjorn method)",
    "text": "2.1 The other method (Bjorn method)\n\nmod2 &lt;- cmdstanr::cmdstan_model(stan_file = here::here('Ex1b.stan'))\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmpq4W877/model-44107d12126b.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\n\ndat2 &lt;- list()\ndat2$y &lt;- df_stan$y\ndat2$x &lt;- df_stan$x\ndat2$x_cens &lt;- df_stan$cens\ndat2$N &lt;- length(dat2$y)\ndat2$DL &lt;- lod\n\n\nfit2 &lt;- mod2$sample(dat2, seed = 100, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpq4W877/model-44107d12126b.stan', line 64, column 3 to column 55)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 9.0 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 finished in 9.1 seconds.\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 9.4 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 9.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 9.3 seconds.\nTotal execution time: 9.7 seconds.\n\n\n\nfit2$summary() |&gt;\n    dplyr::filter(!startsWith(variable, \"x\")) |&gt;\n    print(n = Inf)\n\n# A tibble: 6 × 10\n  variable      mean      median      sd     mad       q5     q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;num&gt;       &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -3106.    -3106.      22.2    22.0    -3.14e+3 -3.07e3  1.00    1419.\n2 a            0.659     0.661    0.320   0.317   1.32e-1  1.18e0  1.00    2706.\n3 b            2.06      2.06     0.0555  0.0567  1.96e+0  2.15e0  1.00    2879.\n4 s            4.94      4.94     0.108   0.109   4.77e+0  5.12e0  1.00    6305.\n5 mu_x        -0.116    -0.00464  9.83    9.84   -1.62e+1  1.60e1  1.00    5353.\n6 sigma_x      0.502     0.353    0.498   0.353   2.82e-2  1.44e0  1.00    4783.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\n\npost &lt;- posterior::as_draws_array(fit2)"
  },
  {
    "objectID": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "href": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "title": "3  Example Model 2: One censored outcome",
    "section": "3.1 Lower limit of detection",
    "text": "3.1 Lower limit of detection\nFor the first example, we’ll work with an outcome that has a lower limit of detection. First we need to simulate the data, which means we need to write out a generative model for the data. We’ll randomly sample x for the purposes of generating data, but for the purposes of our model we’ll assume x_i is a completely observed covariate and thus is known and does not need a random component in the model.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\n\\mathrm{DL}, & y^*_i \\leq \\mathrm{DL} \\\\\ny^*_i, & y^*_i &gt; \\mathrm{DL}\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\ni &= 1, 2, \\ldots, n\n\\end{align*}\n\\] Here, DL is the Detection Limit, aka the lower limit of detection for the variable. Of course in our generative model, we have set \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\) to be fixed population parameters, but for Bayesian inference we would need to assign suitable priors. Let’s set the values and simulate our data. The parameters I set for this example are as follows.\n\n\n\nParameter\nValue\nMeaning\n\n\n\n\n\\(n\\)\n271\nSample size\n\n\n\\(\\alpha\\)\n72\nRegression intercept\n\n\n\\(\\beta\\)\n3\nRegression slope\n\n\n\\(\\sigma\\)\n5\nStandard deviation of outcome\n\n\n\\(\\mathrm{DL}\\)\n80\nLower limit of detection\n\n\n\nThe \\(x\\)-values were drawn from a uniform distribution on \\((0, 10)\\). Since we know the true population parameters for our simulation, we can plot the data to see the effect of the censoring process on our observed \\(y\\) values.\n\n\n\n\n\nIn this plot, the black data points show our observed data. For those observations where the \\(y\\) value was below the limit of detection and thus censored, the gray points show the true latent values, which we could have observed with a perfect measurement process. The gray line segments connect each latent measurement to its corresponding observed measurement.\nApproximatly \\(22.88\\%\\) of data points were below the limit of detection and were therefore censored. Of course in real life, we would only observe the black points (observed values), and the gray points would be unobservable to us. But for the purposes of understanding how to analyze censored data, visualizing how different the observed and latent datasets are is quite valuable and informative. Since the datasets look so different, we should not be surprised that our regression estimates would be incorrect if we treated all of the censored values as the same constant value, or ignored them entirely!\nSo, if our standard linear regression model that we know and love (even the Bayesian version) would give us incorrect estimates using any of these naive methods, how then are we to proceed? According to the Stan manual (Stan Development Team 2023, chap. 4), there are two main ways of handling the censoring in the outcome in our model. The first of these methods relies on imputation and the second on integration of the likelihood function and manual updating of the target likelihood in Stan. The imputation method is conceptually easier and less mathematically daunting, so we begin our treatment there.\n\n3.1.1 Imputation-type method\nThe first method for dealing with censored data treats the censored values as missing values where the latent value is constrained to fall within a specific range. For a normally distributed outcome, all values below the lower limit of detection are constrained to fall within \\((-\\infty, \\mathrm{DL})\\).\nREAD THAT PART OF RETHINKING AND EXPLAIN HOW MISSING DATA WORKS HERE!!!\nTo implement such a model in Stan, we need to pass in the number of observed and the number of censored values and the observed y-values in Stan. We then declare the censored \\(y\\)-values as a parameter in the Stan code, meaning they will be sampled from their constrained distribution during the fitting process, whereas the observed \\(y\\) values will be used to update the parameter estimates.\nFirst, let’s look at the Stan code for this model.\nSHOW THE STAN CODE HERE.\nSince the data need to be in kind of a clunky format to use this method, we first need to do some wrangle and get the data in the correct format for Stan.\n\ndat_2a &lt;- list()\nwith(\n    sim_data, {\n        dat_cens &lt;- subset(sim_data, cens)\n        dat_obs &lt;- subset(sim_data, !cens)\n        dat_2a$N_cens &lt;&lt;- nrow(dat_cens)\n        dat_2a$N_obs &lt;&lt;- nrow(dat_obs)\n        dat_2a$y_obs &lt;&lt;- dat_obs$y\n        dat_2a$x_obs &lt;&lt;- dat_obs$x\n        dat_2a$y_cens &lt;&lt;- dat_cens$y\n        dat_2a$x_cens &lt;&lt;- dat_cens$x\n        dat_2a$DL &lt;&lt;- as.integer(sim_parms$DL)\n    }\n)\n\nstr(dat_2a)\n\nList of 7\n $ N_cens: int 62\n $ N_obs : int 209\n $ y_obs : num [1:209] 92.6 104.2 86.2 88.7 94.9 ...\n $ x_obs : num [1:209] 8.98 9.31 3.42 8.5 8.14 ...\n $ y_cens: num [1:62] 80 80 80 80 80 80 80 80 80 80 ...\n $ x_cens: num [1:62] 0.227 0.555 1.168 2.853 2.434 ...\n $ DL    : int 80\n\n\nNow we can compile the Stan program (via cmdstanr as usual).\n\nmod_2a &lt;- cmdstanr::cmdstan_model(here::here(\"Ex2a.stan\"), compile = FALSE)\nmod_2a$compile(pedantic = TRUE, force_recompile = TRUE)\n\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/model-3bd42607a40.stan', line 56, column 21: Argument\n    0.01 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/model-3bd42607a40.stan', line 55, column 18: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/model-3bd42607a40.stan', line 54, column 19: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning: The parameter y_cens has no priors. This means either no prior is\n    provided, or the prior(s) depend on data variables. In the later case,\n    this may be a false positive.\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/model-3bd42607a40.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nAnd since the program compiles correctly, we can use Stan’s sampling algorithm to generate samples from the posterior distribution. We’ll run 4 chains in parallel with 500 warmup iterations and 5000 sampling iterations per chains, with all of the other control parameters (e.g. maximum treedepth and adaptive delta) left at the cmdstan defaults. This many samples is overkill for this problem, but it is also quite fast and thus we can do many samples just to be safe.\n\nfit_2a &lt;- mod_2a$sample(\n    dat_2a, seed = 100, parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 5000\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 1 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 1 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 1 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 2 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 2 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 2 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 3 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 3 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 4 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 4 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 1 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 1 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 1 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 2 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 2 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 2 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 3 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 3 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 4 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 4 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 1 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 1 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 2 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 3 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 4 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 4 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 4 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 1 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 1 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 2 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 2 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 3 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 4 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 1 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 1 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 2 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 3 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 1 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 1 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 2 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 3 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 4 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 1 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 2 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 2 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 3 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 3 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 4 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 4 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 1 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 2 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 3 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 3 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 4 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 4 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 4 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 1 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 1 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 2 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 3 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 4 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 4 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 1 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 1 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 2 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 2 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 3 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 4 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 4 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 1 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 1 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 2 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 2 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 3 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 3 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 3 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 4 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 4 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 1 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 1 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 3 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 4 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 4 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 1 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 2 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 2 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 3 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 3 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 4 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 4 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 1 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 1 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 2 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 2 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 3 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 3 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 4 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 4 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 4 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 1 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 1 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 2 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 3 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 4 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 1 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 1 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 2 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 3 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 4 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 4 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 1 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 1 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 2 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 2 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 3 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 3 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 4 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 1 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 1 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 2 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 2 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 3 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 4 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 4 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 4 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 1 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 2 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 4 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 4 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 4 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 1 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 1 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 2 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 3 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 3 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 4 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 4 finished in 2.4 seconds.\nChain 1 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 1 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 2 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 2 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 1 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 1 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 2 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 3 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 3 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 2 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 3 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 3 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 2 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 3 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 3 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 3 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 1 finished in 3.1 seconds.\nChain 3 finished in 3.1 seconds.\nChain 2 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 2 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 2 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 2 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 finished in 3.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.0 seconds.\nTotal execution time: 3.9 seconds.\n\n# Extract the posterior samples in a nicer format for later\npost_2a &lt;- posterior::as_draws_df(fit_2a)\n\nThe first thing we should do after sampling is check for any diagnostic warnings. We have access to all of the individual diagnostics, but fortunately cmdstan has a built-in diagnostic checker to flag any potential problems.\n\nfit_2a$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/Ex2a-202310180013-1-5f477a.csv, C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/Ex2a-202310180013-2-5f477a.csv, C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/Ex2a-202310180013-3-5f477a.csv, C:/Users/Zane/AppData/Local/Temp/RtmpMNlOx5/Ex2a-202310180013-4-5f477a.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nGreat, no issues with the sampling procedure, that is what we like to see. Let’s manually check the trace plots for our main three parameters of interest. (We could also check the plots for all of the imputed y-values, but these are unlikely to be interesting or useful, any problems should hopefully propagate through to the interesting parameters.)\n\nbayesplot::mcmc_combo(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nThose look like nice healthy trace plots, so with that combined with our diagnostic check, it seems that the chains mixed well and explored the posterior distribution. We can also check if those parameters were correlated.\n\nbayesplot::mcmc_pairs(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nWe see that the slope and intercept estimates were strongly correlated, which makes sense, and the sigma parameter was slightly correlated with both of those but not strongly with either. We can notice here that the histograms for \\(\\beta\\) and \\(\\sigma\\) are not quite centered at the true values, but they do have some probability mass at those true values. Let’s look at the median estimates and CIs from our samples.\n\npar_sum &lt;-\n    fit_2a$summary(variables = c(\"alpha\", \"beta\", \"sigma\"))\npar_sum |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n73.175358\n73.195200\n0.7612478\n0.7649475\n71.910180\n74.395245\n1.000667\n8533.648\n10108.17\n\n\nbeta\n2.809101\n2.806420\n0.1216903\n0.1226851\n2.614248\n3.010981\n1.000255\n9050.044\n11862.90\n\n\nsigma\n4.897496\n4.891605\n0.2447610\n0.2444807\n4.515708\n5.316280\n1.000053\n16448.745\n13836.85\n\n\n\n\n\nWe can also plot those along with the true values for reference.\n\n\nShow plot code (messy)\ntruth &lt;- tibble::tibble(\n    name = c(\"alpha\", \"beta\", \"sigma\"),\n    value = c(sim_parms$alpha, sim_parms$beta, sim_parms$sigma)\n)\n\nhd &lt;- post_2a |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::select(alpha, beta, sigma) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything())\n\nggplot() +\n    aes(x = value) +\n    geom_histogram(\n        data = subset(hd, name == \"alpha\"),\n        boundary = 0,\n        binwidth = 0.25,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"beta\"),\n        boundary = 0,\n        binwidth = 0.05,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"sigma\"),\n        boundary = 0,\n        binwidth = 0.1,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_vline(\n        data = truth,\n        aes(xintercept = value),\n        linetype = \"dashed\",\n        linewidth = 1,\n        color = \"red\"\n    ) +\n    facet_wrap(~name, scales = \"free\") +\n    labs(x = NULL)\n\n\n\n\n\nFrom these histograms, we can see that while there is a decent amount of samples close to the true values of alpha and beta, the posterior distributions are not centered around the true values. At the time of writing, I am not sure if that is a fixable problem or just something we have to deal with from having imperfectly observed data.\nWe can also do a check of how close the imputed \\(y\\) values were on average to the actual \\(y\\) values.\n\ny_cens_sum &lt;-\n    fit_2a$summary(variables = paste0('y_cens[', 1:dat_2a$N_cens, ']'))\n\ndat_comp &lt;-\n    sim_data |&gt;\n    subset(cens) |&gt;\n    dplyr::select(y_star) |&gt;\n    dplyr::bind_cols(y_cens_sum) |&gt;\n    dplyr::mutate(\n        col = dplyr::case_when(\n            (mean &gt;= y_star) & (q5 &lt;= y_star) ~ TRUE,\n            (mean &lt;= y_star) & (q95 &gt;= y_star) ~ TRUE,\n            TRUE ~ FALSE\n        )\n    )\n\n# ggplot(dat_comp) +\n#   aes(x = y_star, y = mean, ymin = q5, ymax = q95, color = col) +\n#       geom_abline(\n#           slope = 1, intercept = 0, linetype = 2, linewidth = 1,\n#                               alpha = 0.5\n#           ) +\n#   geom_errorbar(alpha = 0.25) +\n#   geom_point() +\n#   coord_fixed() +\n#   scale_color_manual(\n#       values = c(\"orange\", \"turquoise\"),\n#       name = \"CI crosses diagonal\"\n#   )\n\nggplot(dat_comp) +\n    aes(x = (y_star - mean)) +\n    geom_histogram(boundary = 0, binwidth = 1, color = \"black\", fill = \"gray\") +\n    scale_x_continuous(breaks = seq(-10, 10, 2), limits = c(-10, 10)) +\n    labs(\n        x = \"True value - mean estimated value\"\n    )\n\n\n\n\nTODO make this relative error instead to make it easier to understand.\n\n\n3.1.2 Integration-type method\n\n\n\n\n\nStan Development Team. 2023. Stan Modeling Language Users’ Guide and Reference Manual. 2.33 ed."
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html",
    "href": "Ex3-Censored-Outcome-and-Predictor.html",
    "title": "4  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html",
    "title": "5  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html",
    "href": "Ex5-Interval-Censoring.html",
    "title": "6  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html",
    "href": "Ex6-HAI-data-outcome.html",
    "title": "7  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Stan Development Team. 2023. Stan Modeling Language\nUsers’ Guide and Reference Manual.\n2.33 ed."
  }
]
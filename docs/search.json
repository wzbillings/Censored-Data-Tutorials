[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling with Censored Data",
    "section": "",
    "text": "Preface\nThis book contains our notes on dealing with censored data, including methods for dealing with both censored outcomes and predictors. Where possible, we try to include both frequentist and Bayesian models.\n\nThe short link to this website is: https://tinyurl.com/hg-cens.\nThe full link is currently: https://wzbillings.github.io/Censored-Data-Tutorials/.\nFor source code, go here.\nPlease submit bug reports, typos, and comments here.\n\nContributors:\n\nZane Billings (https://wzbillings.com/)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "front-end/intro.html#footnotes",
    "href": "front-end/intro.html#footnotes",
    "title": "1  What are censored data?",
    "section": "",
    "text": "to really get into the details of this, you need Lebesgue’s decomposition theorem which relies on measure theory. Writing out the likelihood in a more general form without the censoring indicators is also not possible without measure theory because it relies on the Radon-Nikodym theorem. If this is the part you want to learn about, you will have to learn it from someone other than me!↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are censored data?</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html",
    "href": "examples/simple-censored-outcome/index.html",
    "title": "2  One censored outcome",
    "section": "",
    "text": "2.1 Glyphosate data simulation\nIn this first example, since we’re trying to understand these models, we’ll simulate our data from a known generative model. This allows us to be confident that our model for censored data is actually helping us to recover the correct parameters. Once we’re more comfortable with the model, we can try to analyze some data with unknown generative processeses.\nGlyphosate is an organophosphonate pesticide originally marketed as Roundup by Monsanto. Roundup was quickly adopted for industrial agriculture, especially after the introduction of genetically modified Roundup-resistant crop species in the mid-90’s. Due to widespread agricultural use in the US, glyphosate is an increasingly common groundwater contaminant, with a Maximum Containment Level Goal of 0.7 parts per million in tap water set by the EPA.\nTo get our data, we’ll first randomly generate our covariates. Let’s assume that our town of interest is a square, so we can normalize all of the distances so that each side of the square has length 1. Then (again somewhat unrealistically) we’ll assume that home X and Y coordinates are independently drawn from a uniform distribution on \\((0, 1)\\). For convenience, we’ll place the only farm in town at \\((0.5, 0.5)\\) and calculate the distances. Then since the other two independent variables are binary, we’ll randomly sample those for each house, say with respective probabilities \\(0.2\\) and \\(0.4\\).\nset.seed(370)\nN &lt;- 147\ngly_preds &lt;-\n    tibble::tibble(\n        house_x_coord = runif(N, 0, 1),\n        house_y_coord = runif(N, 0, 1),\n        dist_from_farm = sqrt((house_x_coord - 0.5)^2 + (house_y_coord - 0.5)^2),\n        personal_gly_use = rbinom(N, 1, 0.2),\n        water_filter_use = rbinom(N, 1, 0.4)\n    )\nhead(gly_preds)\n\n# A tibble: 6 × 5\n  house_x_coord house_y_coord dist_from_farm personal_gly_use water_filter_use\n          &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;            &lt;int&gt;            &lt;int&gt;\n1        0.390         0.852           0.369                1                1\n2        0.0519        0.373           0.466                0                0\n3        0.194         0.472           0.307                0                1\n4        0.961         0.922           0.625                0                1\n5        0.747         0.576           0.259                0                0\n6        0.0105        0.0321          0.677                0                0\nNow we need to describe our data generating model for the outcome. Of course, I am just going to randomly pick some parameters and mess around until they look about right – we know that our glyphosate levels should be somewhere in the neighborhood of \\((0, 1.4)\\)-ish. We’ll use a linear model, which means we assume that: \\[\n\\begin{aligned}\n\\log(y_i^*) &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_{1, i} + \\beta_2 x_{2, i} + \\beta_3x_{3, i}.\n\\end{aligned}\n\\] Here, \\(y_i\\) is the concentration of glyphosate in tap water of house \\(i\\), \\[x_{1, i}\\] is the distance from house \\(i\\) to the farm, \\(x_{2, i}\\) is the personal glyphosate use variable, and \\(x_{3, i}\\) is the water filter use variable. So we need to pick all four of those \\(\\beta_p\\) coefficients and the value of \\(\\sigma^2\\), the residual variance, before we can simulate our glyphosate levels.\n# Make a table of true coefs for using later\ncoefs &lt;-\n    tibble::tribble(\n        ~term, ~estimate,\n        \"(Intercept)\", 0.8,\n        \"dist_from_farm\", -4,\n        \"personal_gly_use\", 0.5,\n        \"water_filter_use\", -0.4\n    )\nres_sd &lt;- 0.25\n\ngly_data &lt;-\n    gly_preds |&gt;\n    dplyr::mutate(\n        mu = 0.8 - 4 * dist_from_farm + 0.5 * personal_gly_use -\n            0.4 * water_filter_use,\n        y_star = exp(rnorm(N, mu, res_sd))\n    )\n\nLoD &lt;- 0.33\n\nsummary(gly_data)\n\n house_x_coord       house_y_coord      dist_from_farm    personal_gly_use\n Min.   :0.0003474   Min.   :0.004179   Min.   :0.04354   Min.   :0.0000  \n 1st Qu.:0.2796628   1st Qu.:0.243920   1st Qu.:0.25278   1st Qu.:0.0000  \n Median :0.5228575   Median :0.471412   Median :0.39295   Median :0.0000  \n Mean   :0.5094258   Mean   :0.484296   Mean   :0.36918   Mean   :0.2381  \n 3rd Qu.:0.7452386   3rd Qu.:0.731020   3rd Qu.:0.46563   3rd Qu.:0.0000  \n Max.   :0.9906258   Max.   :0.998614   Max.   :0.67715   Max.   :1.0000  \n water_filter_use       mu              y_star       \n Min.   :0.0000   Min.   :-2.0985   Min.   :0.09508  \n 1st Qu.:0.0000   1st Qu.:-1.1521   1st Qu.:0.31413  \n Median :0.0000   Median :-0.7472   Median :0.47621  \n Mean   :0.3878   Mean   :-0.7128   Mean   :0.62597  \n 3rd Qu.:1.0000   3rd Qu.:-0.2702   3rd Qu.:0.76989  \n Max.   :1.0000   Max.   : 1.1258   Max.   :3.71580\nYou can see from the above simulation code that the values I ended up choosing are as follows:\n\\[\n\\begin{aligned}\n\\log(y_i^*) &\\sim \\text{Normal}(\\mu_i, 0.5^2) \\\\\n\\mu_i &= -2.5 + 2\\cdot x_{1, i} + 0.5\\cdot x_{2, i} -0.4\\cdot x_{3, i}.\n\\end{aligned}\n\\] These values seemed to give a reasonable range of \\(y\\) values (on the natural scale), and have signs that made sense to me. The intercept represents the concentration of glyphosate expected in tap water for a person who lives on the farm, no personal glyphosate use, and no water filter, and is about \\(2.23\\) ppm, which is quite high and perhaps expected for the point source of the contaminant. With these parameters, we will have \\(28.6\\%\\) of data points below the limit of detection, which is not ideal (of course the ideal is zero percent), but not too bad either.\nOur censoring model looks like this:\n\\[\ny_i = \\begin{cases}\nL, & y_i^* &lt; 0.33 \\text{ ppm} \\\\\ny_i^*, & \\text{otherwise}\n\\end{cases}.\n\\] Our censoring indicator \\(C_i\\) will look like this:\n\\[\nc_i = \\begin{cases}\n1, & y_i^* &lt; 0.33 \\text{ ppm} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n\\] Let’s first apply the censoring to our data. We’ll arbitrarily choose \\(L = 0\\). Then we’ll take a look at the data we would actually observe.\n# Dataset including latent variables\n# Do the censoring\nL &lt;- 0.1\ngly_data_lnt &lt;-\n    gly_data |&gt;\n    dplyr::mutate(\n        # Create the censoring indicator\n        c = ifelse(y_star &lt;= LoD, 1, 0),\n        # Create the censored outcome\n        y = ifelse(c == 1, L, y_star)\n    )\n\n# Dataset including ONLY the observed variables\ngly_data_obs &lt;- gly_data_lnt |&gt;\n    dplyr::select(dist_from_farm, personal_gly_use, water_filter_use, c, y)\n\nhead(gly_data_obs)\n\n# A tibble: 6 × 5\n  dist_from_farm personal_gly_use water_filter_use     c     y\n           &lt;dbl&gt;            &lt;int&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          0.369                1                1     0 0.369\n2          0.466                0                0     0 0.371\n3          0.307                0                1     1 0.1  \n4          0.625                0                1     1 0.1  \n5          0.259                0                0     0 0.913\n6          0.677                0                0     1 0.1\nNote that setting \\(y_i = 0.1\\) if the value is censored is completely arbitrary. Many people will set it to a value like the LoD, or half the LoD, or some crazy thing with \\(\\sqrt{2}\\) in it, and then pretend those are the real values. All of these arbitrary values are equally bad. Let’s look at the distribution of the latent and observed values just to show this. In real life, we can’t see this, but this example should remind us that picking an arbitrary number is not very good.\n# Arrange the data correctly for plotting\ngly_data_lnt |&gt;\n    dplyr::select(\n        \"observed\" = y,\n        \"latent\" = y_star\n    ) |&gt;\n    tidyr::pivot_longer(dplyr::everything()) |&gt;\n    # Now make the plot\n    ggplot() +\n    aes(x = value) +\n    geom_histogram(\n        boundary = 0,\n        binwidth = 0.1,\n        color = \"black\",\n        fill = \"gray\"\n    ) +\n    facet_wrap(vars(name)) +\n    ggtitle(paste0(\"Censored values coded as \", L))\nLet’s also take a look at what the data would have looked like if we set, say \\(L = 0.33\\) (the LoD).\ngly_data_lnt |&gt;\n    # Set censored values to LoD\n    dplyr::mutate(y = ifelse(y == L, 0.33, y)) |&gt;\n    # Arrange the data for the plot\n    dplyr::select(\n        \"observed\" = y,\n        \"latent\" = y_star\n    ) |&gt;\n    tidyr::pivot_longer(dplyr::everything()) |&gt;\n    # Make the plot\n    ggplot() +\n    aes(x = value) +\n    geom_histogram(\n        boundary = 0,\n        binwidth = 0.1,\n        color = \"black\",\n        fill = \"gray\"\n    ) +\n    facet_wrap(vars(name)) +\n    ggtitle(\"Censored values coded as LoD\")\nWe can see how this histogram makes the censored values look like actual data (cat screaming emoji)!! Whereas the previous set of histograms with a spike at zero should signal that there is something strange going on in the data. So substitution can cause data to be misleading. For this reason, it can sometimes be useful for analysts to record values as “Nondetect” or “&lt; LoD” in the dataset (or some other kind of text indicating it is not a regular number), forcing the analyst to clean up the data before it can be statistically examined. The problem can be somewhat avoided if we include an explicit indicator of censoring in our data, like so.\ngly_data_lnt |&gt;\n        # Set censored values to LoD\n    dplyr::mutate(y = ifelse(y == L, 0.33, y)) |&gt;\n    dplyr::select(\n        \"observed\" = y,\n        \"latent\" = y_star,\n        c\n    ) |&gt;\n    tidyr::pivot_longer(-c) |&gt;\n    # Now make the plot\n    ggplot() +\n    aes(x = value, fill = factor(c)) +\n    geom_histogram(\n        boundary = 0,\n        binwidth = 0.1,\n        color = \"black\",\n        alpha = 0.5,\n        position = \"stack\"\n    ) +\n    facet_wrap(vars(name)) +\n    scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"), name  = \"Below LoD?\") +\n    ggtitle(paste0(\"Censored values coded as LoD\"))\ngly_data_lnt |&gt;\n    dplyr::select(\n        \"observed\" = y,\n        \"latent\" = y_star,\n        c\n    ) |&gt;\n    tidyr::pivot_longer(-c) |&gt;\n    # Now make the plot\n    ggplot() +\n    aes(x = value, fill = factor(c)) +\n    geom_histogram(\n        boundary = 0,\n        binwidth = 0.1,\n        color = \"black\",\n        alpha = 0.5,\n        position = \"stack\"\n    ) +\n    facet_wrap(vars(name)) +\n    scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"), name  = \"Below LoD?\") +\n    ggtitle(paste0(\"Censored values coded as \", L))\nHere, we can see that the histogram conflates part of the censored and non-censored values because of the binwidth we set. All that is to show, when there is a possibly of censored data, we should be extra careful as analysts to make sure we aren’t computing incorrect statistics.\nAs another instructive example, let us first attempt to estimate the mean and SD of the glyphosate concentrations. We know an unbiased estimate of marginal mean and CI (that is, the statistic if we ignore all of the x values), because we have the underlying latent values. So let’s estimate those first. (Because we have a normal distribution, we could probably get the analytical marginal mean assuming unknown \\(x_i, p\\) values, but we won’t do that here.) In R, we can quickly construct the Wald-type CI based on the t-distribution using the t.test() function.\nlatent_t_test &lt;-\n    gly_data_lnt$y_star |&gt;\n    # Remember we made a log-normal assumption so we take the log here\n    log() |&gt;\n    t.test() |&gt;\n    broom::tidy() |&gt;\n    # Re-exponentiate the results\n    dplyr::mutate(dplyr::across(c(estimate, conf.low, conf.high), exp))\nprint(latent_t_test)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.485     -12.5 1.05e-24       146    0.432     0.544 One Samp… two.sided\nNow, if we compute the same test using the observed values, we can see what happens.\nobserved_t_test &lt;-\n    gly_data_obs$y |&gt;\n    # Remember we made a log-normal assumption so we take the log here\n    log() |&gt;\n    t.test() |&gt;\n    broom::tidy() |&gt;\n    # Re-exponentiate the results\n    dplyr::mutate(dplyr::across(c(estimate, conf.low, conf.high), exp))\nprint(observed_t_test)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.391     -11.7 8.40e-23       146    0.334     0.458 One Samp… two.sided\nThe estimate is much lower! In fact, the 95% CI doesn’t even cover the latent estimate! Of course, we can arbitrarily change the estimate by recoding the censored values. If we bumped them up to the LoD, the estimate would go up and if we made them lower, it would go down.\nDespite knowing this, let’s see what happens in our linear model.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#glyphosate-data-simulation",
    "href": "examples/simple-censored-outcome/index.html#glyphosate-data-simulation",
    "title": "2  One censored outcome",
    "section": "",
    "text": "Source population. Our fictional source population will be \\(n\\) randomly sampled households from a fictional city, indexed from \\(i = 1, \\ldots, n\\). We only have one measurement per household. For the purposes of our study, we’ll assume that each of these houses has their own water supply (which is unrealistic but sufficient for a censored data tutorial).\nOutcome variable. For this example, our outcome (or dependent) variable is \\(y_i\\), the log concentration of glyphosate in parts per million (ppm) detected from the tap in household \\(i\\) using an analytical test.\nCensoring structure. Our investigators have decided to use the test described in this paper, which has a lower limit of detection of 2 µM. Given a molar mass of 169.07 g/Mol for glyphosate, we can convert this to an LoD of 0.33 ppm, just under half of the MCLG. We assume there is no upper LoD or interval censoring.\nIndependent variables. For the sake of this example, we will measure a few independent variables at the household level. Without doing any research on the actual patterns of glyphosate distribution in groundwater, we’ll assume that glyphosate concentration is affected by:\n\ndistance to the nearest agriculture site (km).\nwhether the home uses glyphosate-based pesticides for personal gardening.\nwhether the home has a water filter on the primary tap.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#naive-linear-models",
    "href": "examples/simple-censored-outcome/index.html#naive-linear-models",
    "title": "2  One censored outcome",
    "section": "2.2 Naive linear models",
    "text": "2.2 Naive linear models\nNow, if we were entrusted with a data set for analysis and had no idea it was censored, we would typically assume that values at the LoD are measured exactly. Though, as we discussed, some EDA might be suggestive, we will fit the ordinary linear regression model we described in the data generating process earlier (here, we are fortunate enough to know that this is an appropriate model, which is always untrue in the real world).\n\n# Fit the linear model\nnaive_lm &lt;-\n    lm(\n        log(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use,\n        data = gly_data_obs\n    )\n# Print the results in a little table\nnaive_lm |&gt;\n    broom::tidy(conf.int = TRUE) |&gt;\n    tibble::add_column(truth = coefs$estimate) |&gt;\n    dplyr::select(term, estimate, conf.low, conf.high, truth) |&gt;\n    knitr::kable(digits = 3)\n\n\n\n\nterm\nestimate\nconf.low\nconf.high\ntruth\n\n\n\n\n(Intercept)\n0.967\n0.732\n1.202\n0.8\n\n\ndist_from_farm\n-5.135\n-5.687\n-4.582\n-4.0\n\n\npersonal_gly_use\n0.767\n0.578\n0.955\n0.5\n\n\nwater_filter_use\n-0.496\n-0.661\n-0.331\n-0.4\n\n\n\n\n\nHere in the table we can see that all of the estimates are biased away from the null, which is exactly what we don’t want in this kind of study – we would, if anything, prefer that they be biased towards the null so we avoid overstating the effect. Importantly, we can see that the CI’s for dist_from_farm and personal_gly_use do not even contain the true value! So even though we know the exact data generating process, and we know our model reflects that, if we don’t account for censoring, we can get completely wrong estimates!\nSo, then, what are we to do?",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#integration-method-for-censored-data",
    "href": "examples/simple-censored-outcome/index.html#integration-method-for-censored-data",
    "title": "2  One censored outcome",
    "section": "2.3 Integration method for censored data",
    "text": "2.3 Integration method for censored data\nWe can regain some measure of our lost diginity using the integration trick we discussed in the introduction. Of course, in the introduction, we only talked about adjusting for censoring in the univariate case, but fortunately we are modeling the conditional distribution of \\(y\\) so we can use the same trick:\n\\[\n\\begin{aligned}\n\\mathcal{L}\\left(\\theta \\mid y_i, x_i \\right) &= f_{Y_i \\mid X_i = x_i}(y_i \\mid \\theta, x_i) \\\\\n&= \\bigg( f(y_i \\mid \\theta, x) \\bigg)^{1 - c_i} \\bigg( P(Y_i = y_i \\mid x)\\bigg)^{c_i} \\\\\n&= \\bigg( f(y_i \\mid \\theta, x_i) \\bigg)^{1 - c_i} \\bigg( \\int_{-\\infty}^{y_\\min} f(y_i \\mid \\theta, x_i) \\ dy_i \\bigg)^{c_i} \\\\\n&= \\bigg( f(y_i \\mid \\theta, x_i) \\bigg)^{1 - c_i} \\bigg( F(y_\\min \\mid \\theta, x_i) \\bigg)^{c_i}.\n\\end{aligned}\n\\]\nThe likelihood for \\(y_i\\) is easy to write out here since the censoring structure is (relatively) simple. This gives rise to the likelihood of the sample, which (under the assumption of mutual conditional independence) is \\[\n\\mathcal{L}\\left(\\theta \\mid x, y \\right) =  \\prod_{i = 1}^n \\mathcal{L}\\left(\\theta \\mid y_i, x_i \\right).\n\\]\nNow that we’ve conducted the likelihood, we can do either of the usual things we would do to estimate the parameters: find the argument \\(\\theta\\) that maximizes the likelihood, or apply some priors and use an algorithm to estimate a Bayesian posterior.\nDoing either of these is not too complicated for this specific example – we can easily write a function to optimize, or we could do some kind of grid or quadratic posterior approximation. For this example, neither of those is very difficult and should converge easily. But, we have the benefit of excellent statistical tools that have already been written, so we might as well use them.\n\n2.3.1 Frequentist models\nThe R package survival (which actually predates the R language) implements parametric models of this form for many common distributions. Don’t let the name fool you: we can do models other than survival analysis. In this particular case, we just need to specify a parametric model with a normal distribution and left censoring. The syntax for this is a bit strange, we need to use the Surv() function to set up a “survival object” which we pass as the response variable in the survreg() function.\n\n# First we have to transform the outcome\nsurv_model &lt;- survival::survreg(\n    # Creating the \"survival time\" outcome\n    survival::Surv(\n        # If the value is lower than LoD, replace it w/ LoD, then take the log\n        pmax(y, LoD) |&gt; log(),\n        # The censoring indicator needs to be the opposite of what makes sense --\n        # zero for censored, one for uncensored -- it's actually an indicator of\n        # an \"event\" occurring, for us this is the event\n        # \"getting a reliable measurement.\"\n        !c,\n        # Specify left censoring\n        type = 'left'\n    ) ~\n        # All the other linear model stuff as usual\n        dist_from_farm + personal_gly_use + water_filter_use,\n    data = gly_data_obs,\n    dist = \"gaussian\"\n)\n\nsurv_model |&gt;\n    broom::tidy(conf.int = TRUE) |&gt;\n    tibble::add_column(truth = c(coefs$estimate, log(res_sd))) |&gt;\n    dplyr::select(term, estimate, conf.low, conf.high, truth) |&gt;\n    knitr::kable(digits = 3)\n\n\n\n\nterm\nestimate\nconf.low\nconf.high\ntruth\n\n\n\n\n(Intercept)\n0.735\n0.588\n0.882\n0.800\n\n\ndist_from_farm\n-3.854\n-4.245\n-3.463\n-4.000\n\n\npersonal_gly_use\n0.537\n0.423\n0.651\n0.500\n\n\nwater_filter_use\n-0.388\n-0.494\n-0.282\n-0.400\n\n\nLog(scale)\n-1.264\nNA\nNA\n-1.386\n\n\n\n\n\nDespite all the finagling we have to do, we can see that the estimates are now much better. Although the point estimates are not as close as we would expect if we had the latent uncensored outcome variable, the confidence intervals actually contain the true values this time. (Note that Log(scale) is the estimated residual SD on the log scale, but for some reason the CI doesn’t get calculated by any of the survival methods.) Notably, the CIs for this model are actually smaller than the the CIs for the naive model, even though we’re assuming there is extra uncertainty in the outcome. But we have accounted for this uncertainty in the model, so it doesn’t leak into the parameter estimates (of course that’s a non-technical explanation).\nThe worst part here is doing all that Surv() stuff, but fortunately there are ways to avoid having to do all that. This method is commonly called the “Tobit model” in econometrics, and there is a well-developed literature around this model, and some variations and extensions. In particular, the AER package provides a function called tobit() that allows one to specify these kind of simple censoring models in standard R syntax, and automatically does the conversion to a survreg() model.\n\ntobit_model &lt;- AER::tobit(\n    log(y) ~ dist_from_farm + personal_gly_use + water_filter_use,\n    data = gly_data_obs,\n    left = log(LoD),\n    right = Inf,\n    dist = \"gaussian\"\n)\n\nsummary(tobit_model)\n\n\nCall:\nAER::tobit(formula = log(y) ~ dist_from_farm + personal_gly_use + \n    water_filter_use, left = log(LoD), right = Inf, dist = \"gaussian\", \n    data = gly_data_obs)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           147             42            105              0 \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.73500    0.07495   9.807  &lt; 2e-16 ***\ndist_from_farm   -3.85438    0.19947 -19.323  &lt; 2e-16 ***\npersonal_gly_use  0.53676    0.05830   9.208  &lt; 2e-16 ***\nwater_filter_use -0.38809    0.05408  -7.176 7.15e-13 ***\nLog(scale)       -1.26354    0.06921 -18.257  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 0.2827 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 7 \nLog-likelihood: -33.8 on 5 Df\nWald-statistic: 432.1 on 3 Df, p-value: &lt; 2.22e-16 \n\n\nNow, if we want, it’s also not too bad to do this in a Bayesian framework.\n\n\n2.3.2 Bayesian models\nFirst we need to describe some basic priors that will work. For this example, we know that the parameters actually have “true values” so assigning a degenerate prior would actually reflect our true beliefs, but it’s silly. So we’ll assign some usual weakly informative priors that will help our model work right. If you want to do the priors with the variance of 10000 to be “objective” or whatever you can do that but I don’t think it’s a good idea. I’ll base my priors largely on the advice in Statistical Rethinking by Richard McElreath and the Stan Prior Choice Guide.\nWe need a prior for everything in our model that isn’t observed in the data. For this model, that’s our residual variance and the beta coefficients. We’ll set the priors as follows:\n\\[\n\\begin{aligned}\n\\beta_j &\\sim \\mathrm{Normal}\\left(0, 3\\right), \\quad j = {0, 1, 2, 3}; \\\\\n\\sigma &\\sim \\text{Half-t}\\left(0, 3, \\nu = 3\\right).\n\\end{aligned}\n\\] here, we use \\(\\mathrm{t}\\left(\\mu, \\sigma, \\nu\\right)\\) to denote the location-scale Student’s \\(t\\)-dsitribution with \\(\\nu\\) degrees of freedom. We use normal distributions centered at 0 for the \\(\\beta_j\\) to indicate our skepticism about the strength of the effects (if we were to pretend we didn’t know the generative model). Because \\(\\sigma\\) must be postive, we actually use the “half” version of the distribution, which has strictly positive support.\nNow, for this kind of model where only the outcome is censored, we can actually have the super-handy brms R package do the heavy lifting for us. All we have to do is a bit of data cleaning, and a bit of working specifying the model structure, but the package will handle writing, compiling, and running all of the Stan code, which is very conveninent. First we’ll fit the naive model, to show the brms syntax, and then we’ll cover the censored adjustment.\n\n2.3.2.1 naive model in brms\nAt first glance, the brms syntax appears to be quite difficult to use. While complex, it is concise and specifying the syntax is much less difficult than writing Stan code for many models. Fitting a brms model broadly requires 5 things from us: the first two are the formula, in lm() type syntax with a few extras, and the data, as we expect. We also need to specify a distributional family argument for the likelihood, which we implicitly do when we call lm(). As with base R, the gaussian likelihood is the default in brms, but it’s good practice to be clear. We also need to specify the priors, which are written in a specific way, and the Stan control parameters which control the Bayesian routine. Because brms is a wrapper for Stan’s NUTS sampler, the fitting routine is much more complicated than the routine for lm() and there’s a lot we can change.\nIf we just ignore these arguments, we can still get some results. Note that when you run a brms model like this, you’ll first get a compiling message, then several intermittant progress updates about how fitting is going. I have those silenced here because it generates a lot of them.\n\nbasic_brms_fit &lt;- brms::brm(\n    formula = log(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use,\n    data = gly_data_obs,\n    silent = 2\n)\n\n\nsummary(basic_brms_fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use \n   Data: gly_data_obs (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.97      0.12     0.74     1.20 1.00     4615     2983\ndist_from_farm      -5.13      0.28    -5.68    -4.58 1.00     4761     2902\npersonal_gly_use     0.77      0.10     0.57     0.95 1.00     4472     2826\nwater_filter_use    -0.50      0.08    -0.66    -0.33 1.00     5215     3088\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.03     0.44     0.56 1.00     4653     2929\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause the model is simple, we don’t have to do much. The estimates here are very similar to the estimates from the naive frequentist model. Let’s apply some priors and some Stan arguments and see what happens. First, we’ll set up the priors.\nI recommend reading the brms documentation and papers to learn more about how this prior syntax works, but we can also take a look at the default priors that brms set for our basic model.\n\nprior_summary(basic_brms_fit)\n\n                   prior     class             coef group resp dpar nlpar lb ub\n                  (flat)         b                                             \n                  (flat)         b   dist_from_farm                            \n                  (flat)         b personal_gly_use                            \n                  (flat)         b water_filter_use                            \n student_t(3, -0.7, 2.5) Intercept                                             \n    student_t(3, 0, 2.5)     sigma                                         0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n\n\nHere, we can see that the coefficients have (flat) priors, which often works for this simple of a model, but is usually not a good choice because it can be difficult for the sampler. The default prior for sigma, the residual SD, is very similar to what I picked, and I think is generally a good default prior. Anyways, based on the class column from this output, we can figure out how to set up a prior. Note that the lb for sigma is the lower bound.\n\nmy_priors &lt;- c(\n    prior(normal(0, 3), class = \"b\"),\n    prior(normal(0, 3), class = \"Intercept\"),\n    prior(student_t(3, 0, 3), class = \"sigma\", lb = 0)\n)\n\nmy_priors\n\n              prior     class coef group resp dpar nlpar   lb   ub source\n       normal(0, 3)         b                            &lt;NA&gt; &lt;NA&gt;   user\n       normal(0, 3) Intercept                            &lt;NA&gt; &lt;NA&gt;   user\n student_t(3, 0, 3)     sigma                               0 &lt;NA&gt;   user\n\n\nEverything else we pass in, other than these four arguments, will be Stan control parameters. If you want to learn about Stan specifics, there are way better resources than this, so I recommend you read those. I just want to make it clear that these other things are telling Stan how it should run the fitting routine.\n\nnaive_brms_fit &lt;- brms::brm(\n    formula = log(y) ~ dist_from_farm + personal_gly_use + water_filter_use,\n    data = gly_data_obs,\n    family = gaussian(),\n    prior = my_priors,\n    warmup = 1000,\n    iter = 2000,\n    chains = 4,\n    cores = 4,\n    seed = 32134,\n    backend = \"cmdstanr\",\n    silent = 2\n)\n\n\nsummary(naive_brms_fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(y) ~ dist_from_farm + personal_gly_use + water_filter_use \n   Data: gly_data_obs (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.95      0.12     0.71     1.18 1.00     5085     3143\ndist_from_farm      -5.09      0.28    -5.65    -4.55 1.00     5070     3066\npersonal_gly_use     0.77      0.09     0.59     0.95 1.00     4225     2823\nwater_filter_use    -0.49      0.08    -0.66    -0.33 1.00     4805     3507\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.03     0.44     0.56 1.00     4607     2924\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates don’t change that much from the previous fit. Again, that’s because a simple linear model with data generated from a linear model is really easy to handle. Once you start getting into more complicated models, the priors and stan arguments can matter a lot. But now that we’ve got all that taken care of, we should talk about censoring.\n\n\n2.3.2.2 censored model in brms\nIn brms, we can handle censoring by a special formula syntax that tells the model to do a likelihood adjustment. Notably, how to do this is kind of buried in the documentation. And at time of writing, it never explains that the likelihood adjustment is what it actually does (but it is). The best place to read about this functionality and other additional options for brms is on this page of the documentation. We need to do a bit of data cleaning first before we can handle the censoring though. As you can see on that page, we need our censoring indicator variable to contain -1 for left-censored data, instead of 1 like we currently have. The censoring bound should be given in y, so we need to transform it again so that all censored values are written down as the actual LoD.\n\nbrms_data &lt;-\n    gly_data_obs |&gt;\n    dplyr::mutate(\n        # Transform censoring indicator\n        censored = ifelse(c == 1, -1, 0),\n        # Transform the outcome\n        outcome = pmax(log(y), log(LoD)),\n        # Only keep the variables we plan to give to brms\n        .keep = \"unused\"\n    )\n\nhead(brms_data)\n\n# A tibble: 6 × 5\n  dist_from_farm personal_gly_use water_filter_use censored outcome\n           &lt;dbl&gt;            &lt;int&gt;            &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1          0.369                1                1        0 -0.997 \n2          0.466                0                0        0 -0.991 \n3          0.307                0                1       -1 -1.11  \n4          0.625                0                1       -1 -1.11  \n5          0.259                0                0        0 -0.0908\n6          0.677                0                0       -1 -1.11  \n\n\nNow we can fit the brms model with the censoring correction.\n\ncens_brms_fit &lt;- brms::brm(\n    formula = outcome | cens(censored) ~ dist_from_farm + personal_gly_use +\n        water_filter_use,\n    data = brms_data,\n    family = gaussian(),\n    prior = my_priors,\n    warmup = 1000,\n    iter = 2000,\n    chains = 4,\n    cores = 4,\n    seed = 32134,\n    backend = \"cmdstanr\",\n    silent = 2\n)\n\n\nsummary(cens_brms_fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome | cens(censored) ~ dist_from_farm + personal_gly_use + water_filter_use \n   Data: brms_data (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.73      0.08     0.58     0.89 1.00     4533     3014\ndist_from_farm      -3.85      0.20    -4.26    -3.47 1.00     4388     3225\npersonal_gly_use     0.54      0.06     0.42     0.66 1.00     4382     2588\nwater_filter_use    -0.39      0.06    -0.50    -0.28 1.00     4732     2676\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.29      0.02     0.25     0.34 1.00     4531     3359\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain, our results are similar to the frequentist model. All of the true values are now inside of the estimated 95% credible intervals, with the one exception of sigma, the residual SD, which had a true value of 0.25 – so we’ll say it’s inside the credible interval up to measurement/computer precision, there’s no way that small of a difference would practically matter.\nUnfortunately, not every model can be handled with brms, even though it’s very flexible. For example, when we talk about censored predictor values, we won’t be able to use brms for the approach we’ll use. Instead, we’ll have to write our own Stan code, which is even more flexible than brms (though it lacks many of the convenience features).\n\n\n2.3.2.3 Custom Stan code with cmdstanr\nMoving from using only R to writing a Stan program is the biggest jump we have to make in our quest to deal with censored data. If you don’t know anything about Stan, I recommend the Statistical Rethinking book mentioned earlier, as the associated rethinking package is quite easy to use to specify a lot of basic statistical models (with guidance from the text), and provides a method to view the Stan code generated by a model. The brms package also provides the stancode() method, but the Stan code generated by brms employs many tips to make the model more efficient, and so can be difficult to interpret if you aren’t familiar with Stan.\nAnyways, I don’t plan to explain the details of writing Stan code here, like I mentioned with brms and the R language and even the statistics stuff we’re doing, there are a lot better resources you can find with a quick google that are better than what I could write. So instead I already wrote the Stan code, and here’s the code.\n\n\n\n\n\n\nModel code\n\n\n\n\n\n//\n// Dealing with a censored outcome in linear regression using the\n// integration approach\n// author: Zane Billings\n// started: 2024-05-18\n//\n\n// The input data consists of:\n// - n (positive integer): the number of data records\n// - p (positive integer): number of covariates to include.\n// - y (vector of real numbers): the outcome, assumed to follow a normal\n//   distribution where the mean is based on a linear model.\n// - c (array of integers that must be 0 or 1): should be 0 if the corresponding\n//   observation in y is completely observed, or 1 if the observation is\n//   censored (below the detection limit).\n// - X (n x p matrix where p is the number of predictors): the matrix of\n//   covariates to use in the linear model.\n// - DL (real scalar): the lower limit of detection for the observed values of\n//   the outcome variable, y. This must be less than or equal to the smallest\n//   observed value of the outcome.\ndata {\n\tint&lt;lower=1&gt; n;\n\tint&lt;lower=1&gt; p;\n\tvector[n] y;\n\tarray[n] int&lt;lower=0, upper=1&gt; c;\n\tmatrix[n, p] X;\n\treal&lt;upper=min(y)&gt; DL;\n}\n\n// The parameters accepted by the model.\n// - alpha (real scalar): the intercept\n// - beta (real vector of length P): vector of slope coefficients\n// - sigma (positive real scalar): the residual variance.\nparameters {\n\treal alpha;\n\tvector[p] beta;\n\treal&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated. This is the part where we put priors and\n// likelihood calculations.\nmodel {\n\t// Priors for parameters -- note that by specifying priors for beta (a vector)\n\t// like this, we implicitly assume all beta[j] have independent priors of the\n\t// same form.\n\talpha ~ student_t(3, 0, 3);\n\tbeta ~ student_t(3, 0, 3);\n\tsigma ~ student_t(3, 0, 3);\n\t\n\t// Calculate the mean from the linear model. We can do this in a vectorized\n\t// way to improve efficeincy.\n\tvector[n] mu = X * beta + alpha;\n\t\n\t// Loop through each observation and update the likelihood as neceeded. If\n\t// c[i] = 0, update using the typical Normal likelihood function. If c[i] = 1,\n\t// we need to do the integration thing.\n\t// For this simple case where all the detection limits are the same, we could\n\t// vectorize this. But this is easier to read.\n\tfor (i in 1:n) {\n\t\tif (c[i] == 0) {\n\t\t\ty[i] ~ normal(mu[i], sigma);\n\t\t} else if (c[i] == 1) {\n\t\t\ttarget += normal_lcdf(DL | mu[i], sigma);\n\t\t}\n\t}\n}\n\n\n\nNow we can talk about running Stan code in R, which I actually will talk a bit about. Once you have the Stan file set up, you have to figure out how to get Stan running. I’ll use (and recommend that everyone else) use cmdstanr, which is an R package you can install. You’ll need to do some extra setup if you’ve never used the package before, so follow their guide and I strongly recommend following the steps outside of an R project, just open the base R gui and copy and paste their code.\nOnce you have cmdstanr installed, the first thing we need to do is load the Stan file and compile the model. I almost always prefer to do this in two separate steps in case I need to control or force the compilation step.\n\nstan_pth &lt;- here::here(pth_base, \"censored-outcome-integration.stan\")\nstan_mod &lt;- cmdstanr::cmdstan_model(stan_file = stan_pth, compile = FALSE)\n\nThe stan_mod object is set up with cmdstanr now, so it knows where to find the Stan code it will need to run. If you try to print that object, it will print the Stan code and will also tell you if there are any syntactical errors in the code. Since there are no syntactical errors, we can try and compile the Stan code. This step is likely not familiar if you only use R code. Stan code needs to be translated into machine-level language before it can be run, which dramatically speeds up the run time for complex models. This creates an .exe file which can be run by cmdstanr without needing to compile again. Here I’ll compile in pedantic mode which will give you suggestions on common issues that can make your model worse, without explicitly causing errors. This will spit out a lot of messages that I’ve elected to hide. They aren’t very useful most of the time.\n\nstan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n\nNow that the program is compiled, we need to set up our data in a specific format before we can run the HMC sampler. Stan accepts data in an R list() format, where each of the items can be different sizes and types. You can see in the data{} section of the printed Stan code what items you need to put in the list, and what their types should be. So we’ll do some quick data cleaning.\n\nstan_data &lt;- list(\n    # Number of data points\n    n = nrow(gly_data_obs),\n    # number of predictors\n    p = 3,\n    # vector of outcome values\n    y = brms_data$outcome,\n    # Vector of censoring indicators\n    c = gly_data_obs$c,\n    # Matrix of predictor values (size n x 3)\n    X = with(\n        gly_data_obs,\n        matrix(\n            c(dist_from_farm, personal_gly_use, water_filter_use),\n            ncol = 3\n        )\n    ),\n    # limit of detection -- needs to be on same scale as y vector\n    DL = log(LoD)\n)\nstr(stan_data, 1)\n\nList of 6\n $ n : int 147\n $ p : num 3\n $ y : num [1:147] -0.997 -0.9906 -1.1087 -1.1087 -0.0908 ...\n $ c : num [1:147] 0 0 1 1 0 1 0 0 0 0 ...\n $ X : num [1:147, 1:3] 0.369 0.466 0.307 0.625 0.259 ...\n $ DL: num -1.11\n\n\nWith the data in this format, we can go ahead and pass it to Stan and invoke the sampler. Since all of the model options are specified in the actual Stan code file this time, everything we pass to the sample() method is a control argument for how cmdstan runs the sampler.\n\nstan_fit &lt;- stan_mod$sample(\n    data = stan_data,\n    seed = 10896,\n    chains = 4,\n    parallel_chains = 4,\n    iter_sampling = 4000,\n    iter_warmup = 1000,\n    refresh = 0\n)\n\n\nstan_fit$summary()\n\n# A tibble: 6 × 10\n  variable   mean median     sd    mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     57.9   58.2   1.63   1.47   54.8   59.9    1.00    6337.    8673.\n2 alpha     0.734  0.734 0.0772 0.0769  0.610  0.861  1.00    8057.    9348.\n3 beta[1]  -3.86  -3.86  0.205  0.204  -4.21  -3.53   1.00    8098.    8821.\n4 beta[2]   0.539  0.538 0.0603 0.0599  0.440  0.638  1.00   14237.   10780.\n5 beta[3]  -0.389 -0.389 0.0566 0.0551 -0.482 -0.297  1.00   12444.   10933.\n6 sigma     0.291  0.290 0.0209 0.0208  0.259  0.327  1.00   13094.    9660.\n\n\nIf you compare the numbers between this fit and the brms fit, they’re pretty much exactly the same. Again, it’s largely because this model is so simple. If we were doing a more complicated brms-compatible model, the brms fit would probably be a bit better cause of all the tricks it does to write the Stan code in an efficient way. But for models with censored predictors we’ll have to use our own Stan code, so it’s worth seeing how to do it for this relatively easier case.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#conclusions",
    "href": "examples/simple-censored-outcome/index.html#conclusions",
    "title": "2  One censored outcome",
    "section": "2.4 Conclusions",
    "text": "2.4 Conclusions\nIn this example, we saw how to deal with censored outcomes in regression models. This method of adjusting the likelihood generalizes to arbitrarily complex models as long as the outcome follows this kind of simple censoring pattern. This method also generalizes to each observaiton of the outcome having a different limit of detection, and we can have both upper and lower LoDs, or even interval censoring patterns, as long as we know and we can write down the censoring limits for each observation. But we’ll discuss more of that kind of stuff in future examples.\nBecause only the outcome is censored, there are a lot of methods available to us for dealing with this kind of data, including pre-built methods in survival for frequentist ML models, and brms for Bayesian models. But as we’ll see in the next example, not everything can be so easy.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#appendix-notes-on-modeling",
    "href": "examples/simple-censored-outcome/index.html#appendix-notes-on-modeling",
    "title": "2  One censored outcome",
    "section": "Appendix: notes on modeling",
    "text": "Appendix: notes on modeling\nSince this is a simple example focused on dealing with the censoring adjustment, I decided not to get into the weeds about modeling anywhere else. There are a few technical choices I made/didn’t make, so I decided to write them down here for posterity, without cluttering up the main text.\n\nYou should definitely use cmdstanr for running Stan models. In general, it stays up to date much better, is a bit easier to use, and is compatible with almost everything rstan is (and the formats are interchangable). It can be annoying to install, but it’s worth the hassle.\nA lot of people like flat or very diffuse priors because they are “objective”. I dislike them for a number of reasons. First of all, they often reduce the ability of the model to sample efficiently. Second, due to the inherent researcher degrees of freedom in setting up the model structure and what data are present, I don’t think talking about the “objectivity” of priors makes any sense. Finally, the priors represent our prior beliefs about the parameters, and a flat prior means we think any possible value is equally likely. This is why people often think this is an “objective” prior but I think that’s silly. I certainly don’t think my beta coefficient is equally likely to be 1 vs. negative three billion and seven, but that’s what the flat prior says. In general, I know the effect should be fairly weak, so we should use skeptical priors that allow the parameter to get larger if the data support that.\nIn general for “generic priors”, I tend to prefer \\(t\\) and half-\\(t\\) priors with 3 degrees of freedom. These have thicker tails than normal/half-normal priors, but not as thick as Cauchy priors, which some people will recommend. A Cauchy prior reflects a belief that the variance of a variable is infinite, which is not plausible to me, and Cauchy priors also cause sampling efficiency to plummet in many cases without much of a benefit. So in general if I had to pick a “generic default prior” it would be \\(t\\) with three degrees of freedom.\nI didn’t talk about centering and scaling here, but in general you should typically scale your variables (e.g. divide by the MAD) before putting them into Stan, simply because it helps the sampler.\nCentering can also be helpful, and if you want to do any kind of variable selection or comparison between variables, you need to center them. Centering can make coefficient interpretations more difficult, but one of the benefits of Bayesian modeling is that any posterior calculation we want to make comes with an entire posterior distribution of estimates, so we never have to worry about figuring out CIs for a posteriori calculations. In general it’s often better to do these kind of computational tricks to make the model fit efficiently, and then do transformations and predictions of interest afterwards.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-outcome/index.html#appendix-imputation-methods",
    "href": "examples/simple-censored-outcome/index.html#appendix-imputation-methods",
    "title": "2  One censored outcome",
    "section": "Appendix: imputation methods",
    "text": "Appendix: imputation methods\nThe other approach to dealing with censoring is to use a constrained imputation method. In general, multiple imputation (MI) works well with Bayesian inference, and you can implement MI and subsequent pooling methods to correct for censoring, so long as the MI method can be constrained to account for the limits on censored values.\nIn addition to standard MI methods, you can also impute values within the Stan model – this is a big advantages of Bayesian methods. I won’t go into details about either of these imputation methods, but for a brief discussion you can read this StackExchange post. The Stan manual has an example of this as well. Similar imputation methods are covered in books like Bayesian Data Analysis by Gelman et al, and we can account for censoring by specifying constraints.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>One censored outcome</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-predictor/index.html",
    "href": "examples/simple-censored-predictor/index.html",
    "title": "3  One censored predictor",
    "section": "",
    "text": "3.1 Data-Generating Process\nThe data will discuss consist of \\(n\\) observations of a predictor variable, \\(x\\), and an outcome variable, \\(y\\). To make this example simple, we’ll assume there is a linear relationship between \\(x\\) and \\(y\\). So the data generating process (DGP) for the outcome, \\(y_i, \\ i = \\{1, 2, \\ldots, n\\}\\), is \\[\n\\begin{align*}\ny_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i^*\n\\end{align*}\n\\] where \\(x_i^*\\) represents the true value of our predictor. Importantly, for the predictor, \\(x\\), to be censored, there also has to be a data generating process for \\(x\\). If we conduct a controlled experiment where we determine the values for \\(x\\), it doesn’t make sense for \\(x\\) to be censored (which is why this case is discussed less often than the case of a censored outcome). But if we are conducting an observational study, where we expect \\(x\\) to influence \\(y\\), but we cannot directly manipulate the value of \\(x\\), then it makes sense for \\(x\\) to potentially be censored. Like with a censored outcome, we can discuss the DGP for the latent or true \\(x_i^*\\) values, and the observation process which generates our imperfect observation \\(x_i\\). Of course, the most simple example would be a Normal distribution1: \\[\nx_i^* \\sim \\text{Normal}\\left(\\lambda, \\tau^2\\right).\n\\]\nThis is the part where we’ll just say you can put whichever distribution here as well – don’t worry, we’ll do another non-normal example after this. Now that we have a DGP for the latent \\(X^*\\) values, we need to specify our observation model. For a simple case of censoring, this could include a lower limit of detection (LoD), an upper LoD, or both. For example, with a lower LoD, the observation model might look like this:\n\\[\nx_i = \\begin{cases}\nx_{\\min}, & x_i^* \\leq x_{\\min} \\\\\nx_i^*, & x_i^* &gt; x_{\\min}\n\\end{cases}; \\quad x_{\\min} \\in \\mathbb{R}.\n\\]\nHere, \\(x_{\\min}\\) is a constant value representing a lower limit of detection – if the latent value \\(x_i^*\\) is less than the threshold represented by \\(x_{\\min}\\), we just observe \\(x_{\\min}\\). If the true value is greater than this threshold, we observe the true value. This is not a unique observation process – we can, in principle, write down any value for the censored observations. To avoid confusion, we should also define an indicator variable for whether the \\(i\\)th observation is censored: \\[\nc_i = I\\left( x_i^* \\leq x_{\\min} \\right) =\n\\begin{cases}\n1, & x_i^* \\leq x_{\\min} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n\\] Typically, we can observe all values of \\(c_i\\), and we assume that these are measured perfectly (although this is not strictly necessary, as we could incorporate measurement error and thus misclassification models into our observation process, but we neglect those here for the sack of simplicity). If you receive a data set you know a variable is censored, but there is no way to determine which values are censored due to improper coding and recording, there is not much you can do to resolve the situation. So it is typically best to record censored values using some value which could not have been observed if the observation were not censored. Do not worry if this description is abstract – next we will consider a concrete example which will hopefully help to make these concepts concrete.\nFor reference, we can write down the entire data-generating process all together. Note that from this DGP, we observe \\((x_i, c_i, y_i); \\ i = 1, \\ldots, n\\).\n\\[\n\\begin{align*}\ny_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i^* \\\\\nx_i^* &\\sim \\text{Normal}\\left(\\lambda, \\tau^2\\right) \\\\\nx_i &= \\begin{cases}\nx_{\\min}, & x_i^* \\leq x_{\\min} \\\\\nx_i^*, & x_i^* &gt; x_{\\min}\n\\end{cases}; \\\\\nc_i &= I\\left( x_i^* \\leq x_{\\min} \\right)\n\\end{align*}\n\\] Here, \\(\\sigma\\) and \\(\\tau\\) are positive real numbers, and the following are real-valued constants: \\(\\beta_0\\), \\(\\beta_1\\), \\(x_{\\min}\\), and \\(\\lambda\\).",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-predictor/index.html#weighing-cats",
    "href": "examples/simple-censored-predictor/index.html#weighing-cats",
    "title": "3  One censored predictor",
    "section": "3.2 Weighing cats",
    "text": "3.2 Weighing cats\nNow that we’ve gone through the basics of the data-generating process, let’s set up the DGP for an example data set. Once we’ve worked out the DGP, we’ll take a look at some simulated data from this example. Then we’ll finally discuss methods for dealing with censored predictor data.\nSuppose we’re conducting a study on cholesterol in cats, and we want to know whether elevated cholesterol levels are associated with cat weight – presumably, heavier cats have higher overall cholesterol values. For simplicity, we limit our study to adult male American shorthair cats (we can recruit other types of cats after we finish this pilot study). According to this article, the normal weight for such a cat is from 11 – 15 pounds. So, we’ll take 13 lbs. as the average weight of a cat, and 2 lbs. as the standard deviation.So, letting \\(x_i^*\\) represent the true weight of the \\(i\\)th cat, we would write \\[x_i^* \\sim \\mathrm{Normal} \\left(13, 2^2\\right).\\]\nNow, the problem is that we do not have a very good scale – our scale is accurate to the tenth of a pound, but the highest value it can measure is 14 lbs[^Our scale could also have a lower bound, but in this case it’s probably so low that we would never get any left-censored observations.]. In terms of the DGP, we would say that \\[x_{\\max} = 14.\\]\nUsing that information, we can then write out the observation model for the weight data. \\[\nx_i = \\begin{cases}\nx_{\\max}, & x_{\\max} \\leq x_i^*  \\\\\nx_i^*, & x_i^* &lt; x_{\\max}\n\\end{cases}.\n\\] This is not much more complicated than the abstract example we wrote out before that only had a lower LoD, in fact it’s easily taken care of in the same likelihood step. Now, we’ll also need to set up an indicator variable, which we call \\(c_i\\), that tells us if our weight values are censored or not.\n\\[\nc_i = \\begin{cases}\n1, & x_i \\geq x_{\\max} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n\\]\nNext, we need to pick a distribution for our outcome. Since many biological concentration values often follow log-normal distributions, let’s use that for our cholesterol levels. Based on this article (I have no idea how accurate it is, if at all, just like the previous one), the normal amount of cholesterol for a cat is 1.8 – 3.9 mMol/liter. So, we just need to choose distribution parameters. Note that for log-normal outcomes, it is almost always easier to use a normal distribution (Gaussian) model on the log-scale than it is to actually fit a lognormal model, so we’ll do that.\n\nIf we treat the range the same as we did previously, we can estimate that the overall average is around 2.85, with a spread of 1.05. That seems a little high, so let’s assume they gave us a two standard deviation range, and cut it in half, and we’ll round that down to \\(0.5\\), which is probably good enough for government work. (Of course if you want to simulate the data, you can make whatever arbitrary assumptions like this that you prefer instead.)\nSo, we would write the distribution for our cholesterol values, which we’ll call \\(y\\), as\n\\[\n\\log(y_i) \\sim \\text{Normal}\\left(\\mu_i, \\sigma\\right).\n\\]\nTo relate this to the weight values, we would then write out our linear model for \\(\\mu_i\\), the mean, as\n\\[\n\\mu_i = \\alpha + \\beta \\cdot x^*_i.\n\\]\nChoosing values of \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\), that make sense together and match the ranges we expect is kind of hard – for the purposes of this simulation, I just messed around until I got values that I liked.\n\n# List of data generating parameters / simulation parameters\ndata_gen_parms &lt;- list(\n    # Number of observations to generate\n    n = 337,\n    # Set a static constant value for the lower limit of detection (LoD)\n    ulod = 14,\n    # Linear model intercept\n    alpha = 0.35,\n    # Linear model slope\n    beta = 0.06,\n    # Linear model residual variance\n    sigma = 0.15,\n    # Mean of normally distributed x values\n    w_mu = 13,\n    # SD of normally distributed x values\n    w_sd = 2,\n    # pRNG seed\n    seed = S\n)\n\nSo for this example, I choose the simulation parameters \\(\\sigma = 0.15\\), \\(\\alpha = 0.35\\), and \\(\\beta = 0.06\\). We can interpret these parameters in the standard way for linear regression, noting that they are in terms of the log-cholesterol. Now we need to simulate the data. I randomly decided that we should have 337 cats in our study sample.\n\ngenerate_cat_weight_data &lt;- function(\n        n, ulod, alpha, beta, sigma, w_mu, w_sd, seed = NULL\n) {\n    # Set the seed, using the current unix time if one is not given\n    if(is.null(seed)) {set.seed(Sys.time())} else {set.seed(seed)}\n    \n    # Generate the data\n    l &lt;- tibble::tibble(\n        w_star = rnorm(n, w_mu, w_sd),\n        w = ifelse(w_star &gt;= ulod, ulod, w_star),\n        c = ifelse(w_star &gt;= ulod, 1, 0),\n        mu = alpha + beta * w_star,\n        y = exp(rnorm(n, mu, sigma))\n    )\n    \n    o &lt;- dplyr::select(l, w, y, c)\n    \n    out &lt;- list(\"latent\" = l, \"observed\" = o)\n    return(out)\n}\ndat &lt;- do.call(generate_cat_weight_data, data_gen_parms)\ndat_latent &lt;- dat$latent\ndat_observed &lt;- dat$observed\n\ndplyr::glimpse(dat_latent)\n\nRows: 337\nColumns: 5\n$ w_star &lt;dbl&gt; 12.44332, 11.27354, 14.33210, 12.46073, 13.34774, 13.84893, 14.…\n$ w      &lt;dbl&gt; 12.44332, 11.27354, 14.00000, 12.46073, 13.34774, 13.84893, 14.…\n$ c      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ mu     &lt;dbl&gt; 1.0965989, 1.0264123, 1.2099258, 1.0976436, 1.1508645, 1.180935…\n$ y      &lt;dbl&gt; 2.387532, 3.998243, 2.382242, 2.930991, 3.419141, 3.019194, 3.4…\n\n\nYou can use this function to generate similar datasets with arbitrary parameters if you want. Note that in this example, about 31 percent of our predictor values are censored.\nNow let’s take a quick look at the latent data so we can visualize the effect of censoring the predictor.\n\ndat_latent |&gt;\n    ggplot() +\n    geom_vline(\n        xintercept = data_gen_parms$ulod,\n        linetype = \"dashed\",\n        color = \"gray\"\n    ) +\n    geom_segment(\n        aes(\n            x = w_star, xend = w,\n            y = y, yend = y\n        ),\n        color = \"gray\"\n    ) +\n    geom_point(aes(x = w_star, y = y), color = \"gray\", size = 2) +\n    geom_point(aes(x = w, y = y), color = \"black\", size = 2) +\n    labs(x = \"Weight\", y = \"Cholesterol\")\n\n\n\n\n\n\n\n\nHere the gray points show the true latent values of the censored points, and the black points show what we actually observed. You can see that we obviously observe a much smaller range of data when censoring happens. If we had the latent variables in real life, we could use a standard regression model to estimate them, like this.\n\nlatent_glm &lt;- glm(\n    log(y) ~ w_star,\n    family = \"gaussian\",\n    data = dat_latent\n)\nbroom::tidy(latent_glm, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.324    0.0581       5.58 4.99e- 8   0.210     0.438 \n2 w_star        0.0624   0.00436     14.3  1.47e-36   0.0539    0.0710\n\n\nDue to sampling error, the estimates are not perfect, but the confidence intervals contain the true values and the estiamtes are reasonably close.\n\nSo, the natural next question is what happens when we fit the model with the actual observed data that’s been censored? (I always call this the “naive” model, because we are naively hoping for a model that breaks our assumptions to work.)\n\nnaive_glm &lt;- glm(\n    log(y) ~ w,\n    family = \"gaussian\",\n    data = dat_observed\n)\nbroom::tidy(naive_glm, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.134    0.0871       1.54 1.24e- 1  -0.0365    0.305 \n2 w             0.0795   0.00680     11.7  1.09e-26   0.0662    0.0929\n\n\nIn this case, we can see that the estimates are definitely biased. The CI for the slope doesn’t even include the true value. Honestly, most models we fit in real life are probably more incorrect than this though, so interpreting this coefficient with a hefty amount of caution would probably not be the end of the world. However, we can do better, and censoring can also cause worse problems.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-predictor/index.html#more-censoring",
    "href": "examples/simple-censored-predictor/index.html#more-censoring",
    "title": "3  One censored predictor",
    "section": "3.3 More censoring!",
    "text": "3.3 More censoring!\nOf course, if ignoring the problem didn’t make a difference, we would want to just ignore it, right? But let’s see what happens when we increase the amount of data that are censored. So this time, let’s say our scale has a maximum of 12.5 lbs[^A good question to ask at this point is whether we should actually be doing this study if we can’t get better measures. But some immunological or environmental studies actually have more than 50% censored or missing data, and sometimes the question is so important that we really want to get the most out of the data we have.]. So first let’s rerun the simulation.\n\ndata_gen_parms_2 &lt;- data_gen_parms\ndata_gen_parms_2$ulod &lt;- 12.5\n# Resimulate the data -- note that because we used the same random seed, the\n# only that will change is the amount of censoring.\ndat2 &lt;- do.call(generate_cat_weight_data, data_gen_parms_2)\ndat2_latent &lt;- dat2$latent\ndat2_observed &lt;- dat2$observed\n\ndplyr::glimpse(dat2_latent)\n\nRows: 337\nColumns: 5\n$ w_star &lt;dbl&gt; 12.44332, 11.27354, 14.33210, 12.46073, 13.34774, 13.84893, 14.…\n$ w      &lt;dbl&gt; 12.44332, 11.27354, 12.50000, 12.46073, 12.50000, 12.50000, 12.…\n$ c      &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, …\n$ mu     &lt;dbl&gt; 1.0965989, 1.0264123, 1.2099258, 1.0976436, 1.1508645, 1.180935…\n$ y      &lt;dbl&gt; 2.387532, 3.998243, 2.382242, 2.930991, 3.419141, 3.019194, 3.4…\n\n\nIf we plot these new simulated data, we can see that many more data points are censored that before, and about 60 percent of the predictor values are censored; much more than the previous example.\n\ndat2_latent |&gt;\n    ggplot() +\n    geom_vline(\n        xintercept = data_gen_parms$ulod,\n        linetype = \"dashed\",\n        color = \"gray\"\n    ) +\n    geom_segment(\n        aes(\n            x = w_star, xend = w,\n            y = y, yend = y\n        ),\n        color = \"gray\"\n    ) +\n    geom_point(aes(x = w_star, y = y), color = \"gray\", size = 2) +\n    geom_point(aes(x = w, y = y), color = \"black\", size = 2) +\n    labs(x = \"Weight\", y = \"Cholesterol\")\n\n\n\n\n\n\n\n\nNow let’s see what happens when we fit a naive model.\n\nnaive_glm_2 &lt;- glm(\n    log(y) ~ w,\n    family = \"gaussian\",\n    data = dat2_observed\n)\nbroom::tidy(naive_glm_2, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -0.109    0.144     -0.758 4.49e- 1  -0.392      0.173\n2 w              0.104    0.0120     8.73  1.21e-16   0.0810     0.128\n\n\nNow we can see that the estimates are much worse. The intercept is actually negative, with a wide confidence interval, and the slope has been severely inflated. So let’s see what we can do about that.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-predictor/index.html#model-for-censored-predictors",
    "href": "examples/simple-censored-predictor/index.html#model-for-censored-predictors",
    "title": "3  One censored predictor",
    "section": "3.4 Model for censored predictors",
    "text": "3.4 Model for censored predictors\nLet’s first write out the entire data generating process in one place.\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}\\left(\\mu_i, \\sigma\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal} \\left(\\mu_x, \\sigma_x^2 \\right) \\\\\nx_i &= \\begin{cases}\nx_{\\max}, & x_{\\max} \\leq x_i^*  \\\\\nx_i^*, & x_i^* &lt; x_{\\max}\n\\end{cases}\n\\end{aligned}\n\\]\nHere, we assume \\(x_{\\max} = 12.5\\) is a known constant. In future tutorials, we’ll discuss what to do if this isn’t a known constant, but in most situations that arise in a lab study, we do know the censoring limits.\nNow, the unfortunate thing about censored predictors like this, is that there are (to my knowledge) no out-of-the-box models that can adjust the likelihood. Most frequentist methods assume that the \\(x_i\\) predictor variables are known constants which have no intrinsic error. Of course this is rarely true (outside of a specific type of experimental setup), and there are errors-in-variables models which can help to address this. But I’m not aware of any errors-in-variables implementations that allow for arbitrary and censored predictor distributions.\nBecause directly modeling the likelihood of the predictor variable is quite difficult (we will discuss this in future tutorials), we’ll use a constrained imputation approach in Stan. Sometimes called a “full Bayesian” approach, we specify a distribution for the predictor values. Then, we can use observations of the predicted values to estimate the parameters, and we can impute censored observations from a truncated distribution where the imputed value is guaranteed to be less than the limit of detection (for a lower limit).\n\nThis type of model is implemented in the following Stan code. To fit a Stan model, we’ll also need to specify priors, and I decided to use the same generic Student’s (Half) \\(t\\) priors that I discussed in the previous section.\n\n\n\n\n\n\nModel code\n\n\n\n\n\n//\n// Regression model with a single censored predictor\n// Censoring handled by constrained imputation\n// Zane Billings\n// 2024-07-15\n//\n\n// The input data consists of:\n// N: positive scalar integer. The number of data records.\n// y: real vector of length N. The vector of outcome variable observations.\n// x: real vector of length N. The vector of predictor variable observations.\n// c: integer array of length N, either 0 or 1 for all entries.\n//   c[i] = 0 if observation i is observed and c[i] = 1 if observation i is\n//   censored.\n// u: real vector of length N. The vector of lower limits of detection.\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; N_obs;\n  array[N] real y;\n  array[N_obs] real x_obs;\n  real LoD;\n}\n\n// Transformed data\n// Computed from passed data\ntransformed data {\n  // Number of censored and observed observations\n  int&lt;lower=0&gt; N_cens = N - N_obs;\n}\n\n// The parameters accepted by the model.\nparameters {\n  // Regression parameters\n  real a, b;\n  real&lt;lower = 0&gt; sigma;\n  \n  // x distribution parameters\n  real x_mu;\n  real &lt;lower=0&gt; x_sd;\n  \n  // Vector of censored x values\n  array[N_cens] real&lt;upper=LoD&gt; x_cens;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  // x holder\n  vector[N] x;\n  // mu vector\n  vector[N] mu;\n  \n  // Priors\n  a ~ student_t(3, 0, 2);\n  b ~ student_t(3, 0, 2);\n  sigma ~ student_t(3, 0, 2);\n  x_mu ~ student_t(3, 0, 2);\n  x_sd ~ student_t(3, 0, 2);\n  \n  // Likelihood\n  x_obs ~ normal(x_mu, x_sd);\n  x_cens ~ normal(x_mu, x_sd);\n  x = to_vector(append_array(x_obs, x_cens));\n  mu = a + b * x;\n  y ~ normal(mu, sigma);\n}\n\n\n\n\nAs with last time, first we need to set up a data list that matches what the Stan code expects in the data block. Note that for the imputation method, the data block only requires us to pass in the observed values of \\(x\\). This leads to some annoying but necessary data cleaning, where we have to make sure that all of the censored values are together in the dataset and all of the observed values are together in the data set, to make sure the correct \\((x_i, y_i)\\) pairs stay together.\n\ndat2_sorted &lt;- dat2_observed[order(dat2_observed$c), ]\nstan_data &lt;- list(\n    N = nrow(dat2_sorted),\n    N_obs = nrow(dat2_sorted[dat2_sorted$c == 0, ]),\n    y = log(dat2_observed$y),\n    x_obs = dat2_observed$w[dat2_sorted$c == 0],\n    LoD = data_gen_parms_2$ulod\n)\nstr(stan_data, 1)\n\nList of 5\n $ N    : int 337\n $ N_obs: int 134\n $ y    : num [1:337] 0.87 1.386 0.868 1.075 1.229 ...\n $ x_obs: num [1:134] 12.4 11.3 12.5 12.5 12.5 ...\n $ LoD  : num 12.5\n\n\nNext we’ll load and compile the program.\n\nmod_path &lt;- here::here(pth_base, \"censored-predictor-imputation.stan\")\nstan_mod &lt;- cmdstanr::cmdstan_model(mod_path, compile = FALSE)\nstan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n\nNow we can do the fun part of running the sampler.\n\nfit &lt;- stan_mod$sample(\n    data = stan_data,\n    seed = S,\n    parallel_chains = 4,\n    chains = 4,\n    iter_warmup = 1000,\n    iter_sampling = 1250,\n    # Turn off printing messages\n    refresh = 0,\n    show_messages = FALSE,\n    show_exceptions = FALSE\n)\n\nI silenced the printout for the purposes of this vignette, but we can quickly quick the diagnostics to show that, while there were a few divergences in the warmup stage, there were no post-warmup divergent transitions and sampling went fairly well.\n\nfit$diagnostic_summary()\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.6881872 0.7043031 0.7001663 0.6922051\n\n\nNow we can take a look at the posterior distributions of our parameters of interest. Notably, we have full posterior distributions for all of the imputed predictor values, but those are generally not of much interest.\n\nfit$summary(variables = c('a', 'b', 'sigma', 'x_mu', 'x_sd'))\n\n# A tibble: 5 × 10\n  variable    mean  median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 a         0.279   0.277  0.237   0.246   -0.112   0.673  1.00    2856.\n2 b         0.0734  0.0736 0.0200  0.0205   0.0401  0.106  1.00    2862.\n3 sigma     0.193   0.192  0.00764 0.00767  0.181   0.205  1.00    7690.\n4 x_mu     11.8    11.8    0.0522  0.0522  11.7    11.9    1.00    3100.\n5 x_sd      0.681   0.679  0.0376  0.0373   0.622   0.746  1.00    4640.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nHere, we see that some of the parameter estimates are fixed, but others don’t actually seem to be. The model basically can’t tell where the intercept is, and the parameters for the mean and standard deviation of \\(x\\) are not quite right, especially the standard deviation. But more importantly, the estimate of the slope appears to be corrected – while the point estimate is not exactly correct, we are able to recover an accurate credible interval, and the slope is the main parameter of interest. So this imputation method manages to correct our effect estimate, although the issues with identifiability in this model might be harder to fix.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/simple-censored-predictor/index.html#footnotes",
    "href": "examples/simple-censored-predictor/index.html#footnotes",
    "title": "3  One censored predictor",
    "section": "",
    "text": "There’s actually a statistical issue with this Normal/Normal model: it’s not fully identifiable without extra constraints. See the errors-in-variables wikipedia page for a bit more info. But we’ll ignore that for now.↩︎",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/censored-outcome-and-predictor/index.html",
    "href": "examples/censored-outcome-and-predictor/index.html",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "",
    "text": "4.1 Data simulation\nAs usual, we’ll begin our data simulation by writing out the true data generating process (likelihood model) that we’ll use to generate the data. This model is a bit complicated–of course we’ll have the same regression part of the model as we’ve had before, that relates the latent \\(y^*\\) values to the latent \\(x^*\\) values. But then the observation model will include a censoring scheme for the observation of both \\(x\\) and \\(y\\).\nImportantly, in this model we also need to specify a distributional assumption for \\(X\\), otherwise we can’t estimate what the uncensored \\(X\\) values should look like. So for the sake of simplicity, we’ll assume a Gaussian distribution for the \\(x\\)-values as well, although this is definitely something we need to think more about in the future. Furthermore, let’s assume \\(x\\) has a standard normal distribution, since we can standardize \\(x\\) before modeling.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\ny_\\min, & y_i^* \\leq y_\\min \\\\\ny_i^* & y_\\min &lt; y_i^* \\leq y_\\max \\\\\ny_\\max &  y_\\max &lt; y_i^*\n\\end{cases} \\\\\nx_i &= \\begin{cases}\nx_\\min, & x_i^* \\leq x_\\min \\\\\nx_i^* & x_\\min &lt; x_i^* \\leq x_\\max \\\\\nx_\\max &  x_\\max &lt; x_i^*\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}(0, 1)\n\\end{align*}\n\\]\nAgain, we can choose whatever parameters we want for the simulation. I played around with the simulation until I got a plot I thought looked about right. Those simulation parameters are printed below.\nList of 8\n $ n    : num 400\n $ alpha: num 1\n $ beta : num 4\n $ sigma: num 5\n $ y_min: num -9\n $ y_max: num 12\n $ x_min: num -1\n $ x_max: num 2\nSo with those parameters, we can then simulate some data according to this generative model.\n# A tibble: 400 × 5\n    x_star      mu  y_star       x       y\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1.80     8.19   7.08    1.80    7.08  \n 2  1.16     5.64   6.71    1.16    6.71  \n 3  0.155    1.62   6.05    0.155   6.05  \n 4  0.0988   1.40   1.24    0.0988  1.24  \n 5 -3.16   -11.6   -7.35   -1      -7.35  \n 6 -0.682   -1.73  -1.46   -0.682  -1.46  \n 7  1.56     7.25   3.40    1.56    3.40  \n 8 -0.195    0.219 -0.216  -0.195  -0.216 \n 9  0.628    3.51   7.10    0.628   7.10  \n10  0.821    4.28   0.0773  0.821   0.0773\n# ℹ 390 more rows\nSince we’ve simulated the data, we know the latent values and the observed values, so we can plot our simulated data in order to get a better understanding of how much the censoring process will affect our estimates.\nPlotting code\nsim_data |&gt;\n    ggplot() +\n    geom_hline(\n        yintercept = c(sim_parms$y_min, sim_parms$y_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_vline(\n        xintercept = c(sim_parms$x_min, sim_parms$x_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_segment(\n        data = subset(sim_data, (x != x_star) | (y != y_star)),\n        aes(x = x_star, xend = x, y = y_star, yend = y),\n        color = \"gray\",\n        alpha = 0.25,\n        lwd = 1\n    ) +\n    geom_point(aes(x = x_star, y = y_star), color = \"gray\") +\n    geom_point(aes(x = x, y = y)) +\n    coord_cartesian(\n        xlim = c(-3, 3),\n        ylim = c(-22, 22)\n    ) +\n    labs(\n        x = \"Independent variable\",\n        y = \"Dependent variable\"\n    )\nWe can see that a substantial amount of the data points are censored. In total, \\(16.5\\%\\) of records were censored in \\(x\\) only, \\(13.25\\%\\) of records were censored in \\(y\\) only, and \\(5\\%\\) of records were censored in both \\(x\\) and \\(y\\). Thus, \\(24.75\\%\\) of records were censored in some way.\nI also deliberately set the upper and lower limits for both \\(x\\) and \\(y\\) to be asymmetrical so we can more clearly see how our censoring process can strongly bias the estimates: we have more records censored at lower values than higher values, which gives us a shifted window where we observe data.\nSo now that we have the data simulated, we want to try to recover the original parameters with a Bayesian model.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example Model 3: Censored outcome and censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/censored-outcome-and-predictor/index.html#stan-data-setup",
    "href": "examples/censored-outcome-and-predictor/index.html#stan-data-setup",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.2 Stan data setup",
    "text": "4.2 Stan data setup\nI also want to write the Stan code to accept data in a specific format that we want to test. The data should be formatted like the table below.\n\n\n\nX\nX_L\nX_U\nY\nY_L\nY_U\n\n\n\n\n\\(x_1\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_1\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(x_2\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_2\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_n\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_n\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\nHere, \\(x_\\min\\) is the lower limit of detection for \\(x\\) and \\(x_\\max\\) is the upper limit of detection for \\(X\\) (and similar for \\(Y\\)). Eventually, if this is the data format we decide to permanently adopt going forward, we will want to write a suite of helper functions to conveniently get the data in this form. But for now I will do it manually. Fortunately it is quite easy. And if the censoring limits changed for any observations, it would have been easier to store the data in this format in the first place.\n\nstan_data &lt;-\n    sim_data |&gt;\n    dplyr::select(x, y) |&gt;\n    dplyr::mutate(\n        x_l = sim_parms$x_min,\n        x_u = sim_parms$x_max,\n        .after = x\n    ) |&gt;\n    dplyr::mutate(\n        y_l = sim_parms$y_min,\n        y_u = sim_parms$y_max,\n        .after = y\n    )\n\nstan_data |&gt; print(n = 5)\n\n# A tibble: 400 × 6\n        x   x_l   x_u     y   y_l   y_u\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.80      -1     2  7.08    -9    12\n2  1.16      -1     2  6.71    -9    12\n3  0.155     -1     2  6.05    -9    12\n4  0.0988    -1     2  1.24    -9    12\n5 -1         -1     2 -7.35    -9    12\n# ℹ 395 more rows\n\n\nNow we just need to convert the data frame to a list format and add a variable for the number of records.\n\nstan_list &lt;- as.list(stan_data)\nstan_list$N &lt;- nrow(stan_data)\nstr(stan_list)\n\nList of 7\n $ x  : num [1:400] 1.7973 1.1599 0.1547 0.0988 -1 ...\n $ x_l: num [1:400] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ x_u: num [1:400] 2 2 2 2 2 2 2 2 2 2 ...\n $ y  : num [1:400] 7.08 6.71 6.05 1.24 -7.35 ...\n $ y_l: num [1:400] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 ...\n $ y_u: num [1:400] 12 12 12 12 12 12 12 12 12 12 ...\n $ N  : int 400",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example Model 3: Censored outcome and censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/censored-outcome-and-predictor/index.html#stan-code",
    "href": "examples/censored-outcome-and-predictor/index.html#stan-code",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.3 Stan code",
    "text": "4.3 Stan code\nOf course as usual we need to compile the Stan code. The code is also included here for reference.\n\nmod_pth &lt;- here::here(pth_base, \"Ex3.stan\")\nmod &lt;- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod$compile(force_recompile = TRUE)",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example Model 3: Censored outcome and censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/censored-outcome-and-predictor/index.html#model-fitting-and-performance",
    "href": "examples/censored-outcome-and-predictor/index.html#model-fitting-and-performance",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.4 Model fitting and performance",
    "text": "4.4 Model fitting and performance\nNow that the model is successfully compiled, we need to generate MCMC samples from the posterior distribution. We’ll use 4 chains (run in parallel) with 500 warmup iterations and 2500 sampling iterations each, for a total of 10000 samples overall, which should be plenty for this problem. Otherwise, we’ll leave the control parameters at their default values.\n\nfit &lt;- mod$sample(\n    stan_list,\n    seed = 123123,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = FALSE\n)\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkHFdhX/model-5d3c43851799.stan', line 62, column 3 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nAs usual, we want to check the diagnostics, and fortunately cmdstanr gives us an easy to use diagnostic flagger.\n\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpkHFdhX/Ex3-202403032224-1-336e28.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkHFdhX/Ex3-202403032224-2-336e28.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkHFdhX/Ex3-202403032224-3-336e28.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkHFdhX/Ex3-202403032224-4-336e28.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nWe can examine the trace plots and posterior distributions of the parameters of interest to confirm that there is no funny business.\n\npost &lt;- posterior::as_draws_array(fit)\nbayesplot::mcmc_combo(post, par = c(\"alpha\", \"beta\", \"sigma\"))\n\n\n\n\n\n\n\n\nAnd so now we can finally examine the fitted values and compare them to our true simulation values.\n\nfit$summary() |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n0.43\n0.43\n0.26\n0.26\n0.00\n0.86\n1\n8190.67\n7220.56\n\n\nbeta\n4.14\n4.14\n0.29\n0.29\n3.66\n4.63\n1\n9034.90\n7285.48\n\n\nsigma\n5.10\n5.10\n0.19\n0.19\n4.80\n5.42\n1\n9738.63\n7383.51\n\n\n\n\n\nWe can see that our model estimated the slope and variance quite well, although it is not doing too great at figuring out the intercept. In fact, the true value of \\(\\alpha = 1\\) isn’t even in the credible interval. However, the estimates for \\(\\beta\\) and \\(\\sigma\\) are very close to the true estimates. In most applications, the intercept is not too useful and the slope is what we want an accurate estimate of anyway, so this is probably acceptable.\nTODO figure out what else needs to go in this example.",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example Model 3: Censored outcome and censored predictor</span>"
    ]
  },
  {
    "objectID": "examples/censored-outcome-and-multiple-predictors/index.html",
    "href": "examples/censored-outcome-and-multiple-predictors/index.html",
    "title": "5  Censored outcome and multiple predictors",
    "section": "",
    "text": "This is a placeholder",
    "crumbs": [
      "First models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Censored outcome and multiple predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html",
    "href": "examples/nov-data-example/index.html",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "",
    "text": "6.1 Model with one patient group\nFor the first model, we’ll consider a simpler case where we only have one group of patients. This model would be appropriate for, e.g., an observational study where all patients are given the vaccine of interest.\nThe data generating model is as follows:\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p\\right) \\\\\n\\mathrm{logit}(p) &= \\alpha + \\beta \\cdot x_i\n\\end{align*}\n\\] where \\(y_i\\) is a binary outcome where 1 indicates infection and 0 indicates no infection, and \\(x_i\\) is our antibody titer. However, the specific problem we deal with in practice is that these antibody titers tend to have lower limits of detection. Thus, we need to add an observation model to our data generating process to reflect the incomplete observations we obtain of \\(x\\).\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p_i\\right) \\\\\n\\mathrm{logit}(p_i) &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right) \\\\\nx_i &= \\begin{cases}\n\\frac{1}{2}\\mathrm{LoD}, & x^*_i &lt; \\mathrm{LoD} \\\\\nx^*_i, & x^*_i \\geq \\mathrm{LoD}\n\\end{cases} \\\\\n\\end{align*}\n\\] Here, we assume that we work with the \\(x\\) variable on the log scale at all times,\nmostly cause it’s annoying and confusing to write out all the logs every time, so we could also write \\[x_i^* = \\mathrm{log} \\left(z^*_i\\right)\\] and say \\(z^*_i\\) is the actual latent un-logged titer.\nNow that we have the data generating process written out, we can simulate some example data. Note that in this example, we can interpret \\(\\alpha\\) as the log-odds of infection if a person were to have no antibodies. For example, if we assume that this probability is \\(50\\%\\) we would apply the logit transformation to get that \\(\\alpha = 0\\). However, let’s assume that the inoculum dose is quite high and during our subject selection process we’ve included anyone who might have a genetic resistance to the disease (i.e., FUT2- individuals for norovirus). So let’s say if a person has zero antibodies, their probably of getting sick should be \\(90\\%\\). Then, \\(\\log(0.9 / 0.1) \\approx 2.2\\).\nWe then want our true \\(\\beta\\) value to be negative, indicating that as the number of antibodies rise, the log-odds of infection decrease. We can interpret \\(\\beta\\) as the change in the log-odds ratio associated with a one-unit change in antibody titer – the nonlinearity here makes it a bit more difficult to interpret this effect. We can, however, intercept \\(\\exp(\\beta)\\) as the odds ratio between individuals with titer \\(x_i + 1\\) and individuals with titer \\(x_i\\). This corresponds to a nonlinear change in risk that depends on the value of \\(x_i\\). However, if we want the odds of infection to halve for each 1 unit increase in antibody titer, we would set \\(\\beta = -\\log(2) \\approx -0.7\\).\nset.seed(134125)\nsim_parms &lt;- list(\n    n = 110,\n    alpha = 2.2,\n    beta = -1.37,\n    mu_x = 2,\n    sigma_x = 2,\n    LoD = 0\n)\ninv_logit &lt;- function(x) {return(1 / (1 + exp(-x)))}\n\nsim_one_group &lt;- function(n, alpha, beta, mu_x, sigma_x, LoD) {\n    out &lt;- tibble::tibble(\n        x_star = rnorm(n, mu_x, sigma_x),\n        x = ifelse(x_star &lt; LoD, 0.5 * LoD, x_star),\n        p = inv_logit(alpha + beta * x_star),\n        y = rbinom(n, size = 1, prob = p)\n    )\n}\n\nsim_data &lt;- do.call(sim_one_group, sim_parms)\nOf course visualizing the relationship between a binary outcome and a continuous predictor is in some sense more complex than visualizing the relationship between a continuous outcome and a continuous predictor.\nFirst, let’s look at how the distribution of the predictor variable changes if we condition on the outcome.\nsim_data |&gt;\n    tidyr::pivot_longer(cols = c(x, x_star)) |&gt;\n    dplyr::mutate(\n        yf = factor(\n            y,\n            levels = c(0, 1),\n            labels = c(\"Not infected\", \"Infected\")\n        ),\n        name = factor(\n            name,\n            levels = c(\"x_star\", \"x\"),\n            labels = c(\"Latent variable\", \"Observed variable\")\n        )\n    ) |&gt;\n    ggplot() +\n    aes(x = value, fill = yf) +\n    geom_vline(\n        xintercept = 0,\n        linetype = \"dashed\",\n        color = \"black\",\n        linewidth = 1\n    ) +\n    geom_histogram(\n        binwidth = 0.5, boundary = 0, closed = \"left\",\n        position = \"identity\", alpha = 0.6,\n        color = \"black\"\n    ) +\n    scale_x_continuous(\n        name = \"Simulated log titer\",\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_y_continuous(breaks = scales::breaks_pretty()) +\n    scale_fill_brewer(palette = \"Dark2\", name = NULL) +\n    facet_wrap(facets = vars(name))\nEssentially everything below our lower threshold gets bumped up, which will make the summary statistics of the distribution more similar between groups for the observed titer values than they would have been for the latent titer values. However, we can still see a large difference.\ninterp &lt;-\n    tibble::tibble(\n        value = seq(-2, 6, 0.1),\n        p = inv_logit(sim_parms$alpha + sim_parms$beta * value)\n    )\n\ninterp2 &lt;-\n    dplyr::bind_rows(\n        \"Latent variable\" = interp,\n        \"Observed variable\" = interp,\n        .id = \"name\"\n    )\n\nlab1 &lt;- latex2exp::TeX(r\"($Pr(y_{i} = 1 \\ | \\ x_{i})$)\")\n\nsim_data |&gt;\n    tidyr::pivot_longer(cols = c(x, x_star)) |&gt;\n    dplyr::mutate(\n        yf = factor(\n            y,\n            levels = c(0, 1),\n            labels = c(\"Not infected\", \"Infected\")\n        ),\n        name = factor(\n            name,\n            levels = c(\"x_star\", \"x\"),\n            labels = c(\"Latent variable\", \"Observed variable\")\n        )\n    ) |&gt;\n    ggplot() +\n    aes(x = value, color = name) +\n    geom_line(\n        data = interp2, aes(y = p), color = \"darkgray\", linetype = 2, linewidth = 1\n    ) +\n    geom_point(\n        aes(y = y), size = 3, alpha = 0.5,\n        position = position_jitter(width = 0, height = 0.05, seed = 370)\n    ) +\n    scale_x_continuous(\n        name = \"Simulated log titer\",\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_y_continuous(\n        name = lab1,\n        breaks = scales::breaks_pretty()\n    ) +\n    scale_color_brewer(palette = \"Accent\", name = NULL) +\n    facet_wrap(facets = vars(name))\ndata_list &lt;-\n    sim_data |&gt;\n    dplyr::mutate(x_l = sim_parms$LoD) |&gt;\n    dplyr::select(x, x_l, y) |&gt;\n    as.list()\n\ndata_list$N &lt;- sim_parms$n\n\nstr(data_list)\n\nList of 4\n $ x  : num [1:110] 0.123 1.872 0.932 1.865 1.925 ...\n $ x_l: num [1:110] 0 0 0 0 0 0 0 0 0 0 ...\n $ y  : int [1:110] 1 0 1 0 0 0 1 0 0 0 ...\n $ N  : num 110\nmod_pth &lt;- here::here(pth_base, \"Ex4a.stan\")\nmod &lt;- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod$compile(force_recompile = TRUE, pedantic = TRUE)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/model-374c30036286.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\nfit &lt;- mod$sample(\n    data_list,\n    seed = 25452345,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/model-374c30036286.stan', line 59, column 3 to column 32)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 0.3 seconds.\nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 finished in 0.4 seconds.\nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.6 seconds.\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/Ex4a-202403032225-1-7db877.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/Ex4a-202403032225-2-7db877.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/Ex4a-202403032225-3-7db877.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/Ex4a-202403032225-4-7db877.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\nfit$summary()\n\n# A tibble: 5 × 10\n  variable    mean  median    sd   mad      q5      q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -190.   -189.   1.44  1.21  -192.   -188.     1.00    4408.    5589.\n2 alpha       1.91    1.90 0.411 0.416    1.26    2.60   1.00    5508.    5501.\n3 beta       -1.23   -1.22 0.226 0.226   -1.62   -0.881  1.00    5323.    5512.\n4 mu_x        1.84    1.85 0.234 0.231    1.45    2.22   1.00    8886.    6064.\n5 sigma_x     2.32    2.31 0.177 0.174    2.05    2.63   1.00    9161.    6999.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#effect-of-vaccine",
    "href": "examples/nov-data-example/index.html#effect-of-vaccine",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.2 Effect of vaccine",
    "text": "6.2 Effect of vaccine\nOf course, a more interesting question is when we have \\(k\\) different treatment groups. These groups could be vaccine and placebo, like the example that motivated this project, or they could be multiple different vaccine candidates, doses, etc. So we now need to incorporate the effect of the treatment into the model. However, we know that the treatment will have a direct effect on \\(x\\), the antibody titers, and we can add a direct effect on \\(y\\) to represent the combined effect of the vaccine on other facets of the immune system (e.g. cell-mediated responses) which explain variations in infection risk that are not due to antibodies.\nIn this framework, \\(x\\) becomes a mediator of the relationship between \\(t\\), the treatment, and \\(y\\). For simplicity, we model the effect of \\(t\\) on \\(x\\), and the effect of \\(t\\) and \\(x\\) jointly on \\(y\\), both as linear functions of the predictors. Specifically, the data generating model is given as follows.\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p_i\\right) \\\\\n\\mathrm{logit}(p_i) &= \\beta_{1, T[i]} + \\beta_{2, T[i]} \\cdot x^*_i \\\\\n\\log\\left(x_i^*\\right) &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right) \\\\\n\\mu_x &= \\alpha_{T[i]} \\\\\nx_i &= \\begin{cases}\n\\frac{1}{2}\\mathrm{LoD}, & x^*_i &lt; \\mathrm{LoD} \\\\\nx^*_i, & x^*_i \\geq \\mathrm{LoD}\n\\end{cases} \\\\\nT[i] &= \\begin{cases}\n1, & \\text{individual } i \\text{ is in the placebo group} \\\\\n2, & \\text{individual } i \\text{ is in the vaccine group}\n\\end{cases}\n\\end{align*}\n\\]\nGiven the generative model, we can simulate data which follow our assumptions.\n\nset.seed(341341)\n# Some parameters are commented out because I originally had a global\n# intercept for mu and for p, but then the intercept parameters are\n# nonidentifiable under index coding as written.\nsim2_parms &lt;- list(\n    n = 116,\n    #a0 = 2,\n    a1 = c(2.5, 4),\n    #b0 = 1.5,\n    b1 = c(1.7, 2.2),\n    b2 = c(-0.67, -1.37),\n    sigma_x = 1.5,\n    LoD = 3,\n    latent = TRUE\n)\nsim_two_groups &lt;- function(n, b1, b2, a1, sigma_x, LoD,\n                                                     latent = TRUE) {\n    out &lt;- tibble::tibble(\n        # Randomly assign each individual to 1 (placebo) or 2 (vaccine)\n        t = rbinom(n, size = 1, prob = 0.5) + 1,\n        mu = a1[t],\n        x_star = rnorm(n, mu, sigma_x),\n        x = dplyr::if_else(x_star &lt; LoD, 0.5 * LoD, x_star),\n        p = inv_logit(b1[t] + b2[t] * x_star),\n        y = rbinom(n, 1, prob = p)\n    )\n    \n    # If the arg 'latent' is specified as anything other than FALSE, return the\n    # latent variables that we don't observe. Otherwise return only (X, y).\n    if (isFALSE(latent)) {\n        out &lt;- out |&gt; dplyr::select(t, x, y)\n    }\n    \n    return(out)\n}\nsim_data_4b &lt;- do.call(sim_two_groups, sim2_parms)\n\n\ntab_dat &lt;-\n    sim_data_4b |&gt;\n    dplyr::mutate(\n        t = factor(\n            t,\n            levels = c(2, 1),\n            labels = c(\"Vaccine\", \"Placebo\")\n        ),\n        y = factor(\n            y,\n            levels = c(1, 0),\n            labels = c(\"Infected\", \"Not infected\")\n        )\n    )\ntab_dat |&gt;\n    gtsummary::tbl_cross(\n        row = t, col = y,\n        label = list(t ~ \"Treatment\", y ~ \"Outcome\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nTotal\n\n\nInfected\nNot infected\n\n\n\n\nTreatment\n\n\n\n\n\n\n\n\n    Vaccine\n5\n49\n54\n\n\n    Placebo\n28\n34\n62\n\n\nTotal\n33\n83\n116\n\n\n\n\n\n\n\nBecause the data are from a (hypothetical) clinical trial, the typical epidemiological approach to data analysis, if we do not care about the effect of the mediator \\(x\\) would be to calculate the risk ratio.\n\ntab &lt;- table(tab_dat$t, tab_dat$y, dnn = c(\"Treatment\", \"Outcome\"))\nepiR_out &lt;- epiR::epi.2by2(\n    tab,\n    method = \"cohort.count\"\n)\nepiR_out\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +            5           49         54       9.26 (3.08 to 20.30)\nExposed -           28           34         62     45.16 (32.48 to 58.32)\nTotal               33           83        116     28.45 (20.46 to 37.57)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.21 (0.09, 0.49)\nInc odds ratio                                 0.12 (0.04, 0.35)\nAttrib risk in the exposed *                   -35.90 (-50.50, -21.30)\nAttrib fraction in the exposed (%)            -387.74 (-1074.55, -102.54)\nAttrib risk in the population *                -16.71 (-31.57, -1.85)\nAttrib fraction in the population (%)         -58.75 (-89.23, -33.18)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 18.276 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n\n# This one takes the variables in the opposite direction so easier to do it\n# this way\nepiDisplay::csi(\n    caseexp = tab[[1]],\n    controlex = tab[[3]],\n    casenonex = tab[[2]],\n    controlnonex = tab[[4]]\n)\n\n\n          Exposure\nOutcome    Non-exposed Exposed Total\n  Negative 34          49      83   \n  Positive 28          5       33   \n  Total    62          54      116  \n                                    \n           Rne         Re      Rt   \n  Risk     0.45        0.09    0.28 \n                                         Estimate Lower95ci Upper95ci\n Risk difference (Re - Rne)              -0.36    -0.5      -0.2     \n Risk ratio                              0.21     0.1       0.45     \n Protective efficacy =(Rne-Re)/Rne*100   79.5     55.46     90.1     \n   or percent of risk reduced                                        \n Number needed to treat (NNT)            2.79     2         4.99     \n   or -1/(risk difference)                                           \n\n\nSo if we didn’t care about the effect of \\(x\\) at all, we would conclude that the vaccine appears to be protective with a RR of \\(`r\nround(epiR_out\\)massoc.summary[[1, 2]], 2)$ and a 95% CI of $\\left(r round(epiR_out\\(massoc.summary[[1, 3]], 2) - round(epiR_out\\)massoc.summary[[1, 4]], 2)`)$. Note that this analysis is marginal to the censored \\(x_i\\) values, and since the data generating process for \\(y_i\\) relies on the latent \\(x^*_i\\) values, this analysis should not be biased by the censoring process.\nHowever, in our study we specifically want to know how much of the lower risk is explained by the antibody titer, and how much is not. This analysis is more complicated, and requires us to use a regression model. Fortunately we know the data generating process, so writing the Stan code for an accurate model is not too hard.\n\n\nmod_pth &lt;- here::here(pth_base, \"Ex4b.stan\")\nmod4b &lt;- cmdstanr::cmdstan_model(mod_pth, compile = F)\nmod4b$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/model-374c53c17b4c.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\n\nmod4b_data &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = sim2_parms$LoD, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x, x_l) |&gt;\n    as.list()\n\nmod4b_data &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data$t)),\n    mod4b_data\n)\nstr(mod4b_data)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 1.5 3.19 1.5 3.79 4.7 ...\n $ x_l: num [1:116] 3 3 3 3 3 3 3 3 3 3 ...\n\npaste0(\n    \"Naruto checked the data and he says:\\n\",\n    round(mean(mod4b_data$x &lt;= mod4b_data$x_l), 4) * 100,\n    \"% of x values are below the LoD!\\nBelieve it!\"\n) |&gt; cat()\n\nNaruto checked the data and he says:\n46.55% of x values are below the LoD!\nBelieve it!\n\n\n\nfit4b &lt;- mod4b$sample(\n    mod4b_data,\n    seed = 5234521,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/model-374c53c17b4c.stan', line 75, column 3 to column 50)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmp0aU2FX/model-374c53c17b4c.stan', line 75, column 3 to column 50)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 finished in 1.7 seconds.\nChain 3 finished in 1.6 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.8 seconds.\nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 1.9 seconds.\n\n\n\nfit4b$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad       q5       q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__       -157.    -157.    1.95  1.85  -161.    -154.      1.00    4215.\n2 alpha_1[1]    2.45     2.46  0.292 0.284    1.95     2.90    1.00    8685.\n3 alpha_1[2]    3.85     3.85  0.254 0.256    3.42     4.26    1.00    9349.\n4 sigma_x       1.75     1.74  0.181 0.175    1.48     2.07    1.00    8116.\n5 beta_1[1]     0.760    0.758 0.530 0.536   -0.102    1.65    1.00    6824.\n6 beta_1[2]     1.45     1.40  1.17  1.15    -0.351    3.47    1.00    5524.\n7 beta_2[1]    -0.397   -0.392 0.197 0.197   -0.732   -0.0818  1.00    6880.\n8 beta_2[2]    -1.69    -1.61  0.666 0.640   -2.94    -0.752   1.00    5384.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary &lt;- fit4b$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npo &lt;-\n    ggplot(fit_summary) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated with observed (censored) values with correction\"\n    )",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#model-if-x-was-not-censored",
    "href": "examples/nov-data-example/index.html#model-if-x-was-not-censored",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.3 Model if x was not censored",
    "text": "6.3 Model if x was not censored\n\nmod4b_data_l &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = -9999, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x = x_star, x_l) |&gt;\n    as.list()\n\nmod4b_data_l &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data_l$t)),\n    mod4b_data_l\n)\nstr(mod4b_data_l)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 2.07 3.19 2.2 3.79 4.7 ...\n $ x_l: num [1:116] -9999 -9999 -9999 -9999 -9999 ...\n\n\n\nfit4b_l &lt;- mod4b$sample(\n    mod4b_data_l,\n    seed = 5234521,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 finished in 1.4 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.5 seconds.\nChain 3 finished in 1.5 seconds.\nChain 4 finished in 1.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.5 seconds.\nTotal execution time: 1.6 seconds.\n\n\n\nfit4b_l$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__       -166.    -165.    1.87  1.73  -169.    -163.     1.00    4661.\n2 alpha_1[1]    2.63     2.64  0.204 0.203    2.30     2.97   1.00   11142.\n3 alpha_1[2]    3.90     3.91  0.214 0.213    3.55     4.25   1.00   12564.\n4 sigma_x       1.61     1.61  0.108 0.108    1.44     1.79   1.00   10927.\n5 beta_1[1]     1.50     1.48  0.599 0.597    0.545    2.51   1.00    7405.\n6 beta_1[2]     1.21     1.19  1.09  1.09    -0.530    3.06   1.00    7679.\n7 beta_2[1]    -0.673   -0.661 0.213 0.216   -1.04    -0.342  1.00    7502.\n8 beta_2[2]    -1.26    -1.22  0.449 0.446   -2.06    -0.603  1.00    7520.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary_l &lt;- fit4b_l$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npl &lt;-\n    ggplot(fit_summary_l) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated using true latent values\"\n    )",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#do-it-the-naive-way",
    "href": "examples/nov-data-example/index.html#do-it-the-naive-way",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.4 Do it the naive way",
    "text": "6.4 Do it the naive way\n\nmod4b_data_n &lt;- sim_data_4b |&gt;\n    dplyr::mutate(x_l = -9999, t = as.integer(t)) |&gt;\n    dplyr::select(t, y, x = x, x_l) |&gt;\n    as.list()\n\nmod4b_data_n &lt;- c(\n    \"N\" = nrow(sim_data_4b),\n    \"k\" = as.integer(max(mod4b_data_n$t)),\n    mod4b_data_n\n)\nstr(mod4b_data_n)\n\nList of 6\n $ N  : int 116\n $ k  : int 2\n $ t  : int [1:116] 1 1 2 1 1 1 2 2 2 2 ...\n $ y  : int [1:116] 1 0 0 0 0 0 0 0 0 0 ...\n $ x  : num [1:116] 1.5 3.19 1.5 3.79 4.7 ...\n $ x_l: num [1:116] -9999 -9999 -9999 -9999 -9999 ...\n\n\n\nfit4b_n &lt;- mod4b$sample(\n    mod4b_data_n,\n    seed = 873215,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 finished in 1.5 seconds.\nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 1.6 seconds.\nChain 3 finished in 1.5 seconds.\nChain 4 finished in 1.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.5 seconds.\nTotal execution time: 1.7 seconds.\n\n\n\nfit4b_n$summary()\n\n# A tibble: 8 × 10\n  variable       mean   median    sd   mad        q5       q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__       -170.    -169.    1.89  1.73  -173.     -167.      1.00    4136.\n2 alpha_1[1]    2.53     2.53  0.209 0.204    2.19      2.87    1.00   10825.\n3 alpha_1[2]    3.75     3.75  0.217 0.215    3.39      4.11    1.00   10929.\n4 sigma_x       1.63     1.62  0.107 0.106    1.46      1.81    1.00   10108.\n5 beta_1[1]     0.776    0.768 0.521 0.522   -0.0606    1.64    1.00    6724.\n6 beta_1[2]     1.45     1.40  1.17  1.13    -0.390     3.46    1.00    6954.\n7 beta_2[1]    -0.404   -0.396 0.194 0.191   -0.731    -0.0970  1.00    6755.\n8 beta_2[2]    -1.69    -1.61  0.666 0.649   -2.92     -0.748   1.00    6854.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nstr(sim2_parms)\n\nList of 7\n $ n      : num 116\n $ a1     : num [1:2] 2.5 4\n $ b1     : num [1:2] 1.7 2.2\n $ b2     : num [1:2] -0.67 -1.37\n $ sigma_x: num 1.5\n $ LoD    : num 3\n $ latent : logi TRUE\n\n\n\nfit_summary_n &lt;- fit4b_n$summary() |&gt;\n    dplyr::select(variable, median, q5, q95) |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    dplyr::mutate(\n        truth = c(\n            #sim2_parms$a0,\n            sim2_parms$a1[[1]],\n            sim2_parms$a1[[2]],\n            sim2_parms$sigma_x,\n            #sim2_parms$b0,\n            sim2_parms$b1[[1]],\n            sim2_parms$b1[[2]],\n            sim2_parms$b2[[1]],\n            sim2_parms$b2[[2]]\n        )\n    )\n\npn &lt;-\n    ggplot(fit_summary_n) +\n    aes(x = variable, y = median, ymin = q5, ymax = q95) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_pointrange() +\n    geom_point(aes(y = truth), shape = 4, color = \"red\", size = 3, stroke = 1) +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; x marks true simulation value\",\n        subtitle = \"Estimated using censored values without censoring correction\"\n    )\n\n\npo / pl / pn\n\n\n\n\n\n\n\n\n\nall_fits &lt;-\n    dplyr::bind_rows(\n        \"corrected\" = fit_summary,\n        \"latent\" = fit_summary_l,\n        \"naive\" = fit_summary_n,\n        .id = \"model\"\n    )\n\nall_fits |&gt;\n    ggplot() +\n    aes(\n        x = variable, y = median, ymin = q5, ymax = q95,\n        color = model\n    ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    geom_crossbar(\n        aes(y = truth, ymin = truth, ymax = truth),\n        width = 0.5,\n        color = \"black\",\n        fatten = 0.1\n    ) +\n    geom_pointrange(position = position_dodge2(width = 0.3)) +\n    scale_color_brewer(palette = \"Dark2\") +\n    labs(\n        x = NULL,\n        y = \"Parameter value\",\n        title = \"Model-estimated median with 95% CI; line marks true value\"\n    )",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#model-exploration",
    "href": "examples/nov-data-example/index.html#model-exploration",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.5 Model exploration",
    "text": "6.5 Model exploration\nthese curves are implied by the model and parameters, not by the simulated data ::: {.cell}\nx_vec &lt;- seq(-6, 12, 0.01)\nr1 &lt;- sapply(x_vec, \\(x) inv_logit(1.5 + 0.2 - 0.67 * x))\nr2 &lt;- sapply(x_vec, \\(x) inv_logit(1.5 + 0.7 - 1.37 * x))\n\nlayout(matrix(c(1, 2, 3, 3), ncol = 2, byrow = TRUE))\nplot(x_vec, r2 - r1, ylab = \"risk difference\", type = \"l\", xlab = \"\")\nabline(h = 0, lty = 2)\nplot(x_vec, r2 / r1, ylab = \"risk ratio\", type = \"l\", xlab = \"\")\nabline(h = 1, lty = 2)\nlab2 &lt;- latex2exp::TeX(r\"($Pr(y_{i} = 1 \\ | \\ x_{i}, T_{i})$)\")\nplot(\n    NULL, NULL,\n    ylim = c(0, 1),\n    xlim = c(-6, 12),\n    yaxs = \"i\",\n    xaxs = \"i\",\n    xlab = \"Simulated log titer\",\n    ylab = lab2\n)\nlines(x_vec, r1, lty = 2, lwd = 1.5) # placebo\nlines(x_vec, r2, lty = 1, lwd = 1.5) # vaccine\n\n\n\n\n\n\n\n# IDK what's wrong with the legend, seems it doesn't like layout.\n# switch to ggplot to fix\n#legend(x = 9, y = 0.8, c('Unexposed', 'Exposed'), lty = c(2, 1), lwd = 2)\n:::\n\nx_dens &lt;-\n    tibble::tibble(\n        Latent = x_vec,\n        Observed = dplyr::if_else(\n            x_vec &lt; sim2_parms$LoD,\n            sim2_parms$LoD,\n            x_vec\n        ),\n        Placebo = sim2_parms$a1[1],\n        Vaccine = sim2_parms$a1[2]\n    ) |&gt;\n    tidyr::pivot_longer(\n        cols = c(Placebo, Vaccine),\n        names_to = \"t\",\n        values_to = \"mu\"\n    ) |&gt;\n    tidyr::pivot_longer(\n        cols = c(Latent, Observed),\n        names_to = \"o\",\n        values_to = \"x\"\n    ) |&gt;\n    dplyr::mutate(\n        d = dplyr::if_else(\n            o == \"Latent\",\n            dnorm(x, mean = mu, sd = sim2_parms$sigma_x),\n            crch::dcnorm(\n                x, mean = mu, sd = sim2_parms$sigma_x,\n                left = sim2_parms$LoD, right = Inf\n            )\n        )\n    )\n\nanno_df &lt;-\n    x_dens[1:4, ] |&gt;\n    dplyr::filter(o == \"Observed\")\n\nx_dens |&gt;\n    ggplot() +\n    aes(x = x, y = d, linetype = t, group = t) +\n    geom_vline(\n        xintercept = sim2_parms$LoD,\n        linetype = 1, linewidth = 1, color = \"gray\"\n    ) +\n    geom_line(linewidth = 1.5) +\n    geom_point(\n        data = anno_df,\n        size = 2,\n        stroke = 2,\n        shape = 21,\n        color = \"black\",\n        fill = \"darkgray\"\n    ) +\n    facet_grid(vars(o), vars(t)) +\n    scale_linetype_discrete(name = NULL) +\n    scale_x_continuous(breaks = scales::breaks_pretty()) +\n    scale_y_continuous(breaks = scales::breaks_pretty()) +\n    labs(\n        x = \"Simulated log titer\",\n        y = \"Implied probability density\"\n    ) +\n    #coord_cartesian(expand = FALSE, ylim = c(-0.01, 0.28)) +\n    theme(axis.text.y = element_text(size = 10))",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#try-gamma-dist.-for-x-on-non-logged-scale",
    "href": "examples/nov-data-example/index.html#try-gamma-dist.-for-x-on-non-logged-scale",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.6 Try gamma dist. for x on non-logged scale?",
    "text": "6.6 Try gamma dist. for x on non-logged scale?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/nov-data-example/index.html#do-we-want-to-work-out-a-hierarchical-model",
    "href": "examples/nov-data-example/index.html#do-we-want-to-work-out-a-hierarchical-model",
    "title": "6  Example Model 4: Logistic regression with censored predictors",
    "section": "6.7 Do we want to work out a hierarchical model?",
    "text": "6.7 Do we want to work out a hierarchical model?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example Model 4: Logistic regression with censored predictors</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html",
    "href": "examples/interval-censoring-weibull/index.html",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "",
    "text": "7.1 Data generating process\nFor this model, let’s assume that the rate of failure, \\(\\lambda_i\\) is a linear function of some covariate \\(x_i\\). I don’t know that much about machines or what would realistically cause them to fail, but we don’t want to make the example too hard at this point, so we want \\(x_i\\) to be an inherent characteristic of the machine. Let’s say \\(x_i\\) is some integer number from \\(1\\) to \\(5\\) that controls how the machine works – it is inherent to the type of machine. If I can think of a good variable that might work for this, I’ll update that later. The expected failure time increases linearly by some amount \\(\\beta\\) for each unit of increase in \\(x_i\\). There is also some baseline expected failure rate \\(\\alpha\\) shared by all of the machines. So the expected log failure time (if the failure time were constant) can be given as \\[\n\\log(\\lambda_i) = \\alpha + \\beta x_i.\n\\] We use the log link function here to ensure that expected failure times are always positive, while the function of \\(x_i\\) does not necessarily need to be.\nFinally, we assume that the longer each machine operates without being repaired, the more likely the machine is to fail. We represent this by modeling the failure time as a Weibull distribution with constant parameter \\(k\\), which influences how quickly the failure rate changes over time. We assume this parameter is the same for all machines, which are identical other than the setting \\(x_i\\).\nSo then if our failure times were completely observed, the data generating process would be as follows.\n\\[\n\\begin{align*}\nt^*_i &\\sim \\mathrm{Weibull}\\left(k, \\lambda_i\\right) \\\\\n\\lambda_i &= \\alpha + \\beta x_i\n\\end{align*}\n\\]\nBut recall what I said before about running inspections only once a week. If this were the case, assuming our failure times are also measured in weeks, we would also have to apply a censoring mechanism for the data we observe, given by \\[t_i = \\lceil t_i^* \\rceil.\\]\nWe will deal with the interval censoring, as usual, by modifying the likelihood of the outcome. For a completely observed outcome \\(t^*_i\\), the likelihood would be \\[\n\\mathcal{L}\\left(\\theta \\mid t_i^*\\right) = f(t_i^* \\mid \\theta),\n\\] where \\(f(\\cdot)\\) is the Weibull density function. However, a censored data point actually lies at a point mass of probability and for our censored observation, the contribution to the likelihood is \\[\n\\mathcal{L}\\left(\\theta \\mid t_i\\right) = \\mathrm{Pr}\\left(\nL_i&lt; t_i \\leq U_i\n\\right) = \\int_{L_i}^{U_i}f_{T_i}(\\tau) \\ d\\tau = F_{T_i}(U_i) - F_{T_i}(L_i),\n\\] where \\(L_i = \\lfloor t_i^*\\rfloor = t_i - 1\\) and \\(U_i = \\lceil t_i^* \\rceil = t_i\\), both of which are assumed to be known constants after the data are observed.\nThe Stan code for this would be {.stan} target += log_diff_exp(     weibull_lcdf(y[i] | k, lambda[i]),     weibull_lcdf(y[i] - 1 | k, lambda[i]) ) ``` or equivalently target += log_diff_exp( weibull_lcdf(y_u[i] | k, lambda[i]), weibull_lcdf(y_l[i] | k, lambda[i]) )",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#data-generating-process",
    "href": "examples/interval-censoring-weibull/index.html#data-generating-process",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "",
    "text": "if the data is specified in the format we prefer. Note that we use the Stan\ninternal function `log_diff_exp()` for increased numerical precision rather than\ndividing the log values of the two functions manually. Note than there are[some\nconcerns](https://discourse.mc-stan.org/t/interval-censored-data-fails-with-weibull-but-not-gamma/28780)\nabout the numerical stability of the Weibull CDF function implemented in Stan,\nwith a [GitHub issue](https://github.com/stan-dev/math/issues/2810) for\nimproving the stability, open at time of writing. For improved numerical\nstability, we can rewrite the `weibull_lcdf()` function using Stan's newer math\nfunctions with improved stability.\n````{.stan}\nfunctions {\n  real my_weibull_lcdf(real y, real alpha, real sigma) {\n    return log1m_exp(-pow(y / sigma, alpha));\n  }\n}",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#data-simulation",
    "href": "examples/interval-censoring-weibull/index.html#data-simulation",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.2 Data simulation",
    "text": "7.2 Data simulation\nSo with the data generating process, including the censoring mechanism, written down, we can simulate some data. As usual, I decided to just plot the data and mess around with the parameters until I thought it looked right.\n\nset.seed(2384590)\nsim_parms &lt;- list(\n    n = 210,\n    k = 1.5,\n    alpha = 2,\n    beta = -0.35\n)\n\nstr(sim_parms)\n\nList of 4\n $ n    : num 210\n $ k    : num 1.5\n $ alpha: num 2\n $ beta : num -0.35\n\n\n\ngen_data &lt;- function(n, k, alpha, beta) {\n    out &lt;- tibble::tibble(\n        x = sample(\n            1:5, size = n, replace = TRUE,\n            prob = c(0.4, 0.25, 0.2, 0.1, 0.05)\n        ),\n        l_lambda = alpha + beta * x,\n        lambda = exp(l_lambda),\n        t_star = rweibull(n, shape = k, scale = lambda),\n        t = ceiling(t_star)\n    )\n    \n    return(out)\n}\n\nsim_data &lt;- do.call(gen_data, sim_parms)\nprint(sim_data, n = 5)\n\n# A tibble: 210 × 5\n      x l_lambda lambda t_star     t\n  &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     4     0.6    1.82  0.516     1\n2     3     0.95   2.59  3.32      4\n3     4     0.6    1.82  2.06      3\n4     1     1.65   5.21 11.0      11\n5     1     1.65   5.21  9.95     10\n# ℹ 205 more rows\n\nplot(jitter(sim_data$x), sim_data$t)\n\n\n\n\n\n\n\nsim_data |&gt; dplyr::group_by(x) |&gt; dplyr::summarise(eft = mean(t))\n\n# A tibble: 5 × 2\n      x   eft\n  &lt;int&gt; &lt;dbl&gt;\n1     1  5.82\n2     2  4.26\n3     3  3.29\n4     4  2.19\n5     5  1.75\n\n\n\nsim_data |&gt;\n    dplyr::mutate(\n        x_jitter = x + rnorm(nrow(sim_data), 0, 0.1)\n    ) |&gt;\n    ggplot2::ggplot() +\n    aes(x = x_jitter, y = t_star, group = (x)) +\n    geom_point() +\n    geom_segment(\n        aes(x = x_jitter, xend = x, y = t_star, yend = t)\n    )\n\n\n\n\n\n\n\n    geom_count(shape = 21, fill = \"#ffffff50\")\n\ngeom_point: na.rm = FALSE\nstat_sum: na.rm = FALSE\nposition_identity \n\n\nNEED TO LOOK AT HOW BRMS HANDLES INTERVAL CENSORING https://discourse.mc-stan.org/t/mixed-right-left-and-interval-censored-log-normal-with-brms/27571",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#fitting-latent-data",
    "href": "examples/interval-censoring-weibull/index.html#fitting-latent-data",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.3 Fitting latent data",
    "text": "7.3 Fitting latent data\n\ndat_latent &lt;- list()\ndat_latent$N &lt;- nrow(sim_data)\ndat_latent$x &lt;- sim_data$x\ndat_latent$y &lt;- sim_data$t_star\n\nstr(dat_latent)\n\nList of 3\n $ N: int 210\n $ x: int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y: num [1:210] 0.516 3.321 2.061 10.966 9.951 ...\n\n\n\nmod_pth &lt;- here::here(pth_base, \"Ex5a.stan\")\nmod_l &lt;- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod_l$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\n\nfit_l &lt;- mod_l$sample(\n    dat_latent,\n    seed = 546465,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 finished in 1.3 seconds.\nChain 3 finished in 1.3 seconds.\nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.5 seconds.\nChain 4 finished in 1.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.7 seconds.\n\n\n\nfit_l$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -453.    -453.    1.21   0.970  -456.    -452.     1.00    4128.\n2 alpha       2.14     2.14  0.0997 0.101     1.98     2.30   1.00    3284.\n3 beta       -0.360   -0.360 0.0392 0.0396   -0.424   -0.296  1.00    3247.\n4 k           1.45     1.45  0.0782 0.0786    1.32     1.58   1.00    6020.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#naive-method-use-the-same-model-for-censored-data",
    "href": "examples/interval-censoring-weibull/index.html#naive-method-use-the-same-model-for-censored-data",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.4 Naive method – use the same model for censored data",
    "text": "7.4 Naive method – use the same model for censored data\n\ndat_naive &lt;- dat_latent\ndat_naive$y &lt;- sim_data$t\nstr(dat_naive)\n\nList of 3\n $ N: int 210\n $ x: int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y: num [1:210] 1 4 3 11 10 1 3 4 2 4 ...\n\n\n\nfit_n &lt;- mod_l$sample(\n    dat_naive,\n    seed = 546465,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: weibull_lpdf: Scale parameter is inf, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Shape parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: weibull_lpdf: Scale parameter is 0, but must be positive finite! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f1819e036e0.stan', line 38, column 2 to column 31)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 1.4 seconds.\nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 1.6 seconds.\nChain 3 finished in 1.6 seconds.\nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 1.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.6 seconds.\nTotal execution time: 1.8 seconds.\n\n\n\nfit_n$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -461.    -460.    1.24   1.02   -463.    -459.     1.00    3681.\n2 alpha       2.19     2.19  0.0816 0.0820    2.06     2.33   1.00    4263.\n3 beta       -0.307   -0.307 0.0319 0.0316   -0.360   -0.254  1.00    4313.\n4 k           1.76     1.76  0.0948 0.0935    1.61     1.92   1.00    5299.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#fitting-censored-data",
    "href": "examples/interval-censoring-weibull/index.html#fitting-censored-data",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.5 Fitting censored data",
    "text": "7.5 Fitting censored data\nIn this parametrization, the outcome variable \\(t_i\\)Note that this measurement process will generate an integer valued response. However, we know that if the value of \\(t_i = t\\), then in reality \\(t_i \\in (t - 1, t]\\), and we can never know the true value of \\(t_i\\) because we don’t do inspections more often.\n\ndat_censored &lt;- list()\ndat_censored$N &lt;- nrow(sim_data)\ndat_censored$x &lt;- sim_data$x\ndat_censored$y1 &lt;- sim_data$t - 1\ndat_censored$y2 &lt;- sim_data$t\n\nstr(dat_censored)\n\nList of 4\n $ N : int 210\n $ x : int [1:210] 4 3 4 1 1 4 3 1 1 1 ...\n $ y1: num [1:210] 0 3 2 10 9 0 2 3 1 3 ...\n $ y2: num [1:210] 1 4 3 11 10 1 3 4 2 4 ...\n\n\n\nmod_pth &lt;- here::here(pth_base, \"Ex5b.stan\")\nmod_c &lt;- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod_c$compile(pedantic = TRUE, force_recompile = TRUE)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpkPQFPJ/model-4f187f6c53aa.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\n\nfit_c &lt;- mod_c$sample(\n    dat_censored,\n    seed = 3248315,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = T\n)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Rejecting initial value:\n\n\nChain 1   Log probability evaluates to log(0), i.e. negative infinity.\n\n\nChain 1   Stan can't start sampling from this initial value.\n\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 finished in 3.0 seconds.\nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 3.5 seconds.\nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 3.8 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 3.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.5 seconds.\nTotal execution time: 4.0 seconds.\n\n\n\nfit_c$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -458.    -457.    1.26   1.02   -460.    -456.     1.00    4046.\n2 alpha       2.12     2.12  0.101  0.0977    1.96     2.29   1.00    3599.\n3 beta       -0.355   -0.354 0.0401 0.0399   -0.421   -0.288  1.00    3669.\n4 k           1.44     1.44  0.0874 0.0859    1.30     1.59   1.00    5322.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#making-the-interval-wider",
    "href": "examples/interval-censoring-weibull/index.html#making-the-interval-wider",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.6 Making the interval wider",
    "text": "7.6 Making the interval wider\nWe can introduce more",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/interval-censoring-weibull/index.html#midpoint-correction",
    "href": "examples/interval-censoring-weibull/index.html#midpoint-correction",
    "title": "7  Example Model 5: Interval Censoring",
    "section": "7.7 Midpoint correction",
    "text": "7.7 Midpoint correction\n\n7.7.1 Original interval\n\n\n7.7.2 Wider interval",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Model 5: Interval Censoring</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html",
    "href": "examples/HAI-data-example/index.html",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "",
    "text": "8.1 Log-normal model\nBoth of these models will share a lot of similarities, but for some reason I find the lognormal model easier to think about, so we’ll do most of the explaining under the context of this model. We start with the assumption that our underyling, latent titer value (the “true titer”) is some real number which follows a log-normal distribution. We assume that the mean of this distribution is a linear function of the antigenic distance, as we mentioned, and we assume constant variance. In math terms, we assume that \\[\ny_i^* \\sim \\text{Log-normal}\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e}\\right).\n\\tag{8.1}\\] Note that we scale the coefficients by a factor of \\(\\frac{1}{\\log_2{e}}\\) because \\[\\log{y_i^*} \\sim \\text{Normal}\\left( \\frac{\\beta_0 + \\beta_1 x_i}{\\log_2 e}, \\frac{\\sigma}{\\log_2 e} \\right),\\] which combined with the identity that \\[\\log{y_i^*} = \\frac{\\log_2 y_i^*}{\\log_2{e}},\\] implies \\[ \\log_2 y_i^* \\sim \\text{Normal}\\left( \\beta_0 + \\beta_1 x_i, \\sigma \\right) .\\] This gives us a connection to the natural units of measurement, and makes it easier to understand the models and set priors.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html#gamma-model",
    "href": "examples/HAI-data-example/index.html#gamma-model",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "8.2 Gamma model",
    "text": "8.2 Gamma model\nThe gamma model and lognormal model are both right-skewed families which can also approximate normal distributions with the correct set of parameter values. However, the gamma family is somewhat more flexible in the ranges of shapes it can take on, and includes exponential and chi-squared distributions (as examples of shapes which are difficult to match with the log-normal distribution). Additionally, the variance of the log-normal family is independent of the mean, while the variance of the gamma family is proportional to the mean. (Another way to say that is that the coefficient of variation of the log-normal model is always constant, while the coefficient of variation of the gamma model is constant for a given value of the mean, which increases as the mean increases.)\nWhile the log of a gamma random variable does not have a simple interpretation like the log-normal family does, there are known formulas for the moments of this distribution. However, they require complex calculations using derivatives of the gamma function. Notably, there is also a generalized gamma distribution which has been proposed (CITE THIS) as a general model for fitting antibody assay data, but this distribution is not common in practice.\n\n\n\n\n\n\nGamma distribution parametrization\n\n\n\n\n\nThere are at least three common reparametrizations of the Gamma parametrization. Stan implements the standard “shape-scale” parametrization, which is most commonly used by statisticians. In the rethinking package, Richard McElreath implements a “mean-scale” parametrization which uses the substitution \\(\\alpha = \\mu / \\beta\\). However, in a GLM context, the most frequently used parametrization is the “shape-mean” parametrization (CITE AGRESTI AND MCCULOUGH NELDER).\nBeginning with the parametrization made in the Stan guide, we make the substitutions \\(\\alpha = k\\) and \\(\\beta = \\frac{k}{\\mu}\\). By making those substitutions in the density, we can show that \\[E(y) = \\mu \\quad \\text{and} \\quad \\mathrm{Var}(y) = \\frac{\\mu^2}{k}.\\]\nWhen we specify \\(\\mu = f(X)\\) in the context of a linear model, we must then invert the transformation, and when we invoke the gamma distribution in Stan, we would write \\[y \\sim \\text{Gamma}\\left(k, \\frac{k}{\\mu} \\right).\\]\n\n\n\nTo specify a gamma likelihood for our titer outcome, we would write in math terms that \\[\n\\begin{align*}\ny_i^* &\\sim \\text{Gamma}\\left(k, \\frac{k}{\\mu_i}\\right) \\\\\ng(\\mu_i) &= \\beta_0 + \\beta_1 x_i\n\\end{align*}\n\\tag{8.2}\\] where \\(g(\\cdot)\\) is a suitably-chosen link function. Note that the parameters of the gamma distribution must be greater than zero, necessitating the use of a non-identity link function, in contrast to the log-normal distribution where no link function is necessary. The canonical link function here is \\(g(x) = -1/x\\), which is really a pretty useless link function in a real life context because it requires us to place constraints on the \\(\\beta_i\\) values. So, use of the link function \\(g(x) = \\exp(x)\\) is more common, and is what we would use here.\nBecause the gamma distribution does not have an easily-interpretable formula on the log-scale, we fit the distribution on the natural scale of the parameter (although I guess we could also fit a gamma model to the log-scale outcome, but this is unusual) and thus we do not need to worry about the scaling of the parameters. Once again: there is no easy interpretation of the gamma regression parameters on the log-scale.\nNEED TO CITE https://www.jstor.org/stable/2685723?searchText=&searchUri=&ab_segments=&searchKey=&refreqid=fastly-default%3A7ee1f3ec4603b86f40438da20e839c33 and Moulton/Halsey",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html#the-observation-model",
    "href": "examples/HAI-data-example/index.html#the-observation-model",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "8.3 The observation model",
    "text": "8.3 The observation model\nOK, so that would be great if that was all we had to do. But unfortunately, as I mentioned, HAI titers are interval censored and have a lower limit of detection, which we need to include in our model. We do this following the traditional method of updating the likelihood directly. Because of the way that the censoring mechanism occurs in practice, we need to define our censoring mechanism on the log scale. So, define \\[z_i^* = f(y_i^*) = \\log_2 \\left(\\frac{y_i^*}{5}\\right). \\tag{8.3}\\] Notably, this implies that \\[y_i^* = f^{-1}(z_i^*) = 5\\cdot 2^{z_i^*}.\\] The assay is performed by two-fold dilutions, so we actually observe the log-scale value during the measurement process. The limit of detection for the assay is conventionally 1:10, though it could be as low as 1:4 (CITE THAT THING AMANDA SENT ME). However, due to the reagents necessary to run the assay there is a physical limit of detection, and 1:10 has been adopted as a widely used standard in influenza research. For convenience, we will only consider the reciprocal titers, so a titer of 1:10 would be reported as 10. Note that \\[\nf(10) = \\log_2 2 = 1,\n\\] so 1 is the LoD on the log scale, any any values strictly less than one are below the LoD. Following standard conventions, we write these values as 5 on the natural scale, or 0 on the log scale.\nNote also that the measurement process reflects a discretization of an underyling continuous value. The HAI titer measurement is defined as the lowest possible dilution of serum which inhibits agglutination. This value could be, for example, a dilution of 12.5443 (on the natural scale), but we would observe this as a titer of 10 in our assay. Equivalently, the true latent log-scale titer might be 1.327, which we would observe as 1. See Figure Figure 8.1 for an example showing the raw lab assay.\n\n\n\n\n\n\n\n\nFigure 8.1: Example HAI titer data that we would observe while doing the assay in the lab. These assays are run on 96-well plates, where each row is a separate assay and each column is a serial two-fold dilution, beginning at a dilution of 1:10 (of the serum) and progressing rightwards.\n\n\n\n\n\nA convenient way to write this is by noticing that we take the floor of all values on the log scale. So, we can write our data observation model as \\[\ny_i = \\begin{cases}\n0, & y_i^* &lt; 1 \\\\\n\\left\\lfloor y_i^* \\right\\rfloor, & y_i^* \\geq 1\n\\end{cases}\n\\tag{8.4}\\]\nwhere \\(y_i^*\\) is the latent true titer and \\(y_i\\) is the observed titer.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html#data-simulations",
    "href": "examples/HAI-data-example/index.html#data-simulations",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "8.4 Data simulations",
    "text": "8.4 Data simulations\nThe first thing we need to do now is simulate some titers. We probably want to simulate data from both of those two models, in order to see how the distributions differ. So, we’ll ensure that the linear regression coefficients are the same, and we can see what happens in both models.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html#midpoint-correction-instead-of-interval-censoring",
    "href": "examples/HAI-data-example/index.html#midpoint-correction-instead-of-interval-censoring",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "8.5 Midpoint correction instead of interval censoring",
    "text": "8.5 Midpoint correction instead of interval censoring",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "examples/HAI-data-example/index.html#extensions-to-consider-in-the-future",
    "href": "examples/HAI-data-example/index.html#extensions-to-consider-in-the-future",
    "title": "8  Example Model 6: the HAI manifesto",
    "section": "8.6 Extensions to consider in the future",
    "text": "8.6 Extensions to consider in the future\n\nAn individual is randomly missing strains due to laboratory errors\nRepeated measures across study years\nRepeated measures across study years where some strains are systematically missing based on the year\nModels which control for pre-vaccination titer.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example Model 6: the HAI manifesto</span>"
    ]
  },
  {
    "objectID": "implementation/maximum-likelihood/index.html",
    "href": "implementation/maximum-likelihood/index.html",
    "title": "9  Maximum likelihood methods",
    "section": "",
    "text": "This is a placeholder",
    "crumbs": [
      "Implementation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Maximum likelihood methods</span>"
    ]
  },
  {
    "objectID": "implementation/brms/index.html",
    "href": "implementation/brms/index.html",
    "title": "10  Censoring with brms",
    "section": "",
    "text": "this is a placeholder",
    "crumbs": [
      "Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Censoring with `brms`</span>"
    ]
  },
  {
    "objectID": "implementation/stan/index.html",
    "href": "implementation/stan/index.html",
    "title": "11  Custom Bayesian models with Stan",
    "section": "",
    "text": "This is a placeholder",
    "crumbs": [
      "Implementation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Custom Bayesian models with Stan</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian censoring",
    "section": "",
    "text": "Preface\nThis book is where we can store our thoughts and codes on dealing with censoring data in Bayesian models. See the README to find where everything is and how to contribute.\nContributors:\n\nZane Billings (https://wzbillings.com/)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  What are censored data?",
    "section": "",
    "text": "Censoring is a selection phenomenon on data which occurs when we can only obtain precise measurements for certain measurement values. The classical example is a scale which can only measure weights up to a certain threshold, say \\(y_{\\mathrm{max}}\\). A scale that can measure any weight would produce the set of measurements \\(\\{y_1^*, y_2^*, \\ldots, y_n^*\\}\\) where \\(n\\) is the sample size. We call these the latent values or true values. Our imperfect scale would then produce the observed values, \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\leq y_{\\mathrm{max}} \\\\\ny_{\\mathrm{max}}, & y_i^* &gt; y_{\\mathrm{max}}\n\\end{cases}; \\quad i = 1, \\ldots, n.\n\\] Specifically, this is an example of right censoring, where there is an upper limit of detection, or maximum value that we can observe with precision.\nWe can also have the opposite case, where we can detect any theoretical measurement above a certain value. In this case, the data are said to have a lower limit of detection and this phenomenon is called left censoring. For example, imagine we are testing the concentration of lead in the tap water of several buildings. Our test cannot detect lead levels below 5 parts per billion (ppb), but can detect any larger amount of lead. In that case, our observed values would instead look like this: \\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\] where \\(y_{\\mathrm{min}}\\) is the lower limit of detection.\n\\[\ny_i = \\begin{cases}\ny_i^*, & y_i^* \\geq y_{\\mathrm{min}} \\\\\ny_{\\mathrm{min}}, & y_i^* &lt; y_{\\mathrm{min}}\n\\end{cases}; \\quad i = 1, \\ldots, n,\n\\]\nFinally, we can have interval censoring, where we know a data value is within some interval, but we do not know precisely where the value lies within that interval. An example of this is antibody titer dilutions: for flu HAI titer, the values are typically reported as 10, 20, 40, etc., but a value of 10 does not mean the precise value of the measurement should be 10, it means the true value is between 10 and 20. If we assume our titer is measured on the log scale and has no limits of detection, we could write \\[\ny_i = \\lfloor y_i^* \\rfloor; \\quad i = 1, \\ldots, n,\n\\] because we only perform a discrete number of dilutions. This gives us the interval value for \\(y_i\\) as \\[y_i \\in \\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right).\\]\nA given variable can be subject to all of these types of censoring simultaneously: for example, HAI titers are interval censored in this way, but they also have lower limits of detection and upper limits of detection as well (though the upper limits are rarely important in practice because they can be arbitrarily increased during the assay). However, a particular observation of this variable can only be subject to one type of censoring at a time: e.g., if an observation is below the detection limit, that value is left censored, it cannot simultaneously be right censored or interval censored.\nNotably, the distinction between “types” of censoring in this way is useful for several analytic methods, but is not strictly necessary. All censored values can be implicitly treated as interval censored data, where the lower endpoint for a left censored value is negative infinity, and the upper endpoint for a right censored value is positive infinity. Thus, we could write the data generating process for HAI titers with LOD as \\[\ny_i = \\begin{cases}\ny_{\\text{min}}, \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\lfloor y_i^* \\rfloor, \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\ny_{\\text{max}}, y \\leq y_{\\text{max}}\n\\end{cases},\n\\] where \\(y_{\\text{min}}\\) is the lower limit of detection and \\(y_{\\text{max}}\\) is the upper limit of detection. To express the DGP in interval notation, we would write \\[\ny_i \\in \\begin{cases}\n\\left(-\\infty, y_{\\text{min}}\\right), \\ y_i^* &lt; y_{\\text{min}}\\\\\n\\left[\\lfloor y_i^* \\rfloor, \\lfloor y_i^* + 1 \\rfloor\\right), \\ y_{\\text{min}} \\leq y_i^* &lt; y_{\\text{max}} \\\\\n\\left[y_{\\text{max}}, \\infty\\right), \\ y \\leq y_{\\text{max}}\n\\end{cases}.\n\\] Note also that assuming \\(y^*_i\\) is drawn from an absolutely continuous distribution (e.g. normal or lognormal, etc.), the final likelihood model will be equivalent regardless of which intervals are open or closed. This model would allow us to put all of the censored observations into the likelihood function in the same framework without having to worry about sorting the observations into buckets w.r.t. the type of censoring.\nThe probability of each observation \\(y_i\\) can then be expressed as the probability that the random variable \\(Y\\) takes on a realization inside the given interval. If \\(F\\) is the CDF for some parametric distribution which we assume the latent variable \\(y_i^*\\) is drawn from, with parameter \\(\\theta\\), the contribution of \\(y_i\\) to the likelihood is then \\[\\mathcal{L}(\\theta \\mid Y_i) = F(\\text{upper limit of interval}) - F(\\text{lower limit of interval}).\\] If we call the lower limit of the interval for \\(y_i\\) \\(L_i\\) and the corresponding upper limit \\(U_i\\), we can write the likelihood of the sample as \\[\n\\mathcal{L}(\\theta \\mid \\mathbf{Y}) = \\prod_{i=1}^n \\left(\nF(y_i \\mid \\theta)\\bigg\\rvert_{y_i = L_i}^{U_i}\n\\right)^{C_i}\\bigg(f(y_i\\mid \\theta) \\bigg)^{C_i},\n\\] where \\(C_i\\) is the indicator variable for \\(y_i\\) being censored. Notably, for an uncensored observation the likelihood is equal to the density. However, for the typical types of HAI data that we see, all of the assay values are subject to the same censoring process, and thus we could neglect the density component.\nSo now the remaining issue is to specify \\(F\\), the CDF of the latent variables."
  },
  {
    "objectID": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "href": "Ex1-Simple-Censored-Predictor.html#the-other-method-bjorn-method",
    "title": "2  Example Model 1: One censored predictor",
    "section": "2.1 The other method (Bjorn method)",
    "text": "2.1 The other method (Bjorn method)\n\nmod2 &lt;- cmdstanr::cmdstan_model(stan_file = here::here('Ex1b.stan'))\n\n\ndat2 &lt;- list()\ndat2$y &lt;- df_stan$y\ndat2$x &lt;- df_stan$x\ndat2$x_cens &lt;- df_stan$cens\ndat2$N &lt;- length(dat2$y)\ndat2$DL &lt;- lod\n\n\nfit2 &lt;- mod2$sample(dat2, seed = 100, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lcdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/Rtmpq4W877/model-44107d12126b.stan', line 64, column 3 to column 55)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 9.3 seconds.\nChain 4 finished in 9.3 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 9.4 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 9.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 9.4 seconds.\nTotal execution time: 9.6 seconds.\n\n\n\nfit2$summary() |&gt;\n    dplyr::filter(!startsWith(variable, \"x\")) |&gt;\n    print(n = Inf)\n\n# A tibble: 6 × 10\n  variable      mean     median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;num&gt;      &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -3099.    -3098.     23.6    23.3    -3.14e+3 -3.06e+3  1.00    1381.\n2 a            0.158     0.148   0.329   0.327  -3.79e-1  6.89e-1  1.00    2660.\n3 b            2.17      2.17    0.0564  0.0566  2.07e+0  2.26e+0  1.00    2732.\n4 s            4.90      4.90    0.109   0.110   4.73e+0  5.09e+0  1.00    6319.\n5 mu_x        -0.174    -0.0675  9.86    9.45   -1.67e+1  1.62e+1  1.00    6405.\n6 sigma_x      0.510     0.342   0.521   0.363   2.73e-2  1.57e+0  1.00    5287.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\n\npost &lt;- posterior::as_draws_array(fit2)"
  },
  {
    "objectID": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "href": "Ex2-Simple-Censored-Outcome.html#lower-limit-of-detection",
    "title": "3  Example Model 2: One censored outcome",
    "section": "3.1 Lower limit of detection",
    "text": "3.1 Lower limit of detection\nFor the first example, we’ll work with an outcome that has a lower limit of detection. First we need to simulate the data, which means we need to write out a generative model for the data. We’ll randomly sample x for the purposes of generating data, but for the purposes of our model we’ll assume x_i is a completely observed covariate and thus is known and does not need a random component in the model.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\n\\mathrm{DL}, & y^*_i \\leq \\mathrm{DL} \\\\\ny^*_i, & y^*_i &gt; \\mathrm{DL}\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\ni &= 1, 2, \\ldots, n\n\\end{align*}\n\\] Here, DL is the Detection Limit, aka the lower limit of detection for the variable. Of course in our generative model, we have set \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\) to be fixed population parameters, but for Bayesian inference we would need to assign suitable priors. Let’s set the values and simulate our data. The parameters I set for this example are as follows.\n\n\n\nParameter\nValue\nMeaning\n\n\n\n\n\\(n\\)\n271\nSample size\n\n\n\\(\\alpha\\)\n72\nRegression intercept\n\n\n\\(\\beta\\)\n3\nRegression slope\n\n\n\\(\\sigma\\)\n5\nStandard deviation of outcome\n\n\n\\(\\mathrm{DL}\\)\n80\nLower limit of detection\n\n\n\nThe \\(x\\)-values were drawn from a uniform distribution on \\((0, 10)\\). Since we know the true population parameters for our simulation, we can plot the data to see the effect of the censoring process on our observed \\(y\\) values.\n\n\n\n\n\nIn this plot, the black data points show our observed data. For those observations where the \\(y\\) value was below the limit of detection and thus censored, the gray points show the true latent values, which we could have observed with a perfect measurement process. The gray line segments connect each latent measurement to its corresponding observed measurement.\nApproximatly \\(22.88\\%\\) of data points were below the limit of detection and were therefore censored. Of course in real life, we would only observe the black points (observed values), and the gray points would be unobservable to us. But for the purposes of understanding how to analyze censored data, visualizing how different the observed and latent datasets are is quite valuable and informative. Since the datasets look so different, we should not be surprised that our regression estimates would be incorrect if we treated all of the censored values as the same constant value, or ignored them entirely!\nSo, if our standard linear regression model that we know and love (even the Bayesian version) would give us incorrect estimates using any of these naive methods, how then are we to proceed? According to the Stan manual (Stan Development Team 2023, chap. 4), there are two main ways of handling the censoring in the outcome in our model. The first of these methods relies on imputation and the second on integration of the likelihood function and manual updating of the target likelihood in Stan. The imputation method is conceptually easier and less mathematically daunting, so we begin our treatment there.\n\n3.1.1 Imputation-type method\nThe first method for dealing with censored data treats the censored values as missing values where the latent value is constrained to fall within a specific range. For a normally distributed outcome, all values below the lower limit of detection are constrained to fall within \\((-\\infty, \\mathrm{DL})\\).\nREAD THAT PART OF RETHINKING AND EXPLAIN HOW MISSING DATA WORKS HERE!!!\nTo implement such a model in Stan, we need to pass in the number of observed and the number of censored values and the observed y-values in Stan. We then declare the censored \\(y\\)-values as a parameter in the Stan code, meaning they will be sampled from their constrained distribution during the fitting process, whereas the observed \\(y\\) values will be used to update the parameter estimates.\nFirst, let’s look at the Stan code for this model.\nSHOW THE STAN CODE HERE.\nSince the data need to be in kind of a clunky format to use this method, we first need to do some wrangle and get the data in the correct format for Stan.\n\ndat_2a &lt;- list()\nwith(\n    sim_data, {\n        dat_cens &lt;- subset(sim_data, cens)\n        dat_obs &lt;- subset(sim_data, !cens)\n        dat_2a$N_cens &lt;&lt;- nrow(dat_cens)\n        dat_2a$N_obs &lt;&lt;- nrow(dat_obs)\n        dat_2a$y_obs &lt;&lt;- dat_obs$y\n        dat_2a$x_obs &lt;&lt;- dat_obs$x\n        dat_2a$y_cens &lt;&lt;- dat_cens$y\n        dat_2a$x_cens &lt;&lt;- dat_cens$x\n        dat_2a$DL &lt;&lt;- as.integer(sim_parms$DL)\n    }\n)\n\nstr(dat_2a)\n\nList of 7\n $ N_cens: int 62\n $ N_obs : int 209\n $ y_obs : num [1:209] 92.6 104.2 86.2 88.7 94.9 ...\n $ x_obs : num [1:209] 8.98 9.31 3.42 8.5 8.14 ...\n $ y_cens: num [1:62] 80 80 80 80 80 80 80 80 80 80 ...\n $ x_cens: num [1:62] 0.227 0.555 1.168 2.853 2.434 ...\n $ DL    : int 80\n\n\nNow we can compile the Stan program (via cmdstanr as usual).\n\nmod_2a &lt;- cmdstanr::cmdstan_model(here::here(\"Ex2a.stan\"), compile = FALSE)\nmod_2a$compile(pedantic = TRUE, force_recompile = TRUE)\n\nWarning in 'C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/model-f3818262ea.stan', line 56, column 21: Argument\n    0.01 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/model-f3818262ea.stan', line 55, column 18: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/model-f3818262ea.stan', line 54, column 19: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning: The parameter y_cens has no priors. This means either no prior is\n    provided, or the prior(s) depend on data variables. In the later case,\n    this may be a false positive.\n\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/model-f3818262ea.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nAnd since the program compiles correctly, we can use Stan’s sampling algorithm to generate samples from the posterior distribution. We’ll run 4 chains in parallel with 500 warmup iterations and 5000 sampling iterations per chains, with all of the other control parameters (e.g. maximum treedepth and adaptive delta) left at the cmdstan defaults. This many samples is overkill for this problem, but it is also quite fast and thus we can do many samples just to be safe.\n\nfit_2a &lt;- mod_2a$sample(\n    dat_2a, seed = 100, parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 5000\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 1 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 1 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 1 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 1 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 2 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 2 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 2 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 3 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 3 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 4 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 4 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 1 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 2 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 2 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 2 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 3 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 3 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 4 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 4 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 1 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 1 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 2 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 3 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 3 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 4 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 4 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 1 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 1 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 2 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 2 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 4 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 1 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 1 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 2 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 2 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 4 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 1 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 1 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 2 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 4 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 1 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 2 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 3 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 3 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 3 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 4 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 4 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 4 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 1 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 1 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 2 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 3 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 3 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 4 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 4 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 1 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 2 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 2 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 3 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 3 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 4 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 4 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 1 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 1 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 2 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 3 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 4 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 4 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 1 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 2 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 2 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 3 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 3 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 4 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 1 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 1 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 2 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 3 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 3 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 4 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 4 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 1 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 1 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 2 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 2 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 3 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 4 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 1 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 1 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 2 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 3 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 3 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 4 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 4 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 1 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 1 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 2 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 4 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 4 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 1 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 1 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 2 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 2 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 3 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 3 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 4 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 4 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 4 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 1 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 1 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 2 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 2 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 3 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 4 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 4 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 4 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 1 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 1 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 2 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 3 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 3 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 4 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 4 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 4 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 1 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 2 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 2 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 3 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 3 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 4 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 4 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 1 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 2 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 2 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 3 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 3 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 4 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 4 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 4 finished in 2.4 seconds.\nChain 1 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 1 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 2 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 2 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 3 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 2 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 3 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 3 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 1 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 2 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 3 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 3 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 1 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 3 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 1 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 2 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 3 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 1 finished in 3.1 seconds.\nChain 3 finished in 3.0 seconds.\nChain 2 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 2 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 2 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 2 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 finished in 3.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.0 seconds.\nTotal execution time: 3.8 seconds.\n\n# Extract the posterior samples in a nicer format for later\npost_2a &lt;- posterior::as_draws_df(fit_2a)\n\nThe first thing we should do after sampling is check for any diagnostic warnings. We have access to all of the individual diagnostics, but fortunately cmdstan has a built-in diagnostic checker to flag any potential problems.\n\nfit_2a$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2a-202310182050-1-5f1ade.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2a-202310182050-2-5f1ade.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2a-202310182050-3-5f1ade.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2a-202310182050-4-5f1ade.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nGreat, no issues with the sampling procedure, that is what we like to see. Let’s manually check the trace plots for our main three parameters of interest. (We could also check the plots for all of the imputed y-values, but these are unlikely to be interesting or useful, any problems should hopefully propagate through to the interesting parameters.)\n\nbayesplot::mcmc_combo(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nThose look like nice healthy trace plots, so with that combined with our diagnostic check, it seems that the chains mixed well and explored the posterior distribution. We can also check if those parameters were correlated.\n\nbayesplot::mcmc_pairs(post_2a, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nWe see that the slope and intercept estimates were strongly correlated, which makes sense, and the sigma parameter was slightly correlated with both of those but not strongly with either. We can notice here that the histograms for \\(\\beta\\) and \\(\\sigma\\) are not quite centered at the true values, but they do have some probability mass at those true values. Let’s look at the median estimates and CIs from our samples.\n\npar_sum &lt;-\n    fit_2a$summary(variables = c(\"alpha\", \"beta\", \"sigma\"))\npar_sum |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n73.175358\n73.195200\n0.7612478\n0.7649475\n71.910180\n74.395245\n1.000667\n8533.648\n10108.17\n\n\nbeta\n2.809101\n2.806420\n0.1216903\n0.1226851\n2.614248\n3.010981\n1.000255\n9050.044\n11862.90\n\n\nsigma\n4.897496\n4.891605\n0.2447610\n0.2444807\n4.515708\n5.316280\n1.000053\n16448.745\n13836.85\n\n\n\n\n\nWe can also plot those along with the true values for reference.\n\n\nShow plot code (messy)\ntruth &lt;- tibble::tibble(\n    name = c(\"alpha\", \"beta\", \"sigma\"),\n    value = c(sim_parms$alpha, sim_parms$beta, sim_parms$sigma)\n)\n\nhd &lt;- post_2a |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::select(alpha, beta, sigma) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything())\n\nggplot() +\n    aes(x = value) +\n    geom_histogram(\n        data = subset(hd, name == \"alpha\"),\n        boundary = 0,\n        binwidth = 0.25,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"beta\"),\n        boundary = 0,\n        binwidth = 0.05,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_histogram(\n        data = subset(hd, name == \"sigma\"),\n        boundary = 0,\n        binwidth = 0.1,\n        col = \"black\",\n        fill = \"gray\"\n    ) +\n    geom_vline(\n        data = truth,\n        aes(xintercept = value),\n        linetype = \"dashed\",\n        linewidth = 1,\n        color = \"red\"\n    ) +\n    facet_wrap(~name, scales = \"free\") +\n    labs(x = NULL)\n\n\n\n\n\nFrom these histograms, we can see that while there is a decent amount of samples close to the true values of alpha and beta, the posterior distributions are not centered around the true values. At the time of writing, I am not sure if that is a fixable problem or just something we have to deal with from having imperfectly observed data.\nWe can also do a check of how close the imputed \\(y\\) values were on average to the actual \\(y\\) values.\n\ny_cens_sum &lt;-\n    fit_2a$summary(variables = paste0('y_cens[', 1:dat_2a$N_cens, ']'))\n\ndat_comp &lt;-\n    sim_data |&gt;\n    subset(cens) |&gt;\n    dplyr::select(y_star) |&gt;\n    dplyr::bind_cols(y_cens_sum) |&gt;\n    dplyr::mutate(\n        col = dplyr::case_when(\n            (mean &gt;= y_star) & (q5 &lt;= y_star) ~ TRUE,\n            (mean &lt;= y_star) & (q95 &gt;= y_star) ~ TRUE,\n            TRUE ~ FALSE\n        )\n    )\n\n# ggplot(dat_comp) +\n#   aes(x = y_star, y = mean, ymin = q5, ymax = q95, color = col) +\n#       geom_abline(\n#           slope = 1, intercept = 0, linetype = 2, linewidth = 1,\n#                               alpha = 0.5\n#           ) +\n#   geom_errorbar(alpha = 0.25) +\n#   geom_point() +\n#   coord_fixed() +\n#   scale_color_manual(\n#       values = c(\"orange\", \"turquoise\"),\n#       name = \"CI crosses diagonal\"\n#   )\n\nggplot(dat_comp) +\n    aes(x = (y_star - mean)) +\n    geom_histogram(boundary = 0, binwidth = 1, color = \"black\", fill = \"gray\") +\n    scale_x_continuous(breaks = seq(-10, 10, 2), limits = c(-10, 10)) +\n    labs(\n        x = \"True value - mean estimated value\"\n    )\n\n\n\n\nTODO make this relative error instead to make it easier to understand.\n\n\n3.1.2 Integration-type method\nThe second method relies on calculating the direct contribution of the censored data measurements to the likelihood by integrating the density over the region where censored data can occur. That is, if the \\(i\\)th observation is below the detection limit, we know that the contribution of that observation to the sample likelihood is \\[\n\\mathcal{L}(\\theta \\mid y_i) = P(Y_i \\leq \\mathrm{DL}) = \\int_{-\\infty}^{\\mathrm{DL}}f(y_i \\mid \\theta) \\ dy = \\lim_{a \\to -\\infty} \\left[F(y_i \\mid \\theta)\\right]_{a}^{\\mathrm{DL}},\n\\] which is why we refer to this method as “integrating out” the censored values.\nBy adapting the Stan code from the manual (Stan Development Team 2023, cp. 4) to include \\(x\\) values in the calculation of the mean, we can implement this method for dealing with our censored \\(y\\) values. First we’ll load and compile the Stan model.\n\nmod_2b &lt;- cmdstanr::cmdstan_model(here::here(\"Ex2b.stan\"), compile = FALSE)\nmod_2b$compile(force_recompile = TRUE)\n\n//\n// Ex2a: One censored predictor using the imputation method\n// has one lower limit of detection only\n// See: https://mc-stan.org/docs/stan-users-guide/censored-data.html\n// Zane Billings\n// 2023-10-17\n//\n\n// The input data consists of:\n// - N: \n// - N_cens: an integer, the number of data points where the outcome was below\n//   the DL and thus censored.\n// - y: real array of observed outcome variable\n// - cens: int array; indicator which is 1 if the corresponding measurement of\n//   y is censored and 0 otherwise.\n// - x: real array of observed predictor variable\n// - DL: int, the detection limit of the assay (right now it has to be\n//   identical for all observations but we want to change that in future)\ndata {\n\tint&lt;lower=0&gt; N;\n\tint&lt;lower=0, upper=N&gt; N_cens;\n\tarray[N] real y;\n\tarray[N] int cens;\n\tarray[N] real x;\n\tint&lt;upper=to_int(min(y))&gt; DL;\n}\n\n// transformed data are values we can calculate directly from the inputted data.\n// - N_cens is an integer equal to N_cens + N_obs.\n// transformed data {\n// \tint&lt;lower=0 upper=N&gt; N_obs;\n// \tN_obs = N_cens - N;\n// }\n\n// The parameters accepted by the model. Our model\n// accepts the real-valued parameters alpha, beta, (the regression coefs) and\n// the positive real-valued parameter sigma (the variance of the outcome\n// distribution).\nparameters {\n\t// Regression parameters\n\treal alpha;\n\treal beta;\n\treal&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated.\nmodel {\n\t// Priors for parameters\n\talpha ~ normal(0, 100);\n\tbeta ~ normal(0, 100);\n\tsigma ~ exponential(0.01);\n\t\n\t// Loop through each observation and calculate the mean. If the current\n\t// y value is observed, treat it like normal. If it is censored, we need to\n\t// update the likelihood by integrating out the value.\n\tarray[N] real mu;\n\tfor (i in 1:N) {\n\t\tmu[i] = alpha + beta * x[i];\n\t\tif (cens[i] == 0) {\n\t\t\ty[i] ~ normal(mu[i], sigma);\n\t\t} else if (cens[i] == 1) {\n\t\t\ttarget += normal_lcdf(DL | mu[i], sigma);\n\t\t}\n\t}\n}\n\nAs you can see from the above program, the data needs to be in a different format for this method. Actually, it’s much easier to set up the data in the way this program specifies, and it’s very similar to the data frame we already have. We just need a list and a few other components.\n\ndat_2b &lt;- list()\ndat_2b$N &lt;- nrow(sim_data)\ndat_2b$N_cens &lt;- sum(sim_data$cens)\ndat_2b$y &lt;- sim_data$y\ndat_2b$cens &lt;- as.integer(sim_data$cens)\ndat_2b$x &lt;- sim_data$x\ndat_2b$DL &lt;- as.integer(sim_parms$DL)\n\nstr(dat_2b)\n\nList of 6\n $ N     : int 271\n $ N_cens: int 62\n $ y     : num [1:271] 92.6 104.2 86.2 88.7 94.9 ...\n $ cens  : int [1:271] 0 0 0 0 0 0 0 0 0 0 ...\n $ x     : num [1:271] 8.98 9.31 3.42 8.5 8.14 ...\n $ DL    : int 80\n\n\nNow that we have the program ready and the data set up, we can give the data to the program and do some MCMC sampling. We’ll use a similar setup that we did for the previous example, namely 4 parallel chains which each run 500 warmup iterations and 1000 sampling iterations (no need for overkill like we did before).\n\nfit_2b &lt;- mod_2b$sample(\n    dat_2b, seed = 100, parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 1000,\n    show_messages = FALSE\n)\n\n# Extract the posterior samples in a nicer format for later\npost_2b &lt;- posterior::as_draws_df(fit_2b)\n\nWe didn’t get any warnings or errors, which means that the model finished the sampling procedure without any major errors, and we should next check the diagnostics.\n\nfit_2b$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2b-202310182051-1-69625b.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2b-202310182051-2-69625b.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2b-202310182051-3-69625b.csv, C:/Users/Zane/AppData/Local/Temp/Rtmp02Gefc/Ex2b-202310182051-4-69625b.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nEverything looks good here, but let’s again look at the traceplots.\n\nbayesplot::mcmc_combo(post_2b, pars = c('alpha', 'beta', 'sigma'))\n\n\n\n\nWe got some nice, healthy looking fuzzy caterpillars, so now we can be confident in our summary results. So now let’s finally look at the parameter estimates.\n\nfit_2b$summary(variables = c('alpha', 'beta', 'sigma'))\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 alpha    73.1   73.1  0.767 0.803 71.8  74.3   1.00    1324.    1250.\n2 beta      2.82   2.81 0.122 0.126  2.62  3.02  1.00    1285.    1479.\n3 sigma     4.90   4.90 0.243 0.244  4.52  5.32  1.00    2114.    2269.\n\n\nThese estimates are pretty similar to the estimates from the other method, which is good in a way because it means both methods are similar. The frequentist tobit model estimate is also similar (see the appendix).\nUnfortunately, none of the three models to estimate the regression value while taking the censoring into account produce estimates that are exactly the same as the true simulation parameters. However, unlike the much worse naive model estimates, at least our uncertainty intervals correctly contain the true values this time. So we cannot fully erase the effect of the flawed observation process on our data, but we can do a lot better by taking the censored data into consideration."
  },
  {
    "objectID": "Ex2-Simple-Censored-Outcome.html#appendix-tobit-model-check",
    "href": "Ex2-Simple-Censored-Outcome.html#appendix-tobit-model-check",
    "title": "3  Example Model 2: One censored outcome",
    "section": "Appendix: tobit model check",
    "text": "Appendix: tobit model check\nSince we’re dealing with one censored outcome with a known limit of detection, there are actually some well-developed frequentist methods for this problem. Namely, we can use a tobit model, which specificies the likelihood model in the same way we did for the bayesian estimation method, and works very similarly to the second method where we integrate out the censored data points. However, instead of specifying priors to get a posterior distribution via Bayes’ theorem, we instead estimate the parameters by finding the parameters which maximize the sample likelihood.\nMany models for censored outcomes with a variety of distributions are implemented in the R core package survival, but the formula for specifying a tobit model with a gaussian outcome distribution correctly is very unintuitive. Thankfully, the package AER provides a simple tobit() wrapper which translates a more standard formula into the appropriate form for the survReg() function and fits the model. Fitting our model using AER::tobit() is simple.\n\ntobit_model &lt;- AER::tobit(\n    y ~ x,\n    data = sim_data,\n    left = sim_parms$DL,\n    right = Inf,\n    dist = \"gaussian\"\n)\n\nsummary(tobit_model)\n\n\nCall:\nAER::tobit(formula = y ~ x, left = sim_parms$DL, right = Inf, \n    dist = \"gaussian\", data = sim_data)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           271             62            209              0 \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 73.21256    0.75531   96.93   &lt;2e-16 ***\nx            2.80427    0.12071   23.23   &lt;2e-16 ***\nLog(scale)   1.57771    0.04957   31.83   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 4.844 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 6 \nLog-likelihood: -661.2 on 3 Df\nWald-statistic: 539.7 on 1 Df, p-value: &lt; 2.22e-16 \n\n\nIf we want a lot of compatibility with standard R functions however (e.g. broom::tidy() to get the confidence intervals for the parameters), we need to use survreg. Fortunately the documentation for AER::tobit() explains how the formula is transmogrified.\n\nU &lt;- sim_parms$DL\nsurvreg_model &lt;- survival::survreg(\n    survival::Surv(y, y &gt; U, type = 'left') ~ x,\n    data = sim_data,\n    dist = \"gaussian\"\n)\nsummary(survreg_model)\n\n\nCall:\nsurvival::survreg(formula = survival::Surv(y, y &gt; U, type = \"left\") ~ \n    x, data = sim_data, dist = \"gaussian\")\n              Value Std. Error    z      p\n(Intercept) 73.2126     0.7553 96.9 &lt;2e-16\nx            2.8043     0.1207 23.2 &lt;2e-16\nLog(scale)   1.5777     0.0496 31.8 &lt;2e-16\n\nScale= 4.84 \n\nGaussian distribution\nLoglik(model)= -661.2   Loglik(intercept only)= -831\n    Chisq= 339.55 on 1 degrees of freedom, p= 8e-76 \nNumber of Newton-Raphson Iterations: 6 \nn= 271 \n\n\nWe can see that the two models are exactly the same. But since we’ve used a model from survival, we get the benefit of widespread compatibility with other R-ecosystem functionality. For example, we can easily get confidence intervals for all three estimated parameters with broom.\n\nbroom::tidy(survreg_model, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    73.2     0.755       96.9 0            71.7      74.7 \n2 x               2.80    0.121       23.2 2.15e-119     2.57      3.04\n3 Log(scale)      1.58    0.0496      31.8 2.77e-222    NA        NA   \n\n\nOr at least I thought we could. Apparently there is not a built-in method to give the CI for the scale parameter, and we have to do it ourselves.\n\n\nCode for 95% CI for scale\npaste0(\n    \"Scale estimate: \",\n    round(exp(1.58), 2),\n    \", 95% CI: \",\n    round(exp(1.58 - 1.96 * 0.0496), 2),\n    \" - \",\n    round(exp(1.58 + 1.96 * 0.0496), 2),\n    \".\"\n)\n\n\n[1] \"Scale estimate: 4.85, 95% CI: 4.41 - 5.35.\"\n\n\nAnyways, we can compare these to the Bayesian estimates above and see that they are quite similar.\n\n\n\n\n\nStan Development Team. 2023. Stan Modeling Language Users’ Guide and Reference Manual. 2.33 ed."
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#data-simulation",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#data-simulation",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.1 Data simulation",
    "text": "4.1 Data simulation\nAs usual, we’ll begin our data simulation by writing out the true data generating process (likelihood model) that we’ll use to generate the data. This model is a bit complicated–of course we’ll have the same regression part of the model as we’ve had before, that relates the latent \\(y^*\\) values to the latent \\(x^*\\) values. But then the observation model will include a censoring scheme for the observation of both \\(x\\) and \\(y\\).\nImportantly, in this model we also need to specify a distributional assumption for \\(X\\), otherwise we can’t estimate what the uncensored \\(X\\) values should look like. So for the sake of simplicity, we’ll assume a Gaussian distribution for the \\(x\\)-values as well, although this is definitely something we need to think more about in the future. Furthermore, let’s assume \\(x\\) has a standard normal distribution, since we can standardize \\(x\\) before modeling.\n\\[\n\\begin{align*}\ny_i &= \\begin{cases}\ny_\\min, & y_i^* \\leq y_\\min \\\\\ny_i^* & y_\\min &lt; y_i^* \\leq y_\\max \\\\\ny_\\max &  y_\\max &lt; y_i^*\n\\end{cases} \\\\\nx_i &= \\begin{cases}\nx_\\min, & x_i^* \\leq x_\\min \\\\\nx_i^* & x_\\min &lt; x_i^* \\leq x_\\max \\\\\nx_\\max &  x_\\max &lt; x_i^*\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}(0, 1)\n\\end{align*}\n\\]\nAgain, we can choose whatever parameters we want for the simulation. I played around with the simulation until I got a plot I thought looked about right. Those simulation parameters are printed below.\n\n\nList of 8\n $ n    : num 400\n $ alpha: num 1\n $ beta : num 4\n $ sigma: num 5\n $ y_min: num -9\n $ y_max: num 12\n $ x_min: num -1\n $ x_max: num 2\n\n\nSo with those parameters, we can then simulate some data according to this generative model.\n\n\n# A tibble: 400 × 5\n    x_star      mu  y_star       x       y\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1.80     8.19   7.08    1.80    7.08  \n 2  1.16     5.64   6.71    1.16    6.71  \n 3  0.155    1.62   6.05    0.155   6.05  \n 4  0.0988   1.40   1.24    0.0988  1.24  \n 5 -3.16   -11.6   -7.35   -1      -7.35  \n 6 -0.682   -1.73  -1.46   -0.682  -1.46  \n 7  1.56     7.25   3.40    1.56    3.40  \n 8 -0.195    0.219 -0.216  -0.195  -0.216 \n 9  0.628    3.51   7.10    0.628   7.10  \n10  0.821    4.28   0.0773  0.821   0.0773\n# ℹ 390 more rows\n\n\nSince we’ve simulated the data, we know the latent values and the observed values, so we can plot our simulated data in order to get a better understanding of how much the censoring process will affect our estimates.\n\n\nPlotting code\nsim_data |&gt;\n    ggplot() +\n    geom_hline(\n        yintercept = c(sim_parms$y_min, sim_parms$y_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_vline(\n        xintercept = c(sim_parms$x_min, sim_parms$x_max),\n        alpha = 0.5,\n        linewidth = 1,\n        linetype = \"dashed\",\n        color = \"darkgray\"\n    ) +\n    geom_segment(\n        data = subset(sim_data, (x != x_star) | (y != y_star)),\n        aes(x = x_star, xend = x, y = y_star, yend = y),\n        color = \"gray\",\n        alpha = 0.25,\n        lwd = 1\n    ) +\n    geom_point(aes(x = x_star, y = y_star), color = \"gray\") +\n    geom_point(aes(x = x, y = y)) +\n    coord_cartesian(\n        xlim = c(-3, 3),\n        ylim = c(-22, 22)\n    ) +\n    labs(\n        x = \"Independent variable\",\n        y = \"Dependent variable\"\n    )\n\n\n\n\n\nWe can see that a substantial amount of the data points are censored. In total, \\(16.5\\%\\) of records were censored in \\(x\\) only, \\(13.25\\%\\) of records were censored in \\(y\\) only, and \\(5\\%\\) of records were censored in both \\(x\\) and \\(y\\). Thus, \\(24.75\\%\\) of records were censored in some way.\nI also deliberately set the upper and lower limits for both \\(x\\) and \\(y\\) to be asymmetrical so we can more clearly see how our censoring process can strongly bias the estimates: we have more records censored at lower values than higher values, which gives us a shifted window where we observe data.\nSo now that we have the data simulated, we want to try to recover the original parameters with a Bayesian model."
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#stan-data-setup",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#stan-data-setup",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.2 Stan data setup",
    "text": "4.2 Stan data setup\nI also want to write the Stan code to accept data in a specific format that we want to test. The data should be formatted like the table below.\n\n\n\nX\nX_L\nX_U\nY\nY_L\nY_U\n\n\n\n\n\\(x_1\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_1\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(x_2\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_2\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_n\\)\n\\(x_\\min\\)\n\\(x_\\max\\)\n\\(y_n\\)\n\\(y_\\min\\)\n\\(y_\\max\\)\n\n\n\nHere, \\(x_\\min\\) is the lower limit of detection for \\(x\\) and \\(x_\\max\\) is the upper limit of detection for \\(X\\) (and similar for \\(Y\\)). Eventually, if this is the data format we decide to permanently adopt going forward, we will want to write a suite of helper functions to conveniently get the data in this form. But for now I will do it manually. Fortunately it is quite easy. And if the censoring limits changed for any observations, it would have been easier to store the data in this format in the first place.\n\nstan_data &lt;-\n    sim_data |&gt;\n    dplyr::select(x, y) |&gt;\n    dplyr::mutate(\n        x_l = sim_parms$x_min,\n        x_u = sim_parms$x_max,\n        .after = x\n    ) |&gt;\n    dplyr::mutate(\n        y_l = sim_parms$y_min,\n        y_u = sim_parms$y_max,\n        .after = y\n    )\n\nstan_data |&gt; print(n = 5)\n\n# A tibble: 400 × 6\n        x   x_l   x_u     y   y_l   y_u\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.80      -1     2  7.08    -9    12\n2  1.16      -1     2  6.71    -9    12\n3  0.155     -1     2  6.05    -9    12\n4  0.0988    -1     2  1.24    -9    12\n5 -1         -1     2 -7.35    -9    12\n# ℹ 395 more rows\n\n\nNow we just need to convert the data frame to a list format and add a variable for the number of records.\n\nstan_list &lt;- as.list(stan_data)\nstan_list$N &lt;- nrow(stan_data)\nstr(stan_list)\n\nList of 7\n $ x  : num [1:400] 1.7973 1.1599 0.1547 0.0988 -1 ...\n $ x_l: num [1:400] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ x_u: num [1:400] 2 2 2 2 2 2 2 2 2 2 ...\n $ y  : num [1:400] 7.08 6.71 6.05 1.24 -7.35 ...\n $ y_l: num [1:400] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 ...\n $ y_u: num [1:400] 12 12 12 12 12 12 12 12 12 12 ...\n $ N  : int 400"
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#stan-code",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#stan-code",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.3 Stan code",
    "text": "4.3 Stan code\nOf course as usual we need to compile the Stan code. The code is also included here for reference.\n\npth &lt;- here::here(\"Ex3.stan\")\nmod &lt;- cmdstanr::cmdstan_model(pth, compile = FALSE)\nmod$compile(force_recompile = TRUE)\n\n\n\n\n\n\n\nModel code\n\n\n\n\n\n//\n\t// Example 3 Stan Code: censored outcome and censored predictor\n\t// where both the outcome and predictor have lower and upper limits of\n\t// detection (that are known and constant).\n\t// Zane 2023-10-18\n//\n\n// Input data for the model\ndata {\n\tint&lt;lower=0&gt; N;\n\tarray[N] real y;\n\tarray[N] real y_l;\n\tarray[N] real y_u;\n\tarray[N] real x;\n\tarray[N] real x_l;\n\tarray[N] real x_u;\n}\n\n// The parameters accepted by the model.\nparameters {\n\treal alpha;\n\treal beta;\n\treal&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated.\nmodel {\n\t// Define mu vector\n\tvector[N] mu;\n\t\n\t// Priors go here\n\tsigma ~ exponential(1);\n\tbeta ~ normal(0, 2);\n\talpha ~ normal(0, 2);\n\t\n\tfor (i in 1:N) {\n\t\t// Cases for dealing with censored predictor x based on how it is censored\n\t\tif (x[i] &lt;= x_l[i]) {\n\t\t\t// If x is left-censored, use the CDF and add contribution to target\n\t\t\ttarget += normal_lcdf(x_l[i] | 0, 1);\n\t\t} else if (x[i] &gt; x_u[i]) {\n\t\t\t// If x is right-censored, use the complimentary CDF to add to target\n\t\t\ttarget += normal_lccdf(x_u[i] | 0, 1);\n\t\t} else {\n\t\t\t// If x is observed, update evrything like normal\n\t\t\tx[i] ~ normal(0, 1);\n\t\t}\n\t\t\n\t\t// Calculate mu now that x is dealt with\n\t\tmu[i] = alpha + beta * x[i];\n\t\t\n\t\t// Dealing with the outcome likelihood\n\t\t// if Y is below the lower bound, integrate with lcdf\n\t\tif (y[i] &lt;= y_l[i]) {\n\t\t\ttarget += normal_lcdf(y_l[i] | mu[i], sigma);\n\t\t\t// If Y is above the upper bound, integrate with lccdf\n\t\t} else if (y[i] &gt; y_u[i]) {\n\t\t\ttarget += normal_lccdf(y_u[i] | mu[i], sigma);\n\t\t\t// If Y is in the middle of the censoring bounds, update the likelihood\n\t\t\t// like normal.\n\t\t} else {\n\t\t\ty[i] ~ normal(mu[i], sigma);\n\t\t}\n\t}\n}\n\n// End of program"
  },
  {
    "objectID": "Ex3-Censored-Outcome-and-Predictor.html#model-fitting-and-performance",
    "href": "Ex3-Censored-Outcome-and-Predictor.html#model-fitting-and-performance",
    "title": "4  Example Model 3: Censored outcome and censored predictor",
    "section": "4.4 Model fitting and performance",
    "text": "4.4 Model fitting and performance\nNow that the model is successfully compiled, we need to generate MCMC samples from the posterior distribution. We’ll use 4 chains (run in parallel) with 500 warmup iterations and 2500 sampling iterations each, for a total of 10000 samples overall, which should be plenty for this problem. Otherwise, we’ll leave the control parameters at their default values.\n\nfit &lt;- mod$sample(\n    stan_list,\n    seed = 123123,\n    parallel_chains = 4,\n    iter_warmup = 500,\n    iter_sampling = 2500,\n    show_messages = FALSE\n)\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/model-1e7428801b98.stan', line 62, column 3 to column 31)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nAs usual, we want to check the diagnostics, and fortunately cmdstanr gives us an easy to use diagnostic flagger.\n\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-1-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-2-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-3-126c21.csv, C:/Users/Zane/AppData/Local/Temp/RtmpSmiyS2/Ex3-202310182103-4-126c21.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\nWe can examine the trace plots and posterior distributions of the parameters of interest to confirm that there is no funny business.\n\npost &lt;- posterior::as_draws_array(fit)\nbayesplot::mcmc_combo(post, par = c(\"alpha\", \"beta\", \"sigma\"))\n\n\n\n\nAnd so now we can finally examine the fitted values and compare them to our true simulation values.\n\nfit$summary() |&gt;\n    dplyr::filter(variable != \"lp__\") |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n0.43\n0.43\n0.26\n0.26\n0.00\n0.86\n1\n8190.67\n7220.56\n\n\nbeta\n4.14\n4.14\n0.29\n0.29\n3.66\n4.63\n1\n9034.90\n7285.48\n\n\nsigma\n5.10\n5.10\n0.19\n0.19\n4.80\n5.42\n1\n9738.63\n7383.51\n\n\n\n\n\nWe can see that our model estimated the slope and variance quite well, although it is not doing too great at figuring out the intercept. In fact, the true value of \\(\\alpha = 1\\) isn’t even in the credible interval. However, the estimates for \\(\\beta\\) and \\(\\sigma\\) are very close to the true estimates. In most applications, the intercept is not too useful and the slope is what we want an accurate estimate of anyway, so this is probably acceptable.\nTODO figure out what else needs to go in this example."
  },
  {
    "objectID": "Ex4-Infection-Outcome-with-Censored-Predictor.html",
    "href": "Ex4-Infection-Outcome-with-Censored-Predictor.html",
    "title": "5  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "Ex5-Interval-Censoring.html",
    "href": "Ex5-Interval-Censoring.html",
    "title": "6  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "Ex6-HAI-data-outcome.html",
    "href": "Ex6-HAI-data-outcome.html",
    "title": "7  Example Model 2: One censored outcome",
    "section": "",
    "text": "For the second example model, we’ll work on a case"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Stan Development Team. 2023. Stan Modeling Language\nUsers’ Guide and Reference Manual.\n2.33 ed."
  }
]
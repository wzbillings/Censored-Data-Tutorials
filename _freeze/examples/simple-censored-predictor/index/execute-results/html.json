{
  "hash": "cd7cb50012fde5cf23322a0a09db4e3b",
  "result": {
    "engine": "knitr",
    "markdown": "# One censored predictor\n\n::: {.callout-important}\nThe contents of this page might change at any time -- our lab group is still\nworking on simulations and investigating other methods for dealing with\ncensored predictors, so the content here might be buggy.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial setup steps!\n# Need to load all of cmdstanr or some things act weird\nlibrary(cmdstanr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is cmdstanr version 0.8.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan version: 2.34.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n```\n\n\n:::\n\n```{.r .cell-code}\n# ggplot2 theme setup\nlibrary(ggplot2)\nggplot2::theme_set(hgp::theme_ms())\n\n# Load utility functions\nsource(here::here(\"utils.R\"))\n\n# Set up a variable with the base path for this document\npth_base <- here::here(\"examples\", \"simple-censored-predictor\")\n\n# Set a constant value for pRNG seed\nS <- 370\n```\n:::\n\n\n\n\nNow we'll consider the case where our outcome is perfectly observed, but we have\na single predictor value with a limit of detection. Most commonly used models\nconsider predictor values to be completely observed, exact measurements without\nerror, which makes dealing with censoring of a predictor more complicated than\ndealing with censoring of an outcome. In order to address censoring in a\npredictor, we also have to assume the predictor values are, in some way,\nunknown, like we do for the outcomes. Before we discuss methods for dealing\nwith a censored predictor, let's take a look at some simulated data and the\nassociated data generating process.\n\n## Data-Generating Process\n\nThe data will discuss consist of $n$ observations of a predictor variable, $x$,\nand an outcome variable, $y$. To make this example simple, we'll assume there is\na linear relationship between $x$ and $y$. So the data generating process (DGP) \nfor the outcome, $y_i, \\ i = \\{1, 2, \\ldots, n\\}$, is\n$$\n\\begin{align*}\ny_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i^*\n\\end{align*}\n$$\nwhere $x_i^*$ represents the true value of our predictor. Importantly, for\nthe predictor, $x$, to be censored, there also has to be a data generating\nprocess for $x$. If we conduct a controlled experiment where we determine the\nvalues for $x$, it doesn't make sense for $x$ to be censored (which is why\nthis case is discussed less often than the case of a censored outcome). But if\nwe are conducting an observational study, where we expect $x$ to influence $y$,\nbut we cannot directly manipulate the value of $x$, then it makes sense for\n$x$ to potentially be censored. Like with a censored outcome, we can discuss\nthe DGP for the *latent* or true $x_i^*$ values, and the observation process\nwhich generates our imperfect observation $x_i$. Of course, the most simple\nexample would be a Normal distribution[^1]:\n$$\nx_i^* \\sim \\text{Normal}\\left(\\lambda, \\tau^2\\right).\n$$\n\n[^1]: There's actually a statistical issue with this Normal/Normal model: it's\nnot fully identifiable without extra constraints. See the [errors-in-variables\nwikipedia page](https://en.wikipedia.org/wiki/Errors-in-variables_models) for a\nbit more info. But we'll ignore that for now.\n\nThis is the part where we'll just say you can put whichever distribution here\nas well -- don't worry, we'll do another non-normal example after this. Now\nthat we have a DGP for the latent $X^*$ values, we need to specify our\nobservation model. For a simple case of censoring, this could include a lower\nlimit of detection (LoD), an upper LoD, or both. For example, with a lower LoD,\nthe observation model might look like this:\n\n$$\nx_i = \\begin{cases}\nx_{\\min}, & x_i^* \\leq x_{\\min} \\\\\nx_i^*, & x_i^* > x_{\\min}\n\\end{cases}; \\quad x_{\\min} \\in \\mathbb{R}.\n$$\n\nHere, $x_{\\min}$ is a constant value representing a lower limit of detection --\nif the latent value $x_i^*$ is less than the threshold represented by $x_{\\min}$,\nwe just observe $x_{\\min}$. If the true value is greater than this threshold,\nwe observe the true value. This is not a unique observation process -- we can,\nin principle, write down any value for the censored observations. To avoid\nconfusion, we should also define an indicator variable for whether the $i$th\nobservation is censored:\n$$\nc_i = I\\left( x_i^* \\leq x_{\\min} \\right) =\n\\begin{cases}\n1, & x_i^* \\leq x_{\\min} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n$$\nTypically, we can observe all values of $c_i$, and we assume that *these*\nare measured perfectly (although this is not strictly necessary, as we could\nincorporate measurement error and thus misclassification models into our\nobservation process, but we neglect those here for the sack of simplicity). If\nyou receive a data set you know a variable is censored, but there is no way to\ndetermine which values are censored due to improper coding and recording, there\nis not much you can do to resolve the situation. So it is typically best to\nrecord censored values using some value **which could not have been observed if\nthe observation were not censored**. Do not worry if this description is\nabstract -- next we will consider a concrete example which will hopefully help\nto make these concepts concrete.\n\nFor reference, we can write down the entire data-generating process all\ntogether. Note that from this DGP, we observe $(x_i, c_i, y_i); \\ i = 1, \\ldots, n$.\n\n$$\n\\begin{align*}\ny_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i^* \\\\\nx_i^* &\\sim \\text{Normal}\\left(\\lambda, \\tau^2\\right) \\\\\nx_i &= \\begin{cases}\nx_{\\min}, & x_i^* \\leq x_{\\min} \\\\\nx_i^*, & x_i^* > x_{\\min}\n\\end{cases}; \\\\\nc_i &= I\\left( x_i^* \\leq x_{\\min} \\right)\n\\end{align*}\n$$\nHere, $\\sigma$ and $\\tau$ are positive real numbers, and the following are\nreal-valued constants: $\\beta_0$, $\\beta_1$, $x_{\\min}$, and $\\lambda$.\n\n## Weighing cats\n\nNow that we've gone through the basics of the data-generating process, let's\nset up the DGP for an example data set. Once we've worked out the DGP, we'll\ntake a look at some simulated data from this example. Then we'll finally\ndiscuss methods for dealing with censored predictor data.\n\nSuppose we're conducting a study on cholesterol in cats, and we want to know\nwhether elevated cholesterol levels are associated with cat weight --\npresumably, heavier cats have higher overall cholesterol values. For simplicity,\nwe limit our study to adult male American shorthair cats (we can recruit other\ntypes of cats after we finish this pilot study). According to\n[this article](https://web.archive.org/web/20240305033813/https://www.thesprucepets.com/american-shorthair-cat-breed-profile-4774399),\nthe normal weight for such a cat is from 11 -- 15 pounds. So, we'll take 13 lbs.\nas the average weight of a cat, and 2 lbs. as the standard deviation.So, letting $x_i^*$ represent the true weight of the $i$th cat, we would write\n$$x_i^* \\sim \\mathrm{Normal} \\left(13, 2^2\\right).$$\n\nNow, the problem is that we do not have a very good scale -- our scale is\naccurate to the tenth of a pound, but the highest value it can measure is 14\nlbs[^Our scale could also have a lower bound, but in this case it's probably so low that we would never get any left-censored observations.]. In terms of the DGP, we would say that\n$$x_{\\max} = 14.$$\n\nUsing that information, we can then write out the observation model for the\nweight data.\n$$\nx_i = \\begin{cases}\nx_{\\max}, & x_{\\max} \\leq x_i^*  \\\\\nx_i^*, & x_i^* < x_{\\max}\n\\end{cases}.\n$$\nThis is not much more complicated than the abstract example we wrote out before\nthat only had a lower LoD, in fact it's easily taken care of in the same\nlikelihood step. Now, we'll also need to set up an indicator variable, which\nwe call $c_i$, that tells us if our weight values are censored or not.\n\n$$\nc_i = \\begin{cases}\n1, & x_i \\geq x_{\\max} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n$$\n\nNext, we need to pick a distribution for our outcome. Since many biological\nconcentration values often follow [log-normal distributions](https://en.wikipedia.org/wiki/Log-normal_distribution), let's use\nthat for our cholesterol levels. Based on\n[this article](https://web.archive.org/web/20240305035325/https://cats.com/high-cholesterol-in-cats)\n(I have no idea how accurate it is, if at all, just like the previous one),\nthe normal amount of cholesterol for a cat is 1.8 -- 3.9 mMol/liter. So, we just\nneed to choose distribution parameters. Note that for log-normal outcomes, it\nis almost always easier to use a normal distribution (Gaussian) model on the\nlog-scale than it is to actually fit a lognormal model, so we'll do that.\n\n<!--\n::: {.callout-note icon=false collapse=true appearance=\"simple\"}\n\n### Gamma distribution parametrization {.unnumbered}\nThere are at least three common reparametrizations of the Gamma parametrization.\n[Stan](https://mc-stan.org/docs/functions-reference/gamma-distribution.html)\nimplements the standard \"shape-rate\" parametrization, which is most commonly\nused by statisticians. In the `rethinking` package, Richard McElreath implements\na \"mean-scale\" parametrization which uses the substitution\n$\\alpha = \\mu / \\beta$. **However**, in a GLM context, the most frequently\nused parametrization is the \"shape-mean\" parametrization[^See, e.g. McCullagh and Nelder's *Generalized Linear Models*, 2nd edition; or Agresti's *Foundations of Generalized Linear Models*.]. \n\nBeginning with the parametrization made in the Stan guide, we make the\nsubstitutions $\\alpha = k$ and $\\beta = \\frac{k}{\\mu}$.\nBy making those substitutions in the density, we can show that\n$$E(y) = \\mu \\quad \\text{and} \\quad \\mathrm{Var}(y) = \\frac{\\mu^2}{k}.$$\n\nWhen we specify $\\mu = f(X)$ in the context of a linear model, we must then\nconvert back to Stan's parametrization, and when we invoke the gamma distribution in Stan,\nwe would write\n$$y \\sim \\text{Gamma}\\left(k, \\frac{k}{\\mu} \\right).$$\n\n:::\n-->\n\nIf we treat the range the same as we did previously, we can estimate that the\noverall average is around 2.85, with a spread of 1.05. That seems a little high,\nso let's assume they gave us a two standard deviation range, and cut it in half,\nand we'll round that down to $0.5$, which is probably good enough for government\nwork.\n(Of course if you want to simulate the data, you can make whatever arbitrary\nassumptions like this that you prefer instead.)\n\nSo, we would write the distribution for our cholesterol values, which we'll\ncall $y$, as\n\n$$\n\\log(y_i) \\sim \\text{Normal}\\left(\\mu_i, \\sigma\\right).\n$$\n\nTo relate this to the weight values, we would then write out our linear model\nfor $\\mu_i$, the mean, as\n\n$$\n\\mu_i = \\alpha + \\beta \\cdot x^*_i.\n$$\n\nChoosing values of $\\alpha$, $\\beta$, and $\\sigma$, that make sense together and\nmatch the ranges we expect is kind of hard -- for the purposes of this\nsimulation, I just messed around until I got values that I liked.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List of data generating parameters / simulation parameters\ndata_gen_parms <- list(\n\t# Number of observations to generate\n\tn = 337,\n\t# Set a static constant value for the lower limit of detection (LoD)\n\tulod = 14,\n\t# Linear model intercept\n\talpha = 0.35,\n\t# Linear model slope\n\tbeta = 0.06,\n\t# Linear model residual variance\n\tsigma = 0.15,\n\t# Mean of normally distributed x values\n\tw_mu = 13,\n\t# SD of normally distributed x values\n\tw_sd = 2,\n\t# pRNG seed\n\tseed = S\n)\n```\n:::\n\n\n\n\nSo for this example, I choose the simulation parameters\n$\\sigma = 0.15$,\n$\\alpha = 0.35$,\nand $\\beta = 0.06$.\nWe can interpret these parameters in the standard way for linear regression,\nnoting that they are in terms of the log-cholesterol. Now we need to simulate\nthe data. I randomly decided that we should have 337 cats in\nour study sample.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_cat_weight_data <- function(\n\t\tn, ulod, alpha, beta, sigma, w_mu, w_sd, seed = NULL\n) {\n\t# Set the seed, using the current unix time if one is not given\n\tif(is.null(seed)) {set.seed(Sys.time())} else {set.seed(seed)}\n\t\n\t# Generate the data\n\tl <- tibble::tibble(\n\t\tw_star = rnorm(n, w_mu, w_sd),\n\t\tw = ifelse(w_star >= ulod, ulod, w_star),\n\t\tc = ifelse(w_star >= ulod, 1, 0),\n\t\tmu = alpha + beta * w_star,\n\t\ty = exp(rnorm(n, mu, sigma))\n\t)\n\t\n\to <- dplyr::select(l, w, y, c)\n\t\n\tout <- list(\"latent\" = l, \"observed\" = o)\n\treturn(out)\n}\ndat <- do.call(generate_cat_weight_data, data_gen_parms)\ndat_latent <- dat$latent\ndat_observed <- dat$observed\n\ndplyr::glimpse(dat_latent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 337\nColumns: 5\n$ w_star <dbl> 12.44332, 11.27354, 14.33210, 12.46073, 13.34774, 13.84893, 14.…\n$ w      <dbl> 12.44332, 11.27354, 14.00000, 12.46073, 13.34774, 13.84893, 14.…\n$ c      <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ mu     <dbl> 1.0965989, 1.0264123, 1.2099258, 1.0976436, 1.1508645, 1.180935…\n$ y      <dbl> 2.387532, 3.998243, 2.382242, 2.930991, 3.419141, 3.019194, 3.4…\n```\n\n\n:::\n:::\n\n\n\n\nYou can use this function to generate similar datasets with arbitrary\nparameters if you want. Note that in this example, about\n31 percent of our predictor values are\ncensored.\n\nNow let's take a quick look at the latent data so we can visualize the effect\nof censoring the predictor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_latent |>\n\tggplot() +\n\tgeom_vline(\n\t\txintercept = data_gen_parms$ulod,\n\t\tlinetype = \"dashed\",\n\t\tcolor = \"gray\"\n\t) +\n\tgeom_segment(\n\t\taes(\n\t\t\tx = w_star, xend = w,\n\t\t\ty = y, yend = y\n\t\t),\n\t\tcolor = \"gray\"\n\t) +\n\tgeom_point(aes(x = w_star, y = y), color = \"gray\", size = 2) +\n\tgeom_point(aes(x = w, y = y), color = \"black\", size = 2) +\n\tlabs(x = \"Weight\", y = \"Cholesterol\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot latent data-1.png){width=672}\n:::\n:::\n\n\n\n\nHere the gray points show the true latent values of the censored points, and\nthe black points show what we actually observed. You can see that we\nobviously observe a much smaller range of data when censoring happens. If we\nhad the latent variables in real life, we could use a standard regression\nmodel to estimate them, like this.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlatent_glm <- glm(\n\tlog(y) ~ w_star,\n\tfamily = \"gaussian\",\n\tdata = dat_latent\n)\nbroom::tidy(latent_glm, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.324    0.0581       5.58 4.99e- 8   0.210     0.438 \n2 w_star        0.0624   0.00436     14.3  1.47e-36   0.0539    0.0710\n```\n\n\n:::\n:::\n\n\n\n\nDue to sampling error, the estimates are not perfect, but the confidence\nintervals contain the true values and the estiamtes are reasonably close.\n\n<!--\n\nNote that the dispersion estimate is an estimate of $1/k$. We can also get a\nslightly improved estimate of $k$ using a helper function from the `MASS`\npackage.[^The function performs another round of maximum likelihood estimation,\nthe actual estimate from `glm()` is the MLE of the coefficients but not of the\ndispersion.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMASS::gamma.shape(latent_gamma_glm)\n```\n:::\n\n\n\n\nThe estimates are affected by sampling error, perhaps moreso than we would\nexpect from a Gaussian regression model. But overall they're fairly close,\nand our CI for the coefficients includes the true values.\n-->\n\nSo, the natural\nnext question is what happens when we fit the model with the actual observed\ndata that's been censored? (I always call this the \"naive\" model, because we\nare naively hoping for a model that breaks our assumptions to work.)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaive_glm <- glm(\n\tlog(y) ~ w,\n\tfamily = \"gaussian\",\n\tdata = dat_observed\n)\nbroom::tidy(naive_glm, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.134    0.0871       1.54 1.24e- 1  -0.0365    0.305 \n2 w             0.0795   0.00680     11.7  1.09e-26   0.0662    0.0929\n```\n\n\n:::\n:::\n\n\n\n\nIn this case, we can see that the estimates are definitely biased. The CI for\nthe slope doesn't even include the true value. Honestly, most models we fit\nin real life are probably more incorrect than this though, so interpreting\nthis coefficient with a hefty amount of caution would probably not be\nthe end of the world. However, we can do better, and censoring can also cause\nworse problems.\n\n## More censoring!\n\nOf course, if ignoring the problem didn't make a difference, we would want to\njust ignore it, right? But let's see what happens when we increase the amount\nof data that are censored. So this time, let's say our scale has a maximum of\n12.5 lbs[^A good question to ask at this point is whether we should actually be doing this study if we can't get better measures. But some immunological or environmental studies actually have more than 50% censored or missing data, and sometimes the question is so important that we really want to get the most out of the data we have.]. So first let's rerun the simulation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_gen_parms_2 <- data_gen_parms\ndata_gen_parms_2$ulod <- 12.5\n# Resimulate the data -- note that because we used the same random seed, the\n# only that will change is the amount of censoring.\ndat2 <- do.call(generate_cat_weight_data, data_gen_parms_2)\ndat2_latent <- dat2$latent\ndat2_observed <- dat2$observed\n\ndplyr::glimpse(dat2_latent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 337\nColumns: 5\n$ w_star <dbl> 12.44332, 11.27354, 14.33210, 12.46073, 13.34774, 13.84893, 14.…\n$ w      <dbl> 12.44332, 11.27354, 12.50000, 12.46073, 12.50000, 12.50000, 12.…\n$ c      <dbl> 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, …\n$ mu     <dbl> 1.0965989, 1.0264123, 1.2099258, 1.0976436, 1.1508645, 1.180935…\n$ y      <dbl> 2.387532, 3.998243, 2.382242, 2.930991, 3.419141, 3.019194, 3.4…\n```\n\n\n:::\n:::\n\n\n\n\nIf we plot these new simulated data, we can see that many more data points are\ncensored that before, and about 60\npercent of the predictor values are censored; much more than the previous\nexample.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2_latent |>\n\tggplot() +\n\tgeom_vline(\n\t\txintercept = data_gen_parms$ulod,\n\t\tlinetype = \"dashed\",\n\t\tcolor = \"gray\"\n\t) +\n\tgeom_segment(\n\t\taes(\n\t\t\tx = w_star, xend = w,\n\t\t\ty = y, yend = y\n\t\t),\n\t\tcolor = \"gray\"\n\t) +\n\tgeom_point(aes(x = w_star, y = y), color = \"gray\", size = 2) +\n\tgeom_point(aes(x = w, y = y), color = \"black\", size = 2) +\n\tlabs(x = \"Weight\", y = \"Cholesterol\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nNow let's see what happens when we fit a naive model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaive_glm_2 <- glm(\n\tlog(y) ~ w,\n\tfamily = \"gaussian\",\n\tdata = dat2_observed\n)\nbroom::tidy(naive_glm_2, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   -0.109    0.144     -0.758 4.49e- 1  -0.392      0.173\n2 w              0.104    0.0120     8.73  1.21e-16   0.0810     0.128\n```\n\n\n:::\n:::\n\n\n\n\nNow we can see that the estimates are much worse. The intercept is actually\nnegative, with a wide confidence interval, and the slope has been severely\ninflated. So let's see what we can do about that.\n\n## Model for censored predictors\n\nLet's first write out the entire data generating process in one place.\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Normal}\\left(\\mu_i, \\sigma\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal} \\left(\\mu_x, \\sigma_x^2 \\right) \\\\\nx_i &= \\begin{cases}\nx_{\\max}, & x_{\\max} \\leq x_i^*  \\\\\nx_i^*, & x_i^* < x_{\\max}\n\\end{cases}\n\\end{aligned}\n$$\n\nHere, we assume $x_{\\max} = 12.5$ is a known constant.\nIn future tutorials, we'll\ndiscuss what to do if this isn't a known constant, but in most situations\nthat arise in a lab study, we do know the censoring limits.\n\nNow, the unfortunate thing about censored predictors like this, is that there\nare (to my knowledge) no out-of-the-box models that can adjust the likelihood.\nMost frequentist methods assume that the $x_i$ predictor variables are\n**known constants** which have no intrinsic error. Of course this is rarely\ntrue (outside of a specific type of experimental setup), and there are\n[errors-in-variables\nmodels](https://en.wikipedia.org/wiki/Errors-in-variables_models) which can help\nto address this. But I'm not aware of any errors-in-variables implementations\nthat allow for arbitrary and censored predictor distributions.\n\nBecause directly modeling the likelihood of the predictor variable is quite\ndifficult (we will discuss this in future tutorials), we'll use a constrained\nimputation approach in Stan. Sometimes called a \"full Bayesian\" approach, we\nspecify a distribution for the predictor values. Then, we can use observations\nof the predicted values to estimate the parameters, and we can impute censored\nobservations from a truncated distribution where the imputed value is\nguaranteed to be less than the limit of detection (for a lower limit).\n\n<!--\nSo, we'll use Stan to fit a model, and we'll use an approach called **joint\nmodeling**. Instead of optimizing the likelihood of $y$ conditional on $x$,\nwe'll optimize the joint likelihood of $x$ and $y$, which is fairly easy to do\nin Stan. The crucial assumption we will make is that the marginal distributions\nof $x$ and $y$ are conditionally independent. That means that we can multiply\nthe likelihood of $x$ and the likelihood of $y$ conditional on $x$ to get the\njoint likelihood of $x$ and $y$. This is not always true, and there are some\nmore advanced approaches like specifying the functional form of the joint\ndistribution for $x$ and $y$, or [using a copula\nfunction](https://onlinelibrary.wiley.com/doi/10.1002/sim.8995). But I think\nthis simplifying independance assumption is not too different from the\nassumptions we normally make when we do regression, and it's a step above\nassuming the independant variables have no error.\n\nOnce we've specified that we're optimizing the joint likelihood of $x$ and $y$,\nand we're making an independence assumption to construct that likelihood, **we\ncan adjust for censoring in the predictors using the same integration method\nwe used for the outcome.**\n-->\n\nThis type of model is implemented in the following Stan code. To fit a Stan\nmodel, we'll also need to specify priors, and I decided to use the same generic\nStudent's (Half) $t$ priors that I discussed in the previous section.\n\n::: {.callout-note icon=false collapse=true appearance=\"simple\"}\n##### Model code {.unnumbered}\n\n```{.stan include=D:/Research/Current/Censoring-Tutorials/examples/simple-censored-predictor/censored-predictor-imputation.stan}\n```\n\n:::\n\nAs with last time, first we need to set up a data list that matches what the\nStan code expects in the `data` block. Note that for the imputation method,\nthe data block only requires us to pass in the observed values of $x$. This\nleads to some annoying but necessary data cleaning, where we have to make sure\nthat all of the censored values are together in the dataset and all of the\nobserved values are together in the data set, to make sure the correct\n$(x_i, y_i)$ pairs stay together.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2_sorted <- dat2_observed[order(dat2_observed$c), ]\nstan_data <- list(\n\tN = nrow(dat2_sorted),\n\tN_obs = nrow(dat2_sorted[dat2_sorted$c == 0, ]),\n\ty = log(dat2_observed$y),\n\tx_obs = dat2_observed$w[dat2_sorted$c == 0],\n\tLoD = data_gen_parms_2$ulod\n)\nstr(stan_data, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 5\n $ N    : int 337\n $ N_obs: int 134\n $ y    : num [1:337] 0.87 1.386 0.868 1.075 1.229 ...\n $ x_obs: num [1:134] 12.4 11.3 12.5 12.5 12.5 ...\n $ LoD  : num 12.5\n```\n\n\n:::\n:::\n\n\n\n\nNext we'll load and compile the program.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_path <- here::here(pth_base, \"censored-predictor-imputation.stan\")\nstan_mod <- cmdstanr::cmdstan_model(mod_path, compile = FALSE)\nstan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n```\n:::\n\n\n\n\nNow we can do the fun part of running the sampler.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- stan_mod$sample(\n\tdata = stan_data,\n\tseed = S,\n\tparallel_chains = 4,\n\tchains = 4,\n\titer_warmup = 1000,\n\titer_sampling = 1250,\n\t# Turn off printing messages\n\trefresh = 0,\n\tshow_messages = FALSE,\n\tshow_exceptions = FALSE\n)\n```\n:::\n\n\n\n\nI silenced the printout for the purposes of this vignette, but we can quickly\nquick the diagnostics to show that, while there were a few divergences in the\nwarmup stage, there were no post-warmup divergent transitions and sampling went\nfairly well.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.6881872 0.7043031 0.7001663 0.6922051\n```\n\n\n:::\n:::\n\n\n\n\nNow we can take a look at the posterior distributions of our parameters of\ninterest. Notably, we have full posterior distributions for all of the imputed\npredictor values, but those are generally not of much interest.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$summary(variables = c('a', 'b', 'sigma', 'x_mu', 'x_sd'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 10\n  variable    mean  median      sd     mad      q5    q95  rhat ess_bulk\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <dbl>    <dbl>\n1 a         0.279   0.277  0.237   0.246   -0.112   0.673  1.00    2856.\n2 b         0.0734  0.0736 0.0200  0.0205   0.0401  0.106  1.00    2862.\n3 sigma     0.193   0.192  0.00764 0.00767  0.181   0.205  1.00    7690.\n4 x_mu     11.8    11.8    0.0522  0.0522  11.7    11.9    1.00    3100.\n5 x_sd      0.681   0.679  0.0376  0.0373   0.622   0.746  1.00    4640.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nHere, we see that some of the parameter estimates are fixed, but others don't\nactually seem to be. The model basically can't tell where the intercept is,\nand the parameters for the mean and standard deviation of $x$ are not\nquite right, especially the standard deviation. But more importantly, the\nestimate of the slope appears to be corrected -- while the point estimate is\nnot exactly correct, we are able to recover an accurate credible interval,\nand the slope is the main parameter of interest. So this imputation method\nmanages to correct our effect estimate, although the issues with identifiability\nin this model might be harder to fix.\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
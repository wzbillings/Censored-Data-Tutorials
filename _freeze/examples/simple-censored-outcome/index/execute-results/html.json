{
  "hash": "f782d1b8a371c8a59a652ef577e4c164",
  "result": {
    "engine": "knitr",
    "markdown": "# One censored outcome\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup code\nlibrary(cmdstanr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is cmdstanr version 0.8.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan version: 2.34.1\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'brms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n\n```{.r .cell-code}\n# ggplot2 theme setup\nlibrary(ggplot2)\nggplot2::theme_set(hgp::theme_ms())\npth_base <- here::here(\"examples\", \"simple-censored-outcome\")\nsource(here::here(\"utils.R\"))\n```\n:::\n\n\nNow that we've gone through a very broad overview of what censored data is,\nwe'll discuss a simple example with a censored outcome.\n\nFor this example, we'll stay on the theme of concentrations that we started\nin the last chapter. Analyzing immunological, medical, or environmental\nconcentrations is a common use case of censored data that doesn't require us\nto also discuss the intricacies of time-to-event analysis. This time,\nwe'll consider **the amount of glyphosate in household drinking water.**\n\n## Glyphosate data simulation\n\nIn this first example, since we're trying to understand these models, we'll\n**simulate our data** from a known generative model. This allows us to be\nconfident that our model for censored data is actually helping us to recover\nthe correct parameters. Once we're more comfortable with the model, we can\ntry to analyze some data with unknown generative processeses.\n\n[Glyphosate](https://en.wikipedia.org/wiki/Glyphosate) is an organophosphonate pesticide originally marketed as Roundup by Monsanto. Roundup was quickly\nadopted for industrial agriculture, especially after the introduction of\ngenetically modified Roundup-resistant crop species in the mid-90's. Due to\nwidespread agricultural use in the US, glyphosate is an increasingly common\ngroundwater contaminant, with a [Maximum Containment Level Goal of 0.7 parts\nper million in tap water set by the EPA](https://web.archive.org/web/20240509003411/https://www.epa.gov/sites/default/files/2015-06/documents/epa-200.7.pdf).\n\n1. **Source population.** Our fictional source population will be $n$ randomly\nsampled households from a fictional city, indexed from $i = 1, \\ldots, n$. We\nonly have one measurement per household. For the purposes of our study,\nwe'll assume that each of these houses has their own water supply (which is\nunrealistic but sufficient for a censored data tutorial).\n1. **Outcome variable.** For this example, our outcome (or dependent) variable\nis $y_i$, the log concentration of glyphosate in parts per million (ppm) detected\nfrom the tap in household $i$ using an analytical test.\n1. **Censoring structure.** Our investigators have decided to use [the test described in this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6163928/), which has a lower limit of detection of 2 µM. Given a molar mass of 169.07 g/Mol\nfor glyphosate, we can convert this to an LoD of 0.33 ppm, just under half of\nthe MCLG. We assume there is no upper LoD or interval censoring.\n1. **Independent variables.** For the sake of this example, we will measure a\nfew independent variables at the household level. Without doing any research\non the actual patterns of glyphosate distribution in groundwater, we'll assume\nthat glyphosate concentration is affected by:\n\t1. distance to the nearest agriculture site (km).\n\t1. whether the home uses glyphosate-based pesticides for personal gardening.\n\t1. whether the home has a water filter on the primary tap.\n\nTo get our data, we'll first randomly generate our covariates. Let's assume that\nour town of interest is a square, so we can normalize all of the distances so\nthat each side of the square has length 1. Then (again somewhat unrealistically)\nwe'll assume that home X and Y coordinates are independently drawn from a\nuniform distribution on $(0, 1)$. For convenience, we'll place the only farm\nin town at $(0.5, 0.5)$ and calculate the distances. Then since the other\ntwo independent variables are binary, we'll randomly sample those for each house,\nsay with respective probabilities $0.2$ and $0.4$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(370)\nN <- 147\ngly_preds <-\n\ttibble::tibble(\n\t\thouse_x_coord = runif(N, 0, 1),\n\t\thouse_y_coord = runif(N, 0, 1),\n\t\tdist_from_farm = sqrt((house_x_coord - 0.5)^2 + (house_y_coord - 0.5)^2),\n\t\tpersonal_gly_use = rbinom(N, 1, 0.2),\n\t\twater_filter_use = rbinom(N, 1, 0.4)\n\t)\nhead(gly_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  house_x_coord house_y_coord dist_from_farm personal_gly_use water_filter_use\n          <dbl>         <dbl>          <dbl>            <int>            <int>\n1        0.390         0.852           0.369                1                1\n2        0.0519        0.373           0.466                0                0\n3        0.194         0.472           0.307                0                1\n4        0.961         0.922           0.625                0                1\n5        0.747         0.576           0.259                0                0\n6        0.0105        0.0321          0.677                0                0\n```\n\n\n:::\n:::\n\n\nNow we need to describe our data generating model for the outcome. Of course,\nI am just going to randomly pick some parameters and mess around until they\nlook about right -- we know that our glyphosate levels should be somewhere in\nthe neighborhood of $(0, 1.4)$-ish. We'll use a linear model, which means\nwe assume that:\n$$\n\\begin{aligned}\n\\log(y_i^*) &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_{1, i} + \\beta_2 x_{2, i} + \\beta_3x_{3, i}.\n\\end{aligned}\n$$\nHere, $y_i$ is the concentration of glyphosate in tap water of house $i$,\n$$x_{1, i}$$ is the distance from house $i$ to the farm, $x_{2, i}$ is the\npersonal glyphosate use variable, and $x_{3, i}$ is the water filter use\nvariable. So we need to pick all four of those $\\beta_p$ coefficients and the\nvalue of $\\sigma^2$, the residual variance, before we can simulate our glyphosate\nlevels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a table of true coefs for using later\ncoefs <-\n\ttibble::tribble(\n\t\t~term, ~estimate,\n\t\t\"(Intercept)\", 0.8,\n\t\t\"dist_from_farm\", -4,\n\t\t\"personal_gly_use\", 0.5,\n\t\t\"water_filter_use\", -0.4\n\t)\nres_sd <- 0.25\n\ngly_data <-\n\tgly_preds |>\n\tdplyr::mutate(\n\t\tmu = 0.8 - 4 * dist_from_farm + 0.5 * personal_gly_use -\n\t\t\t0.4 * water_filter_use,\n\t\ty_star = exp(rnorm(N, mu, res_sd))\n\t)\n\nLoD <- 0.33\n\nsummary(gly_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n house_x_coord       house_y_coord      dist_from_farm    personal_gly_use\n Min.   :0.0003474   Min.   :0.004179   Min.   :0.04354   Min.   :0.0000  \n 1st Qu.:0.2796628   1st Qu.:0.243920   1st Qu.:0.25278   1st Qu.:0.0000  \n Median :0.5228575   Median :0.471412   Median :0.39295   Median :0.0000  \n Mean   :0.5094258   Mean   :0.484296   Mean   :0.36918   Mean   :0.2381  \n 3rd Qu.:0.7452386   3rd Qu.:0.731020   3rd Qu.:0.46563   3rd Qu.:0.0000  \n Max.   :0.9906258   Max.   :0.998614   Max.   :0.67715   Max.   :1.0000  \n water_filter_use       mu              y_star       \n Min.   :0.0000   Min.   :-2.0985   Min.   :0.09508  \n 1st Qu.:0.0000   1st Qu.:-1.1521   1st Qu.:0.31413  \n Median :0.0000   Median :-0.7472   Median :0.47621  \n Mean   :0.3878   Mean   :-0.7128   Mean   :0.62597  \n 3rd Qu.:1.0000   3rd Qu.:-0.2702   3rd Qu.:0.76989  \n Max.   :1.0000   Max.   : 1.1258   Max.   :3.71580  \n```\n\n\n:::\n:::\n\n\nYou can see from the above simulation code that the values I ended up choosing\nare as follows:\n\n$$\n\\begin{aligned}\n\\log(y_i^*) &\\sim \\text{Normal}(\\mu_i, 0.5^2) \\\\\n\\mu_i &= -2.5 + 2\\cdot x_{1, i} + 0.5\\cdot x_{2, i} -0.4\\cdot x_{3, i}.\n\\end{aligned}\n$$\nThese values seemed to give a reasonable range of $y$ values (on the natural\nscale), and have signs that made sense to me. The intercept represents\nthe concentration of glyphosate expected in tap water for a person who lives\non the farm, no personal glyphosate use, and no water filter, and is\nabout $2.23$ ppm, which is quite high and perhaps expected for the point source\nof the contaminant. With these parameters, we will have\n$28.6\\%$ of data points below\nthe limit of detection, which is not ideal (of course the ideal is zero\npercent), but not too bad either.\n\nOur censoring model looks like this:\n\n$$\ny_i = \\begin{cases}\nL, & y_i^* < 0.33 \\text{ ppm} \\\\\ny_i^*, & \\text{otherwise}\n\\end{cases}.\n$$\nOur censoring indicator $C_i$ will look like this:\n\n$$\nc_i = \\begin{cases}\n1, & y_i^* < 0.33 \\text{ ppm} \\\\\n0, & \\text{otherwise}\n\\end{cases}.\n$$\nLet's first apply the censoring to our data. We'll arbitrarily choose $L = 0$.\nThen we'll take a look at the data we would actually observe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dataset including latent variables\n# Do the censoring\nL <- 0.1\ngly_data_lnt <-\n\tgly_data |>\n\tdplyr::mutate(\n\t\t# Create the censoring indicator\n\t\tc = ifelse(y_star <= LoD, 1, 0),\n\t\t# Create the censored outcome\n\t\ty = ifelse(c == 1, L, y_star)\n\t)\n\n# Dataset including ONLY the observed variables\ngly_data_obs <- gly_data_lnt |>\n\tdplyr::select(dist_from_farm, personal_gly_use, water_filter_use, c, y)\n\nhead(gly_data_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  dist_from_farm personal_gly_use water_filter_use     c     y\n           <dbl>            <int>            <int> <dbl> <dbl>\n1          0.369                1                1     0 0.369\n2          0.466                0                0     0 0.371\n3          0.307                0                1     1 0.1  \n4          0.625                0                1     1 0.1  \n5          0.259                0                0     0 0.913\n6          0.677                0                0     1 0.1  \n```\n\n\n:::\n:::\n\n\nNote that setting $y_i = 0.1$ if the value is censored is **completely arbitrary**.\nMany people will set it to a value like the LoD, or half the LoD, or some\ncrazy thing with $\\sqrt{2}$ in it, and then pretend those are the real values.\n**All of these arbitrary values are equally bad.** Let's look at the\ndistribution of the latent and observed values just to show this.\nIn real life, we can't see this, but this example should remind us that picking\nan arbitrary number is not very good.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Arrange the data correctly for plotting\ngly_data_lnt |>\n\tdplyr::select(\n\t\t\"observed\" = y,\n\t\t\"latent\" = y_star\n\t) |>\n\ttidyr::pivot_longer(dplyr::everything()) |>\n\t# Now make the plot\n\tggplot() +\n\taes(x = value) +\n\tgeom_histogram(\n\t\tboundary = 0,\n\t\tbinwidth = 0.1,\n\t\tcolor = \"black\",\n\t\tfill = \"gray\"\n\t) +\n\tfacet_wrap(vars(name)) +\n\tggtitle(paste0(\"Censored values coded as \", L))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's also take a look at what the data would have looked like if we set, say\n$L = 0.33$ (the LoD).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngly_data_lnt |>\n\t# Set censored values to LoD\n\tdplyr::mutate(y = ifelse(y == L, 0.33, y)) |>\n\t# Arrange the data for the plot\n\tdplyr::select(\n\t\t\"observed\" = y,\n\t\t\"latent\" = y_star\n\t) |>\n\ttidyr::pivot_longer(dplyr::everything()) |>\n\t# Make the plot\n\tggplot() +\n\taes(x = value) +\n\tgeom_histogram(\n\t\tboundary = 0,\n\t\tbinwidth = 0.1,\n\t\tcolor = \"black\",\n\t\tfill = \"gray\"\n\t) +\n\tfacet_wrap(vars(name)) +\n\tggtitle(\"Censored values coded as LoD\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can see how this histogram makes the censored values look like actual data\n(cat screaming emoji)!! Whereas the previous set of histograms with a spike at\nzero should signal that there is something strange going on in the data. So\nsubstitution can cause data to be misleading. For this reason, it can sometimes\nbe useful for analysts to record values as \"Nondetect\" or \"< LoD\" in the dataset\n(or some other kind of text indicating it is not a regular number),\nforcing the analyst to clean up the data before it can be statistically examined.\nThe problem can be somewhat avoided if we include an explicit indicator of\ncensoring in our data, like so.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngly_data_lnt |>\n\t\t# Set censored values to LoD\n\tdplyr::mutate(y = ifelse(y == L, 0.33, y)) |>\n\tdplyr::select(\n\t\t\"observed\" = y,\n\t\t\"latent\" = y_star,\n\t\tc\n\t) |>\n\ttidyr::pivot_longer(-c) |>\n\t# Now make the plot\n\tggplot() +\n\taes(x = value, fill = factor(c)) +\n\tgeom_histogram(\n\t\tboundary = 0,\n\t\tbinwidth = 0.1,\n\t\tcolor = \"black\",\n\t\talpha = 0.5,\n\t\tposition = \"stack\"\n\t) +\n\tfacet_wrap(vars(name)) +\n\tscale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"), name  = \"Below LoD?\") +\n\tggtitle(paste0(\"Censored values coded as LoD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngly_data_lnt |>\n\tdplyr::select(\n\t\t\"observed\" = y,\n\t\t\"latent\" = y_star,\n\t\tc\n\t) |>\n\ttidyr::pivot_longer(-c) |>\n\t# Now make the plot\n\tggplot() +\n\taes(x = value, fill = factor(c)) +\n\tgeom_histogram(\n\t\tboundary = 0,\n\t\tbinwidth = 0.1,\n\t\tcolor = \"black\",\n\t\talpha = 0.5,\n\t\tposition = \"stack\"\n\t) +\n\tfacet_wrap(vars(name)) +\n\tscale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"), name  = \"Below LoD?\") +\n\tggtitle(paste0(\"Censored values coded as \", L))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nHere, we can see that the histogram conflates part of the censored and\nnon-censored values because of the binwidth we set. All that is to show, when\nthere is a possibly of censored data, we should be extra careful as analysts to\nmake sure we aren't computing incorrect statistics.\n\nAs another instructive example, let us first attempt to estimate the mean and SD\nof the glyphosate concentrations. We know an unbiased estimate of marginal mean\nand CI (that is, the statistic if we ignore all of the `x` values), because we\nhave the underlying latent values. So let's estimate those first.\n(Because we have a normal distribution, we could probably get the analytical\nmarginal mean assuming unknown $x_i, p$ values, but we won't do that here.)\nIn R, we can quickly construct the Wald-type CI based on the `t`-distribution\nusing the `t.test()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlatent_t_test <-\n\tgly_data_lnt$y_star |>\n\t# Remember we made a log-normal assumption so we take the log here\n\tlog() |>\n\tt.test() |>\n\tbroom::tidy() |>\n\t# Re-exponentiate the results\n\tdplyr::mutate(dplyr::across(c(estimate, conf.low, conf.high), exp))\nprint(latent_t_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>     <chr>      \n1    0.485     -12.5 1.05e-24       146    0.432     0.544 One Samp… two.sided  \n```\n\n\n:::\n:::\n\n\nNow, if we compute the same test using the observed values, we can see what\nhappens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_t_test <-\n\tgly_data_obs$y |>\n\t# Remember we made a log-normal assumption so we take the log here\n\tlog() |>\n\tt.test() |>\n\tbroom::tidy() |>\n\t# Re-exponentiate the results\n\tdplyr::mutate(dplyr::across(c(estimate, conf.low, conf.high), exp))\nprint(observed_t_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>     <chr>      \n1    0.391     -11.7 8.40e-23       146    0.334     0.458 One Samp… two.sided  \n```\n\n\n:::\n:::\n\n\nThe estimate is much lower! In fact, the 95% CI doesn't even cover the latent\nestimate! Of course, **we can arbitrarily change the estimate by recoding the\ncensored values.** If we bumped them up to the LoD, the estimate would go up\nand if we made them lower, it would go down.\n\nDespite knowing this, let's see what happens in our linear model.\n\n## Naive linear models\n\nNow, if we were entrusted with a data set for analysis and had no idea it was\ncensored, we would typically assume that values at the LoD are measured\nexactly. Though, as we discussed, some EDA might be suggestive, we will fit\nthe ordinary linear regression model we described in the data generating\nprocess earlier (here, we are fortunate enough to know that this is an\nappropriate model, which is always untrue in the real world).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nnaive_lm <-\n\tlm(\n\t\tlog(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use,\n\t\tdata = gly_data_obs\n\t)\n# Print the results in a little table\nnaive_lm |>\n\tbroom::tidy(conf.int = TRUE) |>\n\ttibble::add_column(truth = coefs$estimate) |>\n\tdplyr::select(term, estimate, conf.low, conf.high, truth) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term             | estimate| conf.low| conf.high| truth|\n|:----------------|--------:|--------:|---------:|-----:|\n|(Intercept)      |    0.967|    0.732|     1.202|   0.8|\n|dist_from_farm   |   -5.135|   -5.687|    -4.582|  -4.0|\n|personal_gly_use |    0.767|    0.578|     0.955|   0.5|\n|water_filter_use |   -0.496|   -0.661|    -0.331|  -0.4|\n\n\n:::\n:::\n\n\nHere in the table we can see that all of the estimates are biased away from the\nnull, which is exactly what we don't want in this kind of study -- we would, if\nanything, prefer that they be biased towards the null so we avoid overstating\nthe effect. Importantly, we can see that the CI's for `dist_from_farm`\nand `personal_gly_use` **do not even contain the true value**! So even though\nwe know the exact data generating process, and we know our model reflects that,\n**if we don't account for censoring, we can get completely wrong estimates**!\n\nSo, then, what are we to do?\n\n## Integration method for censored data\n\nWe can regain some measure of our lost diginity using the integration trick we\ndiscussed in the introduction. Of course, in the introduction, we only talked\nabout adjusting for censoring in the univariate case, but fortunately we\nare modeling the **conditional distribution of $y$** so we can use the same\ntrick:\n\n$$\n\\begin{aligned}\n\\mathcal{L}\\left(\\theta \\mid y_i, x_i \\right) &= f_{Y_i \\mid X_i = x_i}(y_i \\mid \\theta, x_i) \\\\\n&= \\bigg( f(y_i \\mid \\theta, x) \\bigg)^{1 - c_i} \\bigg( P(Y_i = y_i \\mid x)\\bigg)^{c_i} \\\\\n&= \\bigg( f(y_i \\mid \\theta, x_i) \\bigg)^{1 - c_i} \\bigg( \\int_{-\\infty}^{y_\\min} f(y_i \\mid \\theta, x_i) \\ dy_i \\bigg)^{c_i} \\\\\n&= \\bigg( f(y_i \\mid \\theta, x_i) \\bigg)^{1 - c_i} \\bigg( F(y_\\min \\mid \\theta, x_i) \\bigg)^{c_i}.\n\\end{aligned}\n$$\n\nThe likelihood for $y_i$ is easy to write out here since the censoring structure\nis (relatively) simple. This gives rise to the likelihood of the sample,\nwhich (under the assumption of mutual conditional independence) is\n$$\n\\mathcal{L}\\left(\\theta \\mid x, y \\right) =  \\prod_{i = 1}^n \\mathcal{L}\\left(\\theta \\mid y_i, x_i \\right).\n$$\n\nNow that we've conducted the likelihood, we can do either of the usual things\nwe would do to estimate the parameters: find the argument $\\theta$ that\nmaximizes the likelihood, or apply some priors and use an algorithm to estimate\na Bayesian posterior.\n\nDoing either of these is not too complicated for this specific example -- we\ncan easily write a function to optimize, or we could do some kind of grid\nor quadratic posterior approximation. For this example, neither of those is\nvery difficult and should converge easily. But, we have the benefit of\nexcellent statistical tools that have already been written, so we might as\nwell use them.\n\n### Frequentist models\n\nThe `R` package `survival` (which actually predates the `R` language) implements\nparametric models of this form for many common distributions. Don't let the\nname fool you: we can do models other than survival analysis. In this particular\ncase, we just need to specify a parametric model with a normal distribution and\nleft censoring. The syntax for this is a bit strange, we need to use the\n`Surv()` function to set up a \"survival object\" which we pass as the response\nvariable in the `survreg()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we have to transform the outcome\nsurv_model <- survival::survreg(\n\t# Creating the \"survival time\" outcome\n\tsurvival::Surv(\n\t\t# If the value is lower than LoD, replace it w/ LoD, then take the log\n\t\tpmax(y, LoD) |> log(),\n\t\t# The censoring indicator needs to be the opposite of what makes sense --\n\t\t# zero for censored, one for uncensored -- it's actually an indicator of\n\t\t# an \"event\" occurring, for us this is the event\n\t\t# \"getting a reliable measurement.\"\n\t\t!c,\n\t\t# Specify left censoring\n\t\ttype = 'left'\n\t) ~\n\t\t# All the other linear model stuff as usual\n\t\tdist_from_farm + personal_gly_use + water_filter_use,\n\tdata = gly_data_obs,\n\tdist = \"gaussian\"\n)\n\nsurv_model |>\n\tbroom::tidy(conf.int = TRUE) |>\n\ttibble::add_column(truth = c(coefs$estimate, log(res_sd))) |>\n\tdplyr::select(term, estimate, conf.low, conf.high, truth) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term             | estimate| conf.low| conf.high|  truth|\n|:----------------|--------:|--------:|---------:|------:|\n|(Intercept)      |    0.735|    0.588|     0.882|  0.800|\n|dist_from_farm   |   -3.854|   -4.245|    -3.463| -4.000|\n|personal_gly_use |    0.537|    0.423|     0.651|  0.500|\n|water_filter_use |   -0.388|   -0.494|    -0.282| -0.400|\n|Log(scale)       |   -1.264|       NA|        NA| -1.386|\n\n\n:::\n:::\n\n\nDespite all the finagling we have to do, we can see that the estimates are now\nmuch better. Although the point estimates are not as close as we would expect if\nwe had the latent uncensored outcome variable, the confidence intervals actually\ncontain the true values this time. (Note that `Log(scale)` is the estimated\nresidual SD on the log scale, but for some reason the CI doesn't get calculated\nby any of the `survival` methods.) Notably, the CIs for this model are actually\nsmaller than the the CIs for the naive model, even though we're assuming there\nis extra uncertainty in the outcome. But we have accounted for this uncertainty\nin the model, so it doesn't leak into the parameter estimates (of course that's\na non-technical explanation).\n\nThe worst part here is doing all that `Surv()` stuff, but fortunately there are\nways to avoid having to do all that. This method is commonly called the \"Tobit\nmodel\" in econometrics, and there is a well-developed literature around this\nmodel, and some variations and extensions. In particular, the `AER` package\nprovides a function called `tobit()` that allows one to specify these kind of\nsimple censoring models in standard `R` syntax, and automatically does the\nconversion to a `survreg()` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntobit_model <- AER::tobit(\n\tlog(y) ~ dist_from_farm + personal_gly_use + water_filter_use,\n\tdata = gly_data_obs,\n\tleft = log(LoD),\n\tright = Inf,\n\tdist = \"gaussian\"\n)\n\nsummary(tobit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nAER::tobit(formula = log(y) ~ dist_from_farm + personal_gly_use + \n    water_filter_use, left = log(LoD), right = Inf, dist = \"gaussian\", \n    data = gly_data_obs)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           147             42            105              0 \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       0.73500    0.07495   9.807  < 2e-16 ***\ndist_from_farm   -3.85438    0.19947 -19.323  < 2e-16 ***\npersonal_gly_use  0.53676    0.05830   9.208  < 2e-16 ***\nwater_filter_use -0.38809    0.05408  -7.176 7.15e-13 ***\nLog(scale)       -1.26354    0.06921 -18.257  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 0.2827 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 7 \nLog-likelihood: -33.8 on 5 Df\nWald-statistic: 432.1 on 3 Df, p-value: < 2.22e-16 \n```\n\n\n:::\n:::\n\n\nNow, if we want, it's also not too bad to do this in a Bayesian framework.\n\n### Bayesian models\n\nFirst we need to describe some basic priors that will work. For this example, we\nknow that the parameters actually have \"true values\" so assigning a degenerate\nprior would actually reflect our true beliefs, but it's silly. So we'll assign\nsome usual weakly informative priors that will help our model work right. If you\nwant to do the priors with the variance of 10000 to be \"objective\" or whatever\nyou can do that but I don't think it's a good idea. I'll base my priors largely\non the advice in [Statistical Rethinking by Richard\nMcElreath](https://xcelab.net/rm/) and the [Stan Prior Choice\nGuide](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations).\n\nWe need a prior for everything in our model that isn't observed in the data. For\nthis model, that's our residual variance and the beta coefficients. We'll set\nthe priors as follows:\n\n$$\n\\begin{aligned}\n\\beta_j &\\sim \\mathrm{Normal}\\left(0, 3\\right), \\quad j = {0, 1, 2, 3}; \\\\\n\\sigma &\\sim \\text{Half-t}\\left(0, 3, \\nu = 3\\right).\n\\end{aligned}\n$$\nhere, we use $\\mathrm{t}\\left(\\mu, \\sigma, \\nu\\right)$ to denote the\nlocation-scale Student's $t$-dsitribution with $\\nu$ degrees of freedom. We use\nnormal distributions centered at 0 for the $\\beta_j$ to indicate our skepticism\nabout the strength of the effects (if we were to pretend we didn't know the\ngenerative model). Because $\\sigma$ must be postive, we actually use the \"half\"\nversion of the distribution, which has strictly positive support.\n\nNow, for this kind of model where only the outcome is censored, we can actually\nhave the super-handy `brms` R package do the heavy lifting for us. All we have\nto do is a bit of data cleaning, and a bit of working specifying the model\nstructure, but the package will handle writing, compiling, and running all of\nthe Stan code, which is very conveninent. First we'll fit the naive model, to\nshow the `brms` syntax, and then we'll cover the censored adjustment.\n\n#### naive model in `brms`\n\nAt first glance, the `brms` syntax appears to be quite difficult to use. While\ncomplex, it is concise and specifying the syntax is much less difficult than\nwriting Stan code for many models. Fitting a `brms` model broadly requires 5\nthings from us: the first two are the formula, in `lm()` type syntax with a few\nextras, and the data, as we expect. We also need to specify a distributional\n`family` argument for the likelihood, which we implicitly do when we call\n`lm()`. As with base `R`, the `gaussian` likelihood is the default in `brms`,\nbut it's good practice to be clear. We also need to specify the `priors`, which\nare written in a specific way, and the Stan control parameters which control the\nBayesian routine. Because `brms` is a wrapper for Stan's NUTS sampler, the\nfitting routine is much more complicated than the routine for `lm()` and there's\na lot we can change.\n\nIf we just ignore these arguments, we can still get some results. Note that when\nyou run a `brms` model like this, you'll first get a compiling message, then\nseveral intermittant progress updates about how fitting is going. I have those\nsilenced here because it generates a lot of them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbasic_brms_fit <- brms::brm(\n\tformula = log(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use,\n\tdata = gly_data_obs,\n\tsilent = 2\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(basic_brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(y) ~ 1 + dist_from_farm + personal_gly_use + water_filter_use \n   Data: gly_data_obs (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.97      0.12     0.74     1.20 1.00     4615     2983\ndist_from_farm      -5.13      0.28    -5.68    -4.58 1.00     4761     2902\npersonal_gly_use     0.77      0.10     0.57     0.95 1.00     4472     2826\nwater_filter_use    -0.50      0.08    -0.66    -0.33 1.00     5215     3088\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.03     0.44     0.56 1.00     4653     2929\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nBecause the model is simple, we don't have to do much. The estimates here are\nvery similar to the estimates from the naive frequentist model. Let's apply some\npriors and some Stan arguments and see what happens. First, we'll set up the\npriors.\n\nI recommend reading the `brms` documentation and papers to learn more about how\nthis prior syntax works, but we can also take a look at the default priors that\n`brms` set for our basic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(basic_brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class             coef group resp dpar nlpar lb ub\n                  (flat)         b                                             \n                  (flat)         b   dist_from_farm                            \n                  (flat)         b personal_gly_use                            \n                  (flat)         b water_filter_use                            \n student_t(3, -0.7, 2.5) Intercept                                             \n    student_t(3, 0, 2.5)     sigma                                         0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\nHere, we can see that the coefficients have `(flat)` priors, which often works\nfor this simple of a model, but is usually not a good choice because it can\nbe difficult for the sampler. The default prior for `sigma`, the residual SD,\nis very similar to what I picked, and I think is generally a good default\nprior. Anyways, based on the `class` column from this output, we can figure\nout how to set up a prior. Note that the `lb` for sigma is the *lower bound*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_priors <- c(\n\tprior(normal(0, 3), class = \"b\"),\n\tprior(normal(0, 3), class = \"Intercept\"),\n\tprior(student_t(3, 0, 3), class = \"sigma\", lb = 0)\n)\n\nmy_priors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              prior     class coef group resp dpar nlpar   lb   ub source\n       normal(0, 3)         b                            <NA> <NA>   user\n       normal(0, 3) Intercept                            <NA> <NA>   user\n student_t(3, 0, 3)     sigma                               0 <NA>   user\n```\n\n\n:::\n:::\n\n\nEverything else we pass in, other than these four arguments, will be Stan\ncontrol parameters. If you want to learn about Stan specifics, there are way\nbetter resources than this, so I recommend you read those. I just want to\nmake it clear that these other things are telling Stan how it should run the\nfitting routine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaive_brms_fit <- brms::brm(\n\tformula = log(y) ~ dist_from_farm + personal_gly_use + water_filter_use,\n\tdata = gly_data_obs,\n\tfamily = gaussian(),\n\tprior = my_priors,\n\twarmup = 1000,\n\titer = 2000,\n\tchains = 4,\n\tcores = 4,\n\tseed = 32134,\n\tbackend = \"cmdstanr\",\n\tsilent = 2\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(naive_brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(y) ~ dist_from_farm + personal_gly_use + water_filter_use \n   Data: gly_data_obs (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.95      0.12     0.71     1.20 1.00     4620     3214\ndist_from_farm      -5.09      0.29    -5.66    -4.53 1.00     4741     2952\npersonal_gly_use     0.77      0.10     0.57     0.96 1.00     4409     3002\nwater_filter_use    -0.50      0.09    -0.67    -0.33 1.00     4721     3286\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.03     0.44     0.56 1.00     4308     3027\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe estimates don't change that much from the previous fit. Again, that's\nbecause a simple linear model with data generated from a linear model is really\neasy to handle. Once you start getting into more complicated models, the\npriors and stan arguments can matter a lot. But now that we've got all\nthat taken care of, we should talk about censoring.\n\n#### censored model in `brms`\n\nIn `brms`, we can handle censoring by a special formula syntax that tells the\nmodel to do a likelihood adjustment. Notably, how to do this is kind of\nburied in the documentation. And at time of writing, it never explains that the\nlikelihood adjustment is what it actually does (but it is). The best place to\nread about this functionality and other additional options for `brms` is on\n[this page](https://paul-buerkner.github.io/brms/reference/brmsformula.html) of\nthe documentation. We need to do a bit of data cleaning first before we can\nhandle the censoring though. As you can see on that page, we need our censoring\nindicator variable to contain `-1` for left-censored data, instead of `1` like\nwe currently have. The censoring bound should be given in `y`, so we need to\ntransform it again so that all censored values are written down as the actual\nLoD.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms_data <-\n\tgly_data_obs |>\n\tdplyr::mutate(\n\t\t# Transform censoring indicator\n\t\tcensored = ifelse(c == 1, -1, 0),\n\t\t# Transform the outcome\n\t\toutcome = pmax(log(y), log(LoD)),\n\t\t# Only keep the variables we plan to give to brms\n\t\t.keep = \"unused\"\n\t)\n\nhead(brms_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  dist_from_farm personal_gly_use water_filter_use censored outcome\n           <dbl>            <int>            <int>    <dbl>   <dbl>\n1          0.369                1                1        0 -0.997 \n2          0.466                0                0        0 -0.991 \n3          0.307                0                1       -1 -1.11  \n4          0.625                0                1       -1 -1.11  \n5          0.259                0                0        0 -0.0908\n6          0.677                0                0       -1 -1.11  \n```\n\n\n:::\n:::\n\n\nNow we can fit the `brms` model with the censoring correction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncens_brms_fit <- brms::brm(\n\tformula = outcome | cens(censored) ~ dist_from_farm + personal_gly_use +\n\t\twater_filter_use,\n\tdata = brms_data,\n\tfamily = gaussian(),\n\tprior = my_priors,\n\twarmup = 1000,\n\titer = 2000,\n\tchains = 4,\n\tcores = 4,\n\tseed = 32134,\n\tbackend = \"cmdstanr\",\n\tsilent = 2\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cens_brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome | cens(censored) ~ dist_from_farm + personal_gly_use + water_filter_use \n   Data: brms_data (Number of observations: 147) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.73      0.08     0.58     0.88 1.00     4699     2549\ndist_from_farm      -3.86      0.20    -4.24    -3.48 1.00     4206     3284\npersonal_gly_use     0.54      0.06     0.42     0.66 1.00     4619     3286\nwater_filter_use    -0.39      0.06    -0.50    -0.28 1.00     4747     3042\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.29      0.02     0.25     0.34 1.00     4037     2797\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nAgain, our results are similar to the frequentist model. All of the true\nvalues are now inside of the estimated 95% credible intervals, with the\none exception of `sigma`, the residual SD, which had a true value of 0.25 --\nso we'll say it's inside the credible interval up to measurement/computer\nprecision, there's no way that small of a difference would practically matter.\n\nUnfortunately, not every model can be handled with `brms`, even though it's\nvery flexible. For example, when we talk about censored predictor values, we\nwon't be able to use `brms` for the approach we'll use. Instead, we'll have to\nwrite our own Stan code, which is even more flexible than `brms` (though it\nlacks many of the convenience features).\n\n#### Custom Stan code with `cmdstanr`\n\nMoving from using only `R` to writing a `Stan` program is the biggest jump we\nhave to make in our quest to deal with censored data. If you don't know anything\nabout Stan, I recommend the Statistical Rethinking book mentioned earlier, as\nthe associated [`rethinking` package](https://github.com/rmcelreath/rethinking)\nis quite easy to use to specify a lot of basic statistical models (with\nguidance from the text), and provides a method to view the Stan code generated\nby a model. The `brms` package also provides the `stancode()` method, but the\nStan code generated by `brms` employs many tips to make the model more efficient,\nand so can be difficult to interpret if you aren't familiar with Stan.\n\nAnyways, I don't plan to explain the details of writing Stan code here, like I\nmentioned with `brms` and the `R` language and even the statistics stuff we're\ndoing, there are a lot better resources you can find with a quick google that\nare better than what I could write. So instead I already wrote the Stan code,\nand here's the code.\n\n::: {.callout-note icon=false collapse=true appearance=\"simple\"}\n##### Model code {.unnumbered}\n\n```{.stan include=D:/proj/Bayesian-Censoring/examples/simple-censored-outcome/censored-outcome-integration.stan}\n```\n\n:::\n\nNow we can talk about running Stan code in `R`, which I actually will talk a bit\nabout. Once you have the Stan file set up, you have to figure out how to get\nStan running. I'll use (and recommend that everyone else) use `cmdstanr`, which\nis an R package you can install. You'll need to do some extra setup if you've\nnever used the package before, so follow\n[their guide](https://mc-stan.org/cmdstanr/articles/cmdstanr.html) and I\nstrongly recommend following the steps outside of an R project, just open the\nbase R gui and copy and paste their code.\n\nOnce you have `cmdstanr` installed, the first thing we need to do is load the\nStan file and compile the model. I almost always prefer to do this in two\nseparate steps in case I need to control or force the compilation step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_pth <- here::here(pth_base, \"censored-outcome-integration.stan\")\nstan_mod <- cmdstanr::cmdstan_model(stan_file = stan_pth, compile = FALSE)\n```\n:::\n\n\nThe `stan_mod` object is set up with `cmdstanr` now, so it knows where to find\nthe Stan code it will need to run. If you try to print that object, it will\nprint the Stan code and will also tell you if there are any syntactical errors\nin the code. Since there are no syntactical errors, we can try and *compile*\nthe Stan code. This step is likely not familiar if you only use R code. Stan\ncode needs to be translated into machine-level language before it can be run,\nwhich dramatically speeds up the run time for complex models. This creates an\n`.exe` file which can be run by `cmdstanr` without needing to compile again.\nHere I'll compile in *pedantic mode* which will give you suggestions on common\nissues that can make your model worse, without explicitly causing errors. This\nwill spit out a lot of messages that I've elected to hide. They aren't very\nuseful most of the time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n```\n:::\n\n\nNow that the program is compiled, we need to set up our data in a specific\nformat before we can run the HMC sampler. Stan accepts data in an R `list()`\nformat, where each of the items can be different sizes and types. You can\nsee in the `data{}` section of the printed Stan code what items you need to\nput in the list, and what their types should be. So we'll do some quick data\ncleaning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <- list(\n\t# Number of data points\n\tn = nrow(gly_data_obs),\n\t# number of predictors\n\tp = 3,\n\t# vector of outcome values\n\ty = brms_data$outcome,\n\t# Vector of censoring indicators\n\tc = gly_data_obs$c,\n\t# Matrix of predictor values (size n x 3)\n\tX = with(\n\t\tgly_data_obs,\n\t\tmatrix(\n\t\t\tc(dist_from_farm, personal_gly_use, water_filter_use),\n\t\t\tncol = 3\n\t\t)\n\t),\n\t# limit of detection -- needs to be on same scale as y vector\n\tDL = log(LoD)\n)\nstr(stan_data, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ n : int 147\n $ p : num 3\n $ y : num [1:147] -0.997 -0.9906 -1.1087 -1.1087 -0.0908 ...\n $ c : num [1:147] 0 0 1 1 0 1 0 0 0 0 ...\n $ X : num [1:147, 1:3] 0.369 0.466 0.307 0.625 0.259 ...\n $ DL: num -1.11\n```\n\n\n:::\n:::\n\n\nWith the data in this format, we can go ahead and pass it to Stan and invoke\nthe sampler. Since all of the model options are specified in the actual Stan\ncode file this time, everything we pass to the `sample()` method is a control\nargument for how cmdstan runs the sampler.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_fit <- stan_mod$sample(\n\tdata = stan_data,\n\tseed = 10896,\n\tchains = 4,\n\tparallel_chains = 4,\n\titer_sampling = 4000,\n\titer_warmup = 1000,\n\trefresh = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_fit$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  variable   mean median     sd    mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     57.9   58.2   1.63   1.43   54.7   59.9    1.00    6723.    9400.\n2 alpha     0.733  0.733 0.0764 0.0761  0.608  0.859  1.00    8918.    9308.\n3 beta[1]  -3.86  -3.86  0.204  0.205  -4.20  -3.53   1.00    9249.    9271.\n4 beta[2]   0.539  0.538 0.0608 0.0611  0.440  0.639  1.00   14857.   10546.\n5 beta[3]  -0.388 -0.388 0.0564 0.0557 -0.483 -0.297  1.00   14272.   10411.\n6 sigma     0.291  0.290 0.0211 0.0208  0.259  0.329  1.00   14809.   10120.\n```\n\n\n:::\n:::\n\n\n\nIf you compare the numbers between this fit and the `brms` fit, they're pretty\nmuch exactly the same. Again, it's largely because this model is so simple.\nIf we were doing a more complicated `brms`-compatible model, the `brms` fit\nwould probably be a bit better cause of all the tricks it does to write the\nStan code in an efficient way. But for models with censored predictors we'll\nhave to use our own Stan code, so it's worth seeing how to do it for this\nrelatively easier case.\n\n## Conclusions\n\nIn this example, we saw how to deal with censored outcomes in regression models.\nThis method of adjusting the likelihood generalizes to arbitrarily complex\nmodels as long as the outcome follows this kind of simple censoring pattern.\nThis method also generalizes to each observaiton of the outcome having a\ndifferent limit of detection, and we can have both upper and lower LoDs, or even\ninterval censoring patterns, as long as we know and we can write down the\ncensoring limits for each observation. But we'll discuss more of that kind of\nstuff in future examples.\n\nBecause only the outcome is censored, there are a lot of methods available to\nus for dealing with this kind of data, including pre-built methods in\n`survival` for frequentist ML models, and `brms` for Bayesian models. But as\nwe'll see in the next example, not everything can be so easy.\n\n## Appendix: notes on modeling {.appendix .unnumbered}\n\nSince this is a simple example focused on dealing with the censoring adjustment,\nI decided not to get into the weeds about modeling anywhere else. There are a\nfew technical choices I made/didn't make, so I decided to write them down\nhere for posterity, without cluttering up the main text.\n\n* You should definitely use `cmdstanr` for running Stan models. In general,\nit stays up to date much better, is a bit easier to use, and is compatible with\nalmost everything `rstan` is (and the formats are interchangable). It can be\nannoying to install, but it's worth the hassle.\n* A lot of people like flat or very diffuse priors because they are \"objective\".\nI dislike them for a number of reasons. First of all, they often reduce the\nability of the model to sample efficiently. Second, due to the inherent\nresearcher degrees of freedom in setting up the model structure and what data\nare present, I don't think talking about the \"objectivity\" of priors makes any\nsense. Finally, the priors represent our prior beliefs about the parameters, and\na flat prior means we think any possible value is equally likely. This is why\npeople often think this is an \"objective\" prior but I think that's silly. I\ncertainly don't think my beta coefficient is equally likely to be 1 vs. negative\nthree billion and seven, but that's what the flat prior says. In general, I\nknow the effect should be fairly weak, so we should use skeptical priors that\nallow the parameter to get larger if the data support that.\n* In general for \"generic priors\", I tend to prefer $t$ and half-$t$ priors\nwith 3 degrees of freedom. These have thicker tails than normal/half-normal\npriors, but not as thick as Cauchy priors, which some people will recommend.\nA Cauchy prior reflects a belief that the variance of a variable is infinite,\nwhich is not plausible to me, and Cauchy priors also cause sampling efficiency\nto plummet in many cases without much of a benefit. So in general if I had to\npick a \"generic default prior\" it would be $t$ with three degrees of freedom.\n* I didn't talk about centering and scaling here, but in general you should\ntypically scale your variables (e.g. divide by the MAD) before putting them\ninto Stan, simply because it helps the sampler.\n* Centering can also be helpful, and if you want to do any kind of variable\nselection or comparison between variables, you need to center them. Centering\ncan make coefficient interpretations more difficult, but one of the benefits of\nBayesian modeling is that any posterior calculation we want to make comes with\nan entire posterior distribution of estimates, so we never have to worry about\nfiguring out CIs for a posteriori calculations. In general it's often better to\ndo these kind of computational tricks to make the model fit efficiently, and\nthen do transformations and predictions of interest afterwards.\n\n## Appendix: imputation methods {.appendix .unnumbered}\n\nThe other approach to dealing with censoring is to use a constrained imputation\nmethod. In general, multiple imputation (MI) works well with Bayesian inference,\nand you can implement MI and subsequent pooling methods to correct for censoring,\nso long as the MI method can be constrained to account for the limits on\ncensored values.\n\nIn addition to standard MI methods, you can also impute values within the Stan\nmodel -- this is a big advantages of Bayesian methods. I won't go into details\nabout either of these imputation methods, but for a brief discussion you can\nread this [StackExchange\npost](https://stats.stackexchange.com/questions/482079/how-best-to-deal-with-a-left-censored-predictor-because-of-detection-limits-in).\nThe [Stan\nmanual](https://mc-stan.org/docs/stan-users-guide/truncation-censoring.html#censored-data)\nhas an example of this as well. Similar imputation methods are covered in books\nlike Bayesian Data Analysis by Gelman et al, and we can account for censoring by\nspecifying constraints.\n\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
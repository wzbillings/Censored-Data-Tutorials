{
  "hash": "15f75c69d6b08a899c1fc9584cb7d53d",
  "result": {
    "markdown": "# Example Model 2: One censored outcome\n\n\n\n\n\nFor the second example model, we'll work on a case where there is one predictor\n(the models generalize easily to the multivariable case) and the outcome is\ncensored. \n\n## Lower limit of detection\n\nFor the first example, we'll work with an outcome that has a lower limit of\ndetection. First we need to simulate the data, which means we need to write\nout a generative model for the data. We'll randomly sample `x` for the purposes\nof generating data, but for the purposes of our model we'll assume `x_i` is\na completely observed covariate and thus is known and does not need a random\ncomponent in the model.\n\n$$\n\\begin{align*}\ny_i &= \\begin{cases}\n\\mathrm{DL}, & y^*_i \\leq \\mathrm{DL} \\\\\ny^*_i, & y^*_i > \\mathrm{DL}\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}\\left(\\mu_i, \\sigma^2\\right) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\ni &= 1, 2, \\ldots, n\n\\end{align*}\n$$\nHere, DL is the [D]{.underline}etection [L]{.underline}imit, aka the lower limit\nof detection for the variable. Of course in our generative model, we have\nset $\\alpha$, $\\beta$, and $\\sigma^2$ to be fixed population parameters, but\nfor Bayesian inference we would need to assign suitable priors. Let's set the\nvalues and simulate our data. The parameters I set for this example are as follows.\n\n\n::: {.cell}\n\n:::\n\n\n| Parameter     | Value | Meaning                       |\n|---------------|-------|-------------------------------|\n| $n$           | 271   | Sample size                   |\n| $\\alpha$      | 72    | Regression intercept          |\n| $\\beta$       | 3     | Regression slope              |\n| $\\sigma$      | 5     | Standard deviation of outcome |\n| $\\mathrm{DL}$ | 80    | Lower limit of detection      |\n\nThe $x$-values were drawn from a uniform distribution on $(0, 10)$. Since we\nknow the true population parameters for our simulation, we can plot the data to\nsee the effect of the censoring process on our observed $y$ values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIn this plot, the black data points show our observed data. For those\nobservations where the $y$ value was below the limit of detection and thus\ncensored, the gray points show the true latent values, which we could have\nobserved with a perfect measurement process. The gray line segments connect each\nlatent measurement to its corresponding observed measurement.\n\nApproximatly $22.88\\%$ of data points were below the limit of detection and\nwere therefore censored. Of course in real life, we would only observe the\nblack points (observed values), and the gray points would be unobservable to\nus. But for the purposes of understanding how to analyze censored data,\nvisualizing how different the observed and latent datasets are is quite\nvaluable and informative. Since the datasets look so different, we should not\nbe surprised that our regression estimates would be incorrect if we treated all\nof the censored values as the same constant value, or ignored them entirely!\n\nSo, if our standard linear regression model that we know and love (even the\nBayesian version) would give us incorrect estimates using any of these\nnaive methods, how then are we to proceed? According to the Stan manual\n[@StanManual, chap. 4], there are two main ways of handling the censoring in\nthe outcome in our model. The first of these methods relies on imputation and\nthe second on integration of the likelihood function and manual updating of the\ntarget likelihood in Stan. The imputation method is conceptually easier and\nless mathematically daunting, so we begin our treatment there.\n\n### Imputation-type method\n\nThe first method for dealing with censored data treats the censored values as\nmissing values where the latent value is constrained to fall within a specific\nrange. For a normally distributed outcome, all values below the lower limit\nof detection are constrained to fall within $(-\\infty, \\mathrm{DL})$.\n\nREAD THAT PART OF RETHINKING AND EXPLAIN HOW MISSING DATA WORKS HERE!!!\n\nTo implement such a model in Stan, we need to pass in the number of observed\nand the number of censored values and the observed y-values in Stan. We then\ndeclare the censored $y$-values as a parameter in the Stan code, meaning they\nwill be sampled from their constrained distribution during the fitting process,\nwhereas the observed $y$ values will be used to update the parameter estimates.\n\nFirst, let's look at the Stan code for this model.\n\nSHOW THE STAN CODE HERE.\n\nSince the data need to be in kind of a clunky format to use this method, we\nfirst need to do some wrangle and get the data in the correct format for Stan.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_2a <- list()\nwith(\n\tsim_data, {\n\t\tdat_cens <- subset(sim_data, cens)\n\t\tdat_obs <- subset(sim_data, !cens)\n\t\tdat_2a$N_cens <<- nrow(dat_cens)\n\t\tdat_2a$N_obs <<- nrow(dat_obs)\n\t\tdat_2a$y_obs <<- dat_obs$y\n\t\tdat_2a$x_obs <<- dat_obs$x\n\t\tdat_2a$y_cens <<- dat_cens$y\n\t\tdat_2a$x_cens <<- dat_cens$x\n\t\tdat_2a$DL <<- as.integer(sim_parms$DL)\n\t}\n)\n\nstr(dat_2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ N_cens: int 62\n $ N_obs : int 209\n $ y_obs : num [1:209] 92.6 104.2 86.2 88.7 94.9 ...\n $ x_obs : num [1:209] 8.98 9.31 3.42 8.5 8.14 ...\n $ y_cens: num [1:62] 80 80 80 80 80 80 80 80 80 80 ...\n $ x_cens: num [1:62] 0.227 0.555 1.168 2.853 2.434 ...\n $ DL    : int 80\n```\n:::\n:::\n\n\nNow we can compile the Stan program (via `cmdstanr` as usual).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_pth <- here::here(pth_base, \"Ex2a.stan\")\nmod_2a <- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod_2a$compile(pedantic = TRUE, force_recompile = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.stan', line 56, column 21: Argument\n    0.01 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.stan', line 55, column 18: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning in 'C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.stan', line 54, column 19: Argument\n    100 suggests there may be parameters that are not unit scale; consider\n    rescaling with a multiplier (see manual section 22.12).\nWarning: The parameter y_cens has no priors. This means either no prior is\n    provided, or the prior(s) depend on data variables. In the later case,\n    this may be a false positive.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIn file included from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array/multi_array_ref.hpp:32,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/multi_array.hpp:34,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint/algebra/multi_array_algebra.hpp:22,\n                 from stan/lib/stan_math/lib/boost_1.78.0/boost/numeric/odeint.hpp:63,\n                 from stan/lib/stan_math/stan/math/prim/functor/ode_rk45.hpp:9,\n                 from stan/lib/stan_math/stan/math/prim/functor/integrate_ode_rk45.hpp:6,\n                 from stan/lib/stan_math/stan/math/prim/functor.hpp:15,\n                 from stan/lib/stan_math/stan/math/rev/fun.hpp:198,\n                 from stan/lib/stan_math/stan/math/rev.hpp:10,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.hpp:2:\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:180:45: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  180 |         : public boost::functional::detail::unary_function<typename unary_traits<Predicate>::argument_type,bool>\n      |                                             ^~~~~~~~~~~~~~\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIn file included from C:/rtools43/ucrt64/include/c++/13.2.0/string:49,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/locale_classes.h:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/bits/ios_base.h:41,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/ios:44,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/istream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/sstream:40,\n                 from C:/rtools43/ucrt64/include/c++/13.2.0/complex:45,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Core:50,\n                 from stan/lib/stan_math/lib/eigen_3.4.0/Eigen/Dense:1,\n                 from stan/lib/stan_math/stan/math/prim/fun/Eigen.hpp:22,\n                 from stan/lib/stan_math/stan/math/rev.hpp:4:\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:214:45: warning: 'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  214 |         : public boost::functional::detail::binary_function<\n      |                                             ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:252:45: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  252 |         : public boost::functional::detail::unary_function<\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:299:45: warning: 'template<cl\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nass _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  299 |         : public boost::functional::detail::unary_function<\n      |                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:345:57: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  345 |     class mem_fun_t : public boost::functional::detail::unary_function<T*, S>\n      |                                                         ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:361:58: warning: 'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  361 |     class mem_fun1_t : public boost::functional::detail::binary_function<T*, A, S>\n      |                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:377:63: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  377 |     class const_mem_fun_t : public boost::functional::detail::unary_function<const T*, S>\n      |                                                               ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:393:64: warning: \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  393 |     class const_mem_fun1_t : public boost::functional::detail::binary_function<const T*, A, S>\n      |                                                                ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:438:61: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  438 |     class mem_fun_ref_t : public boost::functional::detail::unary_function<T&, S>\n      |                                                             ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:454:62: warning: 'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  454 |     class mem_fun1_ref_t : public boost::functional::detail::binary_function<T&, A, S>\n      |                                                              ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:470:67: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  470 |     class const_mem_fun_ref_t : public boost::functional::detail::unary_function<const T&, S>\n      |                                                                   ^~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:487:68: warning: 'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  487 |     class const_mem_fun1_ref_t : public boost::functional::detail::binary_function<const T&, A, S>\n      |                                                                    ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:533:73: warning: 'template<class _Arg, class _Result> struct std::unary_function' is deprecated [-Wdeprecated-declarations]\n  533 |     class pointer_to_unary_function : public boost::functional::detail::unary_function<Arg,Result>\n      |                                                                         ^~~~~~~~~~~~~~\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:117:12: note: declared here\n  117 |     struct unary_function\n      |            ^~~~~~~~~~~~~~\nstan/lib/stan_math/lib/boost_1.78.0/boost/functional.hpp:557:74: warning: 'template<class _Arg1, class _Arg2, class _Result> struct std::binary_function' is deprecated [-Wdeprecated-declarations]\n  557 |     class pointer_to_binary_function : public boost::functional::detail::binary_function<Arg1,Arg2,Result>\n      |                                                                          ^~~~~~~~~~~~~~~\nC:/rtools43/ucrt64/include/c++/13.2.0/bits/stl_function.h:131:12: note: declared here\n  131 |     struct binary_function\n      |            ^~~~~~~~~~~~~~~\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t<T_x, T_sigma, T_l> stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n < 0.0)\n      | \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp: At global scope:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp<M>::write_array(boost::random::ecuyer1988&, std::vector<double, std::allocator<double> >&, std::vector<int>&, std::vector<double, std::allocator<double> >&, bool, bool, std::ostream*) const [with M = Ex2a_model_namespace::Ex2a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014, 0, 2147483563>, boost::random::linear_congruential_engine<unsigned int, 40692, 0, 2147483399> >; std::ostream = std::basic_ostream<char>]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector<double>& theta,\n      | \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.hpp:443: note:   by 'Ex2a_model_namespace::Ex2a_model::write_array'\n  443 |   write_array(RNG& base_rng, std::vector<double>& params_r, std::vector<int>&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp<M>::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = Ex2a_model_namespace::Ex2a_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014, 0, 2147483563>, boost::random::linear_congruential_engine<unsigned int, 40692, 0, 2147483399> >; Eigen::VectorXd = Eigen::Matrix<double, -1, 1>; std::ostream = std::basic_ostream<char>]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/Zane/AppData/Local/Temp/RtmpITeReG/model-4e4015ba1f36.hpp:443: note:   by 'Ex2a_model_namespace::Ex2a_model::write_array'\n  443 |   write_array(RNG& base_rng, std::vector<double>& params_r, std::vector<int>&\n      | \n```\n:::\n:::\n\n\nAnd since the program compiles correctly, we can use Stan's sampling algorithm\nto generate samples from the posterior distribution. We'll run 4 chains in\nparallel with 500 warmup iterations and 5000 sampling iterations per chains,\nwith all of the other control parameters (e.g. maximum treedepth and adaptive\ndelta) left at the `cmdstan` defaults. This many samples is overkill for this\nproblem, but it is also quite fast and thus we can do many samples just to be\nsafe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2a <- mod_2a$sample(\n\tdat_2a, seed = 100, parallel_chains = 4,\n\titer_warmup = 500,\n\titer_sampling = 5000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 5500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 1 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 1 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 1 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 1 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 2 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 2 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 2 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 3 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 3 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 3 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 4 Iteration:  100 / 5500 [  1%]  (Warmup) \nChain 4 Iteration:  200 / 5500 [  3%]  (Warmup) \nChain 4 Iteration:  300 / 5500 [  5%]  (Warmup) \nChain 4 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 1 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 1 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 2 Iteration:  400 / 5500 [  7%]  (Warmup) \nChain 2 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 2 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 2 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 3 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 3 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 3 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  500 / 5500 [  9%]  (Warmup) \nChain 4 Iteration:  501 / 5500 [  9%]  (Sampling) \nChain 4 Iteration:  600 / 5500 [ 10%]  (Sampling) \nChain 4 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 1 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 1 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 2 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 3 Iteration:  700 / 5500 [ 12%]  (Sampling) \nChain 3 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 4 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 4 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 1 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 1 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 2 Iteration:  800 / 5500 [ 14%]  (Sampling) \nChain 2 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration:  900 / 5500 [ 16%]  (Sampling) \nChain 3 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 4 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 4 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 1 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 1 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 2 Iteration: 1000 / 5500 [ 18%]  (Sampling) \nChain 2 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1100 / 5500 [ 20%]  (Sampling) \nChain 3 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 4 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 4 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 1 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 1 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1200 / 5500 [ 21%]  (Sampling) \nChain 2 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1300 / 5500 [ 23%]  (Sampling) \nChain 3 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 4 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 4 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 1 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 2 Iteration: 1400 / 5500 [ 25%]  (Sampling) \nChain 3 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 3 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 4 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 4 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 1 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 1 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 2 Iteration: 1500 / 5500 [ 27%]  (Sampling) \nChain 2 Iteration: 1600 / 5500 [ 29%]  (Sampling) \nChain 3 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 3 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 4 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 4 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 4 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 1 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 1 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 2 Iteration: 1700 / 5500 [ 30%]  (Sampling) \nChain 2 Iteration: 1800 / 5500 [ 32%]  (Sampling) \nChain 3 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 3 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 4 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 4 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 1 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 1 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 2 Iteration: 1900 / 5500 [ 34%]  (Sampling) \nChain 3 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 3 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 4 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 4 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 1 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 2 Iteration: 2000 / 5500 [ 36%]  (Sampling) \nChain 2 Iteration: 2100 / 5500 [ 38%]  (Sampling) \nChain 3 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 3 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 4 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 4 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 1 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 1 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2200 / 5500 [ 40%]  (Sampling) \nChain 2 Iteration: 2300 / 5500 [ 41%]  (Sampling) \nChain 3 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 3 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 4 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 4 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 1 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 1 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 2 Iteration: 2400 / 5500 [ 43%]  (Sampling) \nChain 2 Iteration: 2500 / 5500 [ 45%]  (Sampling) \nChain 3 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 4 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 1 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 1 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 2 Iteration: 2600 / 5500 [ 47%]  (Sampling) \nChain 3 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 3 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 4 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 4 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 1 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 1 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 2 Iteration: 2700 / 5500 [ 49%]  (Sampling) \nChain 2 Iteration: 2800 / 5500 [ 50%]  (Sampling) \nChain 3 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 3 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 4 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 4 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 4 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 1 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 1 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 2 Iteration: 2900 / 5500 [ 52%]  (Sampling) \nChain 2 Iteration: 3000 / 5500 [ 54%]  (Sampling) \nChain 3 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 3 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 4 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 4 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 4 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 1 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 1 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 2 Iteration: 3100 / 5500 [ 56%]  (Sampling) \nChain 3 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 3 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 4 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 4 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 1 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 2 Iteration: 3200 / 5500 [ 58%]  (Sampling) \nChain 2 Iteration: 3300 / 5500 [ 60%]  (Sampling) \nChain 3 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 4 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 4 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 4 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 1 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 1 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 2 Iteration: 3400 / 5500 [ 61%]  (Sampling) \nChain 2 Iteration: 3500 / 5500 [ 63%]  (Sampling) \nChain 3 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 3 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 4 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 4 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 4 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 1 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 3600 / 5500 [ 65%]  (Sampling) \nChain 2 Iteration: 3700 / 5500 [ 67%]  (Sampling) \nChain 3 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 3 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 4 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 4 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 4 finished in 2.8 seconds.\nChain 1 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 1 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 1 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 2 Iteration: 3800 / 5500 [ 69%]  (Sampling) \nChain 3 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 3 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 1 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 1 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 3900 / 5500 [ 70%]  (Sampling) \nChain 2 Iteration: 4000 / 5500 [ 72%]  (Sampling) \nChain 3 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 3 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 1 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 1 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 4100 / 5500 [ 74%]  (Sampling) \nChain 2 Iteration: 4200 / 5500 [ 76%]  (Sampling) \nChain 3 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 3 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 3 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 1 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 1 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 4300 / 5500 [ 78%]  (Sampling) \nChain 2 Iteration: 4400 / 5500 [ 80%]  (Sampling) \nChain 3 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 3 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 1 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 1 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 Iteration: 4500 / 5500 [ 81%]  (Sampling) \nChain 2 Iteration: 4600 / 5500 [ 83%]  (Sampling) \nChain 3 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 3 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 1 finished in 3.5 seconds.\nChain 3 finished in 3.5 seconds.\nChain 2 Iteration: 4700 / 5500 [ 85%]  (Sampling) \nChain 2 Iteration: 4800 / 5500 [ 87%]  (Sampling) \nChain 2 Iteration: 4900 / 5500 [ 89%]  (Sampling) \nChain 2 Iteration: 5000 / 5500 [ 90%]  (Sampling) \nChain 2 Iteration: 5100 / 5500 [ 92%]  (Sampling) \nChain 2 Iteration: 5200 / 5500 [ 94%]  (Sampling) \nChain 2 Iteration: 5300 / 5500 [ 96%]  (Sampling) \nChain 2 Iteration: 5400 / 5500 [ 98%]  (Sampling) \nChain 2 Iteration: 5500 / 5500 [100%]  (Sampling) \nChain 2 finished in 4.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.4 seconds.\nTotal execution time: 4.3 seconds.\n```\n:::\n\n```{.r .cell-code}\n# Extract the posterior samples in a nicer format for later\npost_2a <- posterior::as_draws_df(fit_2a)\n```\n:::\n\n\nThe first thing we should do after sampling is check for any diagnostic\nwarnings. We have access to all of the individual diagnostics, but fortunately\n`cmdstan` has a built-in diagnostic checker to flag any potential problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2a$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2a-202311160806-1-5f59e6.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2a-202311160806-2-5f59e6.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2a-202311160806-3-5f59e6.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2a-202311160806-4-5f59e6.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n```\n:::\n:::\n\n\nGreat, no issues with the sampling procedure, that is what we like to see.\nLet's manually check the trace plots for our main three parameters of interest.\n(We could also check the plots for all of the imputed y-values, but these\nare unlikely to be interesting or useful, any problems should hopefully\npropagate through to the interesting parameters.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_combo(post_2a, pars = c('alpha', 'beta', 'sigma'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThose look like nice healthy trace plots, so with that combined with our\ndiagnostic check, it seems that the chains mixed well and explored the\nposterior distribution. We can also check if those parameters were correlated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_pairs(post_2a, pars = c('alpha', 'beta', 'sigma'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWe see that the slope and intercept estimates were strongly correlated, which\nmakes sense, and the sigma parameter was slightly correlated with both of\nthose but not strongly with either. We can notice here that the histograms for\n$\\beta$ and $\\sigma$ are not quite centered at the true values, but they\ndo have some probability mass at those true values. Let's look at the median\nestimates and CIs from our samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar_sum <-\n\tfit_2a$summary(variables = c(\"alpha\", \"beta\", \"sigma\"))\npar_sum |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|variable |      mean|    median|        sd|       mad|        q5|       q95|     rhat|  ess_bulk| ess_tail|\n|:--------|---------:|---------:|---------:|---------:|---------:|---------:|--------:|---------:|--------:|\n|alpha    | 73.175358| 73.195200| 0.7612478| 0.7649475| 71.910180| 74.395245| 1.000667|  8533.648| 10108.17|\n|beta     |  2.809101|  2.806420| 0.1216903| 0.1226851|  2.614248|  3.010981| 1.000255|  9050.044| 11862.90|\n|sigma    |  4.897496|  4.891605| 0.2447610| 0.2444807|  4.515708|  5.316280| 1.000053| 16448.745| 13836.85|\n:::\n:::\n\n\nWe can also plot those along with the true values for reference.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show plot code (messy)\"}\ntruth <- tibble::tibble(\n\tname = c(\"alpha\", \"beta\", \"sigma\"),\n\tvalue = c(sim_parms$alpha, sim_parms$beta, sim_parms$sigma)\n)\n\nhd <- post_2a |>\n\ttibble::as_tibble() |>\n\tdplyr::select(alpha, beta, sigma) |>\n\ttidyr::pivot_longer(cols = dplyr::everything())\n\nggplot() +\n\taes(x = value) +\n\tgeom_histogram(\n\t\tdata = subset(hd, name == \"alpha\"),\n\t\tboundary = 0,\n\t\tbinwidth = 0.25,\n\t\tcol = \"black\",\n\t\tfill = \"gray\"\n\t) +\n\tgeom_histogram(\n\t\tdata = subset(hd, name == \"beta\"),\n\t\tboundary = 0,\n\t\tbinwidth = 0.05,\n\t\tcol = \"black\",\n\t\tfill = \"gray\"\n\t) +\n\tgeom_histogram(\n\t\tdata = subset(hd, name == \"sigma\"),\n\t\tboundary = 0,\n\t\tbinwidth = 0.1,\n\t\tcol = \"black\",\n\t\tfill = \"gray\"\n\t) +\n\tgeom_vline(\n\t\tdata = truth,\n\t\taes(xintercept = value),\n\t\tlinetype = \"dashed\",\n\t\tlinewidth = 1,\n\t\tcolor = \"red\"\n\t) +\n\tfacet_wrap(~name, scales = \"free\") +\n\tlabs(x = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFrom these histograms, we can see that while there is a decent amount of\nsamples close to the true values of alpha and beta, the posterior distributions\nare not centered around the true values. At the time of writing, I am not sure\nif that is a fixable problem or just something we have to deal with from having\nimperfectly observed data.\n\nWe can also do a check of how close the imputed $y$ values were on average to\nthe actual $y$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_cens_sum <-\n\tfit_2a$summary(variables = paste0('y_cens[', 1:dat_2a$N_cens, ']'))\n\ndat_comp <-\n\tsim_data |>\n\tsubset(cens) |>\n\tdplyr::select(y_star) |>\n\tdplyr::bind_cols(y_cens_sum) |>\n\tdplyr::mutate(\n\t\tcol = dplyr::case_when(\n\t\t\t(mean >= y_star) & (q5 <= y_star) ~ TRUE,\n\t\t\t(mean <= y_star) & (q95 >= y_star) ~ TRUE,\n\t\t\tTRUE ~ FALSE\n\t\t)\n\t)\n\n# ggplot(dat_comp) +\n# \taes(x = y_star, y = mean, ymin = q5, ymax = q95, color = col) +\n# \t\tgeom_abline(\n# \t\t\tslope = 1, intercept = 0, linetype = 2, linewidth = 1,\n# \t\t\t\t\t\t\t\talpha = 0.5\n# \t\t\t) +\n# \tgeom_errorbar(alpha = 0.25) +\n# \tgeom_point() +\n# \tcoord_fixed() +\n# \tscale_color_manual(\n# \t\tvalues = c(\"orange\", \"turquoise\"),\n# \t\tname = \"CI crosses diagonal\"\n# \t)\n\nggplot(dat_comp) +\n\taes(x = (y_star - mean)) +\n\tgeom_histogram(boundary = 0, binwidth = 1, color = \"black\", fill = \"gray\") +\n\tscale_x_continuous(breaks = seq(-10, 10, 2), limits = c(-10, 10)) +\n\tlabs(\n\t\tx = \"True value - mean estimated value\"\n\t)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nTODO make this relative error instead to make it easier to understand.\n\n### Integration-type method\n\nThe second method relies on calculating the direct contribution of the\ncensored data measurements to the likelihood by integrating the density over\nthe region where censored data can occur. That is, if the $i$th observation is\nbelow the detection limit, we know that the contribution of that observation\nto the sample likelihood is\n$$\n\\mathcal{L}(\\theta \\mid y_i) = P(Y_i \\leq \\mathrm{DL}) = \\int_{-\\infty}^{\\mathrm{DL}}f(y_i \\mid \\theta) \\ dy = \\lim_{a \\to -\\infty} \\left[F(y_i \\mid \\theta)\\right]_{a}^{\\mathrm{DL}},\n$$\nwhich is why we refer to this method as \"integrating out\" the censored values.\n\nBy adapting the Stan code from the manual [@StanManual, cp. 4] to include\n$x$ values in the calculation of the mean, we can implement this method for\ndealing with our censored $y$ values. First we'll load and compile the Stan model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_pth <- here::here(pth_base, \"Ex2b.stan\")\nmod_2b <- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nmod_2b$compile(force_recompile = TRUE)\n```\n:::\n\n\n<!-- debug this / figure out if possible after book is working again\n```{.stan include=mod_pth}\n```\n-->\n\nAs you can see from the above program, the data needs to be in a different\nformat for this method. Actually, it's much easier to set up the data in the\nway this program specifies, and it's very similar to the data frame we already\nhave. We just need a list and a few other components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_2b <- list()\ndat_2b$N <- nrow(sim_data)\ndat_2b$N_cens <- sum(sim_data$cens)\ndat_2b$y <- sim_data$y\ndat_2b$cens <- as.integer(sim_data$cens)\ndat_2b$x <- sim_data$x\ndat_2b$DL <- as.integer(sim_parms$DL)\n\nstr(dat_2b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 6\n $ N     : int 271\n $ N_cens: int 62\n $ y     : num [1:271] 92.6 104.2 86.2 88.7 94.9 ...\n $ cens  : int [1:271] 0 0 0 0 0 0 0 0 0 0 ...\n $ x     : num [1:271] 8.98 9.31 3.42 8.5 8.14 ...\n $ DL    : int 80\n```\n:::\n:::\n\n\nNow that we have the program ready and the data set up, we can give the data\nto the program and do some MCMC sampling. We'll use a similar setup\nthat we did for the previous example, namely 4 parallel chains which each run\n500 warmup iterations and 1000 sampling iterations (no need for overkill like\nwe did before).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2b <- mod_2b$sample(\n\tdat_2b, seed = 100, parallel_chains = 4,\n\titer_warmup = 500,\n\titer_sampling = 1000,\n\tshow_messages = FALSE\n)\n\n# Extract the posterior samples in a nicer format for later\npost_2b <- posterior::as_draws_df(fit_2b)\n```\n:::\n\n\nWe didn't get any warnings or errors, which means that the model finished the\nsampling procedure without any major errors, and we should next check the\ndiagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2b$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2b-202311160806-1-69a163.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2b-202311160806-2-69a163.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2b-202311160806-3-69a163.csv, C:/Users/Zane/AppData/Local/Temp/RtmpITeReG/Ex2b-202311160806-4-69a163.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n```\n:::\n:::\n\n\nEverything looks good here, but let's again look at the traceplots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_combo(post_2b, pars = c('alpha', 'beta', 'sigma'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/model 2b traceplot-1.png){width=672}\n:::\n:::\n\n\nWe got some nice, healthy looking fuzzy caterpillars, so now we can be\nconfident in our summary results. So now let's finally look at the parameter\nestimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2b$summary(variables = c('alpha', 'beta', 'sigma'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  <chr>    <num>  <num> <num> <num> <num> <num> <num>    <num>    <num>\n1 alpha    73.1   73.1  0.767 0.803 71.8  74.3   1.00    1324.    1250.\n2 beta      2.82   2.81 0.122 0.126  2.62  3.02  1.00    1285.    1479.\n3 sigma     4.90   4.90 0.243 0.244  4.52  5.32  1.00    2114.    2269.\n```\n:::\n:::\n\n\nThese estimates are pretty similar to the estimates from the other method,\nwhich is good in a way because it means both methods are similar. The frequentist tobit model estimate is also similar (see the appendix).\n\nUnfortunately, none of the three models to estimate the\nregression value while taking the censoring into account produce estimates that\nare exactly the same as the true simulation parameters. However, unlike the\nmuch worse naive model estimates, at least our uncertainty intervals correctly\ncontain the true values this time. So we cannot fully erase the effect of the\nflawed observation process on our data, but we can do a lot better by\ntaking the censored data into consideration.\n\n## Appendix: tobit model check {.appendix .unnumbered}\n\nSince we're dealing with one censored outcome with a known limit of detection,\nthere are actually some well-developed frequentist methods for this problem.\nNamely, we can use a **tobit model**, which specificies the likelihood\nmodel in the same way we did for the bayesian estimation method, and works\nvery similarly to the second method where we integrate out the censored data points. However,\ninstead of specifying priors to get a posterior distribution via Bayes' theorem, we instead estimate the parameters by finding the parameters which maximize the sample likelihood.\n\nMany models for censored outcomes with a variety of distributions are implemented in the R core package `survival`, but the formula for specifying a tobit model with a gaussian outcome distribution correctly is very unintuitive. Thankfully, the package `AER` provides a simple `tobit()` wrapper which translates a more standard formula into the appropriate form for the `survReg()` function and fits the model. Fitting our model using `AER::tobit()` is simple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntobit_model <- AER::tobit(\n\ty ~ x,\n\tdata = sim_data,\n\tleft = sim_parms$DL,\n\tright = Inf,\n\tdist = \"gaussian\"\n)\n\nsummary(tobit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nAER::tobit(formula = y ~ x, left = sim_parms$DL, right = Inf, \n    dist = \"gaussian\", data = sim_data)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           271             62            209              0 \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 73.21256    0.75531   96.93   <2e-16 ***\nx            2.80427    0.12071   23.23   <2e-16 ***\nLog(scale)   1.57771    0.04957   31.83   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 4.844 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 6 \nLog-likelihood: -661.2 on 3 Df\nWald-statistic: 539.7 on 1 Df, p-value: < 2.22e-16 \n```\n:::\n:::\n\n\nIf we want a lot of compatibility with standard R functions however (e.g.\n`broom::tidy()` to get the confidence intervals for the parameters), we need\nto use `survreg`. Fortunately the documentation for `AER::tobit()` explains\nhow the formula is transmogrified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nU <- sim_parms$DL\nsurvreg_model <- survival::survreg(\n\tsurvival::Surv(y, y > U, type = 'left') ~ x,\n\tdata = sim_data,\n\tdist = \"gaussian\"\n)\nsummary(survreg_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsurvival::survreg(formula = survival::Surv(y, y > U, type = \"left\") ~ \n    x, data = sim_data, dist = \"gaussian\")\n              Value Std. Error    z      p\n(Intercept) 73.2126     0.7553 96.9 <2e-16\nx            2.8043     0.1207 23.2 <2e-16\nLog(scale)   1.5777     0.0496 31.8 <2e-16\n\nScale= 4.84 \n\nGaussian distribution\nLoglik(model)= -661.2   Loglik(intercept only)= -831\n\tChisq= 339.55 on 1 degrees of freedom, p= 8e-76 \nNumber of Newton-Raphson Iterations: 6 \nn= 271 \n```\n:::\n:::\n\n\nWe can see that the two models are exactly the same. But since we've used\na model from `survival`, we get the benefit of widespread compatibility with\nother `R`-ecosystem functionality. For example, we can easily get confidence\nintervals for all three estimated parameters with `broom`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(survreg_model, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    73.2     0.755       96.9 0            71.7      74.7 \n2 x               2.80    0.121       23.2 2.15e-119     2.57      3.04\n3 Log(scale)      1.58    0.0496      31.8 2.77e-222    NA        NA   \n```\n:::\n:::\n\n\nOr at least I thought we could. Apparently there is not a built-in method to\ngive the CI for the scale parameter, and we have to do it ourselves.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for 95% CI for scale\"}\npaste0(\n\t\"Scale estimate: \",\n\tround(exp(1.58), 2),\n\t\", 95% CI: \",\n\tround(exp(1.58 - 1.96 * 0.0496), 2),\n\t\" - \",\n\tround(exp(1.58 + 1.96 * 0.0496), 2),\n\t\".\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Scale estimate: 4.85, 95% CI: 4.41 - 5.35.\"\n```\n:::\n:::\n\n\nAnyways, we can compare these to the Bayesian estimates above and see that they\nare quite similar.\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
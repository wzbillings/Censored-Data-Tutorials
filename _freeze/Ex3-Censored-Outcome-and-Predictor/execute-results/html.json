{
  "hash": "43b4936d1a6a91b5c834771feab845bb",
  "result": {
    "markdown": "# Example Model 3: Censored outcome and censored predictor\n\n\n\n\n\nNow that we've covered the simple cases, we'll try to be a bit more\nadventurous. For the next model, we'll simulate data where both the outcome\nand the predictor model are censored. We'll also implement both lower and upper\nlimits of detection for both the outcome and the predictor, so we'll only\nhave one main example in this section. We'll also only implement one method --\nin general the method where we integrate out censored values has been easier to\ncode, so we'll stick with that method for both our predictor and our output\nhere.\n\n## Data simulation\n\nAs usual, we'll begin our data simulation by writing out the true data\ngenerating process (likelihood model) that we'll use to generate the data.\nThis model is a bit complicated--of course we'll have the same regression\npart of the model as we've had before, that relates the latent $y^*$ values to\nthe latent $x^*$ values. But then the observation model will include a\ncensoring scheme for the observation of both $x$ and $y$.\n\nImportantly, in this model we also need to specify a distributional assumption\nfor $X$, otherwise we can't estimate what the uncensored $X$ values should look\nlike. So for the sake of simplicity, we'll assume a Gaussian distribution for\nthe $x$-values as well, although this is definitely something we need to think\nmore about in the future. **Furthermore, let's assume $x$ has a standard\nnormal distribution, since we can standardize $x$ before modeling.**\n\n$$\n\\begin{align*}\ny_i &= \\begin{cases}\ny_\\min, & y_i^* \\leq y_\\min \\\\\ny_i^* & y_\\min < y_i^* \\leq y_\\max \\\\\ny_\\max &  y_\\max < y_i^*\n\\end{cases} \\\\\nx_i &= \\begin{cases}\nx_\\min, & x_i^* \\leq x_\\min \\\\\nx_i^* & x_\\min < x_i^* \\leq x_\\max \\\\\nx_\\max &  x_\\max < x_i^*\n\\end{cases} \\\\\ny^*_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x^*_i \\\\\nx_i^* &\\sim \\mathrm{Normal}(0, 1)\n\\end{align*}\n$$\n\nAgain, we can choose whatever parameters we want for the simulation. I played\naround with the simulation until I got a plot I thought looked about right.\nThose simulation parameters are printed below.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nList of 8\n $ n    : num 400\n $ alpha: num 1\n $ beta : num 4\n $ sigma: num 5\n $ y_min: num -9\n $ y_max: num 12\n $ x_min: num -1\n $ x_max: num 2\n```\n:::\n:::\n\n\nSo with those parameters, we can then simulate some data according to\nthis generative model.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 400 × 5\n    x_star      mu  y_star       x       y\n     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1.80     8.19   7.08    1.80    7.08  \n 2  1.16     5.64   6.71    1.16    6.71  \n 3  0.155    1.62   6.05    0.155   6.05  \n 4  0.0988   1.40   1.24    0.0988  1.24  \n 5 -3.16   -11.6   -7.35   -1      -7.35  \n 6 -0.682   -1.73  -1.46   -0.682  -1.46  \n 7  1.56     7.25   3.40    1.56    3.40  \n 8 -0.195    0.219 -0.216  -0.195  -0.216 \n 9  0.628    3.51   7.10    0.628   7.10  \n10  0.821    4.28   0.0773  0.821   0.0773\n# ℹ 390 more rows\n```\n:::\n:::\n\n\nSince we've simulated the data, we know the latent values and the observed\nvalues, so we can plot our simulated data in order to get a better understanding\nof how much the censoring process will affect our estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plotting code\"}\nsim_data |>\n\tggplot() +\n\tgeom_hline(\n\t\tyintercept = c(sim_parms$y_min, sim_parms$y_max),\n\t\talpha = 0.5,\n\t\tlinewidth = 1,\n\t\tlinetype = \"dashed\",\n\t\tcolor = \"darkgray\"\n\t) +\n\tgeom_vline(\n\t\txintercept = c(sim_parms$x_min, sim_parms$x_max),\n\t\talpha = 0.5,\n\t\tlinewidth = 1,\n\t\tlinetype = \"dashed\",\n\t\tcolor = \"darkgray\"\n\t) +\n\tgeom_segment(\n\t\tdata = subset(sim_data, (x != x_star) | (y != y_star)),\n\t\taes(x = x_star, xend = x, y = y_star, yend = y),\n\t\tcolor = \"gray\",\n\t\talpha = 0.25,\n\t\tlwd = 1\n\t) +\n\tgeom_point(aes(x = x_star, y = y_star), color = \"gray\") +\n\tgeom_point(aes(x = x, y = y)) +\n\tcoord_cartesian(\n\t\txlim = c(-3, 3),\n\t\tylim = c(-22, 22)\n\t) +\n\tlabs(\n\t\tx = \"Independent variable\",\n\t\ty = \"Dependent variable\"\n\t)\n```\n\n::: {.cell-output-display}\n![](Ex3-Censored-Outcome-and-Predictor_files/figure-html/data plot-1.png){width=672}\n:::\n:::\n\n\nWe can see that a substantial amount of the data points are censored. In total,\n$16.5\\%$ of records were censored in $x$ only, $13.25\\%$ of records\nwere censored in $y$ only, and $5\\%$ of records were censored in both\n$x$ and $y$. Thus, $24.75\\%$ of records were censored in some way.\n\nI also deliberately set the upper and lower limits for both $x$ and $y$ to\nbe asymmetrical so we can more clearly see how our censoring process can\nstrongly bias the estimates: we have more records censored at lower values\nthan higher values, which gives us a shifted window where we observe data.\n\nSo now that we have the data simulated, we want to try to recover the original\nparameters with a Bayesian model.\n\n## Stan data setup\n\nI also want to write the Stan code to accept data in a specific format that\nwe want to test. The data should be formatted like the table below.\n\n| X        | X_L      | X_U      | Y        | Y_L      | Y_U      |\n|----------|----------|----------|----------|----------|----------|\n| $x_1$    | $x_\\min$ | $x_\\max$ | $y_1$    | $y_\\min$ | $y_\\max$ |\n| $x_2$    | $x_\\min$ | $x_\\max$ | $y_2$    | $y_\\min$ | $y_\\max$ |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ |\n| $x_n$    | $x_\\min$ | $x_\\max$ | $y_n$    | $y_\\min$ | $y_\\max$ |\n\nHere, $x_\\min$ is the lower limit of detection for $x$ and $x_\\max$ is the\nupper limit of detection for $X$ (and similar for $Y$). Eventually, if this\nis the data format we decide to permanently adopt going forward, we will want\nto write a suite of helper functions to conveniently get the data in this form.\nBut for now I will do it manually. Fortunately it is quite easy. And if the\ncensoring limits changed for any observations, it would have been easier\nto store the data in this format in the first place.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <-\n\tsim_data |>\n\tdplyr::select(x, y) |>\n\tdplyr::mutate(\n\t\tx_l = sim_parms$x_min,\n\t\tx_u = sim_parms$x_max,\n\t\t.after = x\n\t) |>\n\tdplyr::mutate(\n\t\ty_l = sim_parms$y_min,\n\t\ty_u = sim_parms$y_max,\n\t\t.after = y\n\t)\n\nstan_data |> print(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 400 × 6\n        x   x_l   x_u     y   y_l   y_u\n    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  1.80      -1     2  7.08    -9    12\n2  1.16      -1     2  6.71    -9    12\n3  0.155     -1     2  6.05    -9    12\n4  0.0988    -1     2  1.24    -9    12\n5 -1         -1     2 -7.35    -9    12\n# ℹ 395 more rows\n```\n:::\n:::\n\n\nNow we just need to convert the data frame to a list format and add a variable\nfor the number of records.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_list <- as.list(stan_data)\nstan_list$N <- nrow(stan_data)\nstr(stan_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ x  : num [1:400] 1.7973 1.1599 0.1547 0.0988 -1 ...\n $ x_l: num [1:400] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ x_u: num [1:400] 2 2 2 2 2 2 2 2 2 2 ...\n $ y  : num [1:400] 7.08 6.71 6.05 1.24 -7.35 ...\n $ y_l: num [1:400] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 ...\n $ y_u: num [1:400] 12 12 12 12 12 12 12 12 12 12 ...\n $ N  : int 400\n```\n:::\n:::\n\n\n## Stan code\n\nOf course as usual we need to compile the Stan code. The code is also\nincluded here for reference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npth <- here::here(\"Ex3.stan\")\nmod <- cmdstanr::cmdstan_model(pth, compile = FALSE)\nmod$compile(force_recompile = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/model-45387b5d7b21.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t<T_x, T_sigma, T_l> stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n < 0.0)\n      | \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n```\n:::\n:::\n\n\n::: {.callout-note icon=false collapse=true appearance=\"simple\"}\n\n```{.stan include=(pth)}\n```\n\n:::\n\n## Model fitting and performance\n\nNow that the model is successfully compiled, we need to generate MCMC samples\nfrom the posterior distribution. We'll use 4 chains (run in parallel) with\n500 warmup iterations and 2500 sampling iterations each, for a total of 10000\nsamples overall, which should be plenty for this problem. Otherwise, we'll\nleave the control parameters at their default values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- mod$sample(\n\tstan_list,\n\tseed = 123123,\n\tparallel_chains = 4,\n\titer_warmup = 500,\n\titer_sampling = 2500\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/model-45387b5d7b21.stan', line 62, column 3 to column 31)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  501 / 3000 [ 16%]  (Sampling) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Sampling) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Sampling) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Sampling) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Sampling) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Sampling) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Sampling) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Sampling) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Sampling) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 finished in 0.6 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 1.0 seconds.\n```\n:::\n:::\n\n\nAs usual, we want to check the diagnostics, and fortunately `cmdstanr` gives\nus an easy to use diagnostic flagger.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing csv files: C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/Ex3-202310182051-1-1292e5.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/Ex3-202310182051-2-1292e5.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/Ex3-202310182051-3-1292e5.csv, C:/Users/Zane/AppData/Local/Temp/RtmpkPCWNJ/Ex3-202310182051-4-1292e5.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n```\n:::\n:::\n\n\nWe can examine the trace plots and posterior distributions of the parameters\nof interest to confirm that there is no funny business.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- posterior::as_draws_array(fit)\nbayesplot::mcmc_combo(post, par = c(\"alpha\", \"beta\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](Ex3-Censored-Outcome-and-Predictor_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAnd so now we can finally examine the fitted values and compare them to our\ntrue simulation values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$summary() |>\n\tdplyr::filter(variable != \"lp__\") |>\n\tknitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n|variable | mean| median|   sd|  mad|   q5|  q95| rhat| ess_bulk| ess_tail|\n|:--------|----:|------:|----:|----:|----:|----:|----:|--------:|--------:|\n|alpha    | 0.43|   0.43| 0.26| 0.26| 0.00| 0.86|    1|  8190.67|  7220.56|\n|beta     | 4.14|   4.14| 0.29| 0.29| 3.66| 4.63|    1|  9034.90|  7285.48|\n|sigma    | 5.10|   5.10| 0.19| 0.19| 4.80| 5.42|    1|  9738.63|  7383.51|\n:::\n:::\n\n\nWe can see that our model estimated the slope and variance quite well, although\nit is not doing too great at figuring out the intercept. In fact, the true\nvalue of $\\alpha = 1$ isn't even in the credible interval. However, the\nestimates for $\\beta$ and $\\sigma$ are very close to the true estimates.\nIn most applications, the intercept is not too useful and the slope is what we\nwant an accurate estimate of anyway, so this is probably acceptable.\n\nTODO figure out what else needs to go in this example.\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "Ex3-Censored-Outcome-and-Predictor_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}